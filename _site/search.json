[
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html",
    "href": "STAT515_HR_Attrition_FIXED.html",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "",
    "text": "This comprehensive statistical analysis investigates the factors contributing to employee attrition using the IBM HR Analytics Employee Attrition & Performance dataset (n=1470). The study addresses three primary research questions using decision trees, random forests, logistic regression with interaction terms, LASSO variable selection, and stratified departmental analysis.\nKey Findings (computed from models below):\n\nWork-Life Imbalance: OverTime is the strongest predictor of attrition. Decision tree analysis identifies high-risk profiles combining overtime work with low tenure.\nCareer Stagnation & Compensation: Years since last promotion increases attrition risk, and LASSO selects the strongest predictors across work-life, pay, and tenure.\nDepartment-Stratified Satisfaction: Satisfaction variables show different predictive patterns across departments.\n\n\n\nDataset Overview:\n\n\n- Total Employees: 1470 \n\n\n- Overall Attrition Rate: 16.1 %\n\n\n- Employees Who Left: 237 \n\n\n- OverTime Attrition Rate: 30.5 %"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#background",
    "href": "STAT515_HR_Attrition_FIXED.html#background",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.1 Background",
    "text": "2.1 Background\nEmployee turnover represents one of the most significant challenges facing modern organizations. Industry estimates suggest replacing an employee can cost a substantial fraction of their annual salary when accounting for recruiting, training, and lost productivity. Understanding the factors that drive employees to leave enables targeted retention strategies before valuable talent departs.\nThis analysis leverages the IBM HR Analytics dataset (realistic, simulated dataset commonly used for HR analytics). The dataset contains 1,470 employee records with 39 attributes spanning demographics, job characteristics, satisfaction metrics, and work-life balance indicators."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#research-questions",
    "href": "STAT515_HR_Attrition_FIXED.html#research-questions",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.2 Research Questions",
    "text": "2.2 Research Questions\nAn explanation of why these research questions were chosen would be nice\n\nWork-Life Imbalance & Attrition Risk Profiling: How do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict attrition, and can we identify distinct “high-risk” profiles?\nCareer Stagnation & Compensation Effects: At what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?\nDepartment-Stratified Satisfaction Analysis: Does the predictive power of satisfaction variables (JobSatisfaction, EnvironmentSatisfaction, RelationshipSatisfaction) differ across departments, and which matters most within each?"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#dataset-description",
    "href": "STAT515_HR_Attrition_FIXED.html#dataset-description",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.3 Dataset Description",
    "text": "2.3 Dataset Description\n\n\nDataset Dimensions: 1470 employees, 41 variables\n\n\n\nTable 1: Key Variables by Research Question\n\n\nVariable\nDescription\nResearch Q\n\n\n\n\nAttrition\nEmployee left the company (Yes/No) - Response Variable\nAll\n\n\nOverTime\nWhether employee works overtime (Yes/No)\nQ1\n\n\nDistanceFromHome\nDistance from home to workplace (miles)\nQ1\n\n\nWorkLifeBalance\nWork-life balance rating (1-4, higher = better)\nQ1\n\n\nYearsAtCompany\nTotal years at current company\nQ1\n\n\nYearsSinceLastPromotion\nYears since last promotion\nQ2\n\n\nYearsInCurrentRole\nYears in current role\nQ2\n\n\nMonthlyIncome\nMonthly salary\nQ2\n\n\nPercentSalaryHike\nPercent salary increase\nQ2\n\n\nJobLevel\nJob level within company hierarchy (1-5)\nQ2\n\n\nJobSatisfaction\nJob satisfaction rating (1-4)\nQ3\n\n\nEnvironmentSatisfaction\nEnvironment satisfaction rating (1-4)\nQ3\n\n\nRelationshipSatisfaction\nRelationship satisfaction rating (1-4)\nQ3\n\n\nDepartment\nDepartment\nQ3"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#class-imbalance-analysis",
    "href": "STAT515_HR_Attrition_FIXED.html#class-imbalance-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.1 Class Imbalance Analysis",
    "text": "3.1 Class Imbalance Analysis\n\n\n\n\n\nFigure 1: Attrition Distribution - Class Imbalance\n\n\n\n\n\nClass Weights for Imbalanced Learning:\n\n\n- Weight for Class 0 (No Attrition): 0.5961 \n\n\n- Weight for Class 1 (Attrition): 3.1013 \n\n\nThe dataset exhibits a significant class imbalance in employee attrition, with 83.9% of employees (1,233) having “No” as their attrition value and only 16.1% (237) having “Yes.” This imbalance can challenge predictive modeling, as algorithms may be biased toward the majority class. To mitigate this, class weights were calculated inversely proportional to class frequencies, assigning a weight of 0.5961 to the majority class (No attrition) and 3.1013 to the minority class (Yes attrition). These weights can be applied during model training to give greater importance to underrepresented cases, improving the model’s ability to correctly identify employees at risk of leaving."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#attrition-by-key-categorical-variables",
    "href": "STAT515_HR_Attrition_FIXED.html#attrition-by-key-categorical-variables",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.2 Attrition by Key Categorical Variables",
    "text": "3.2 Attrition by Key Categorical Variables\n\n\n\n\n\nFigure 2: Attrition Rates by Key Categorical Variables\n\n\n\n\nThe graphical analysis suggests that employee attrition varies substantially across several workplace and demographic factors. Employees who work overtime show a markedly higher attrition rate (30.5%) compared to those who do not work overtime (10.4%), indicating overtime may be a strong contributor to turnover. Attrition also differs by department, with Sales experiencing the highest rate (20.6%), followed by HR (19%), while Research & Development has the lowest attrition (13.8%). Marital status appears to be associated with attrition as well, as single employees have a notably higher turnover rate (25.5%) than married (12.5%) or divorced employees (10.1%). Finally, business travel frequency shows a clear pattern: employees who travel frequently have the highest attrition rate (24.9%), followed by those who travel rarely (15%), while non-traveling employees have the lowest attrition (8%). Overall, these graphs indicate that workload demands, job role characteristics, and personal circumstances may play important roles in employee turnover."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#chi-square-tests-for-categorical-associations",
    "href": "STAT515_HR_Attrition_FIXED.html#chi-square-tests-for-categorical-associations",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.3 Chi-Square Tests for Categorical Associations",
    "text": "3.3 Chi-Square Tests for Categorical Associations\n\n\n\nTable 2: Chi-Square Tests for Categorical Variables\n\n\nVariable\nχ²\ndf\np-value\nCramer's V\nSig.\n\n\n\n\nOverTime\n87.56\n1\n0.0000000\n0.2441\n***\n\n\nDepartment\n10.80\n2\n0.0045256\n0.0857\n**\n\n\nJobRole\n86.19\n8\n0.0000000\n0.2421\n***\n\n\nMaritalStatus\n46.16\n2\n0.0000000\n0.1772\n***\n\n\nBusinessTravel\n24.18\n2\n0.0000056\n0.1283\n***\n\n\nGender\n1.12\n1\n0.2905724\n0.0276\n\n\n\n\n\n\nOverTime and JobRole show the strongest relationships with attrition, both highly significant (p &lt; 0.001) and exhibiting moderate effect sizes, suggesting that employees’ overtime status and job roles are meaningfully related to the likelihood of leaving the company. MaritalStatus and BusinessTravel are also highly significant, though with smaller effect sizes, indicating weaker but still relevant associations with attrition. Department shows a statistically significant relationship with attrition as well, but the small effect size suggests a limited practical impact. In contrast, Gender is not significantly associated with attrition, and its negligible effect size indicates that it is unlikely to influence whether an employee leaves the organization."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#numeric-variables-comparison",
    "href": "STAT515_HR_Attrition_FIXED.html#numeric-variables-comparison",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.4 Numeric Variables Comparison",
    "text": "3.4 Numeric Variables Comparison\n\n\n\n\n\nFigure 3: Distribution of Key Numeric Variables by Attrition Status\n\n\n\n\nWhen examining key numerical values in our analysis, we observe noticeable differences in attrition rates across employee categories. Attrition was more common among younger employees, those with lower monthly incomes, and employees with fewer years at the company. Additionally, employees who worked farther from home exhibited higher rates of attrition.\n\n\n\nTable 3: T-Tests Comparing Numeric Variables by Attrition Status\n\n\n\nVariable\nMean (No)\nMean (Yes)\nDifference\np-value\nSig.\n\n\n\n\nmean in group No\nAge\n37.56\n33.61\n-3.95\n0.0000000\n***\n\n\nmean in group No1\nMonthlyIncome\n6832.74\n4787.09\n-2045.65\n0.0000000\n***\n\n\nmean in group No2\nDistanceFromHome\n8.92\n10.63\n1.72\n0.0041365\n**\n\n\nmean in group No3\nYearsAtCompany\n7.37\n5.13\n-2.24\n0.0000002\n***\n\n\nmean in group No4\nYearsInCurrentRole\n4.48\n2.90\n-1.58\n0.0000000\n***\n\n\nmean in group No5\nYearsSinceLastPromotion\n2.23\n1.95\n-0.29\n0.1986513\n\n\n\nmean in group No6\nTotalWorkingYears\n11.86\n8.24\n-3.62\n0.0000000\n***\n\n\nmean in group No7\nJobSatisfaction\n2.78\n2.47\n-0.31\n0.0001052\n***\n\n\nmean in group No8\nEnvironmentSatisfaction\n2.77\n2.46\n-0.31\n0.0002092\n***\n\n\nmean in group No9\nWorkLifeBalance\n2.78\n2.66\n-0.12\n0.0304657\n*"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#research-question",
    "href": "STAT515_HR_Attrition_FIXED.html#research-question",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.1 Research Question",
    "text": "4.1 Research Question\nAt what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#decision-tree-model",
    "href": "STAT515_HR_Attrition_FIXED.html#decision-tree-model",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.1 Decision Tree Model",
    "text": "4.1 Decision Tree Model\n\n\n\n\n\nFigure 4: Decision Tree for Work-Life Attrition Risk Profiling\n\n\n\n\n\n\n\n\n\nFigure 5: Decision Tree Feature Importance\n\n\n\n\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 249  21\n         1 119  52\n                                          \n               Accuracy : 0.6825          \n                 95% CI : (0.6368, 0.7258)\n    No Information Rate : 0.8345          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2529          \n                                          \n Mcnemar's Test P-Value : 2.444e-16       \n                                          \n            Sensitivity : 0.6766          \n            Specificity : 0.7123          \n         Pos Pred Value : 0.9222          \n         Neg Pred Value : 0.3041          \n             Prevalence : 0.8345          \n         Detection Rate : 0.5646          \n   Detection Prevalence : 0.6122          \n      Balanced Accuracy : 0.6945          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nDecision Tree AUC-ROC: 0.7084"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#logistic-regression-with-interaction-terms",
    "href": "STAT515_HR_Attrition_FIXED.html#logistic-regression-with-interaction-terms",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.2 Logistic Regression with Interaction Terms",
    "text": "4.2 Logistic Regression with Interaction Terms\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ OverTime_Binary + DistanceFromHome + \n    WorkLifeBalance + YearsAtCompany + OT_x_WLB + OT_x_Distance + \n    OT_x_YearsAtCompany, family = binomial(link = \"logit\"), data = hr_q1)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -1.14314    0.41339  -2.765  0.00569 **\nOverTime_Binary      0.89098    0.63757   1.397  0.16227   \nDistanceFromHome     0.01648    0.01195   1.380  0.16760   \nWorkLifeBalance     -0.30426    0.13679  -2.224  0.02613 * \nYearsAtCompany      -0.05304    0.02098  -2.528  0.01148 * \nOT_x_WLB             0.20612    0.20979   0.982  0.32586   \nOT_x_Distance        0.02246    0.01814   1.238  0.21561   \nOT_x_YearsAtCompany -0.06240    0.03301  -1.890  0.05875 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1298.6  on 1469  degrees of freedom\nResidual deviance: 1167.0  on 1462  degrees of freedom\nAIC: 1183\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nTable 4: Logistic Regression Odds Ratios (Q1)\n\n\nVariable\nβ\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nOverTime_Binary\n0.8910\n2.4375\n0.6998\n8.5502\n0.1623\n\n\n\nDistanceFromHome\n0.0165\n1.0166\n0.9926\n1.0403\n0.1676\n\n\n\nWorkLifeBalance\n-0.3043\n0.7377\n0.5653\n0.9671\n0.0261\n*\n\n\nYearsAtCompany\n-0.0530\n0.9483\n0.9081\n0.9860\n0.0115\n*\n\n\nOT_x_WLB\n0.2061\n1.2289\n0.8137\n1.8538\n0.3259\n\n\n\nOT_x_Distance\n0.0225\n1.0227\n0.9872\n1.0600\n0.2156\n\n\n\nOT_x_YearsAtCompany\n-0.0624\n0.9395\n0.8796\n1.0016\n0.0588"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#random-forest-model",
    "href": "STAT515_HR_Attrition_FIXED.html#random-forest-model",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.3 Random Forest Model",
    "text": "4.3 Random Forest Model\n\n\n\n\n\nFigure 6: Random Forest Variable Importance\n\n\n\n\n\n\n\n\n\nFigure 7: ROC Curves Comparison - Q1 Models"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#research-question-1",
    "href": "STAT515_HR_Attrition_FIXED.html#research-question-1",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.1 Research Question",
    "text": "5.1 Research Question\nDoes the predictive power of employee satisfaction variables differ across departments?"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#traintest-split-weighted-logistic-regression",
    "href": "STAT515_HR_Attrition_FIXED.html#traintest-split-weighted-logistic-regression",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.1 Train/Test Split + Weighted Logistic Regression",
    "text": "5.1 Train/Test Split + Weighted Logistic Regression\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole + \n    MonthlyIncome + PercentSalaryHike + JobLevel, family = binomial(), \n    data = train_q2, weights = weights_q2)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              1.757e+00  3.319e-01   5.293 1.20e-07 ***\nYearsSinceLastPromotion  1.428e-01  2.757e-02   5.180 2.22e-07 ***\nYearsInCurrentRole      -1.780e-01  2.683e-02  -6.635 3.24e-11 ***\nMonthlyIncome           -9.407e-05  5.010e-05  -1.878  0.06045 .  \nPercentSalaryHike       -5.107e-02  1.844e-02  -2.770  0.00561 ** \nJobLevel                -6.872e-02  2.009e-01  -0.342  0.73233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1426.5  on 1028  degrees of freedom\nResidual deviance: 1309.6  on 1023  degrees of freedom\nAIC: 1740.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nTable 5: Weighted Logistic Regression Odds Ratios (Q2)\n\n\nVariable\nβ\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nYearsSinceLastPromotion\n0.1428\n1.1535\n1.0935\n1.2185\n0.0000\n***\n\n\nYearsInCurrentRole\n-0.1780\n0.8369\n0.7933\n0.8814\n0.0000\n***\n\n\nMonthlyIncome\n-0.0001\n0.9999\n0.9998\n1.0000\n0.0604\n\n\n\nPercentSalaryHike\n-0.0511\n0.9502\n0.9163\n0.9850\n0.0056\n**\n\n\nJobLevel\n-0.0687\n0.9336\n0.6297\n1.3855\n0.7323"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#lasso-variable-selection",
    "href": "STAT515_HR_Attrition_FIXED.html#lasso-variable-selection",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.2 LASSO Variable Selection",
    "text": "5.2 LASSO Variable Selection\n\n\n\n\n\nFigure 8: LASSO Cross-Validation\n\n\n\n\n\n\n\n\n\nFigure 9: LASSO Selected Variable Coefficients\n\n\n\n\n\n\n\nTable 6: LASSO Selected Variables\n\n\nVariable\nCoefficient\n\n\n\n\nOverTime_Binary\n1.6306\n\n\nStockOptionLevel\n-0.5456\n\n\nEnvironmentSatisfaction\n-0.3751\n\n\nJobSatisfaction\n-0.3263\n\n\nWorkLifeBalance\n-0.2454\n\n\nYearsSinceLastPromotion\n0.1537\n\n\nNumCompaniesWorked\n0.1486\n\n\nYearsInCurrentRole\n-0.1228\n\n\nYearsWithCurrManager\n-0.1221\n\n\nJobLevel\n-0.0816\n\n\nYearsAtCompany\n0.0723\n\n\nTotalWorkingYears\n-0.0478\n\n\nAge\n-0.0364\n\n\nDistanceFromHome\n0.0348\n\n\nPercentSalaryHike\n-0.0159"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#roc-auc-and-threshold-analysis-test-set",
    "href": "STAT515_HR_Attrition_FIXED.html#roc-auc-and-threshold-analysis-test-set",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.3 ROC-AUC and Threshold Analysis (Test Set)",
    "text": "5.3 ROC-AUC and Threshold Analysis (Test Set)\n\n\n\n\n\nFigure 10: ROC Curve and Threshold Analysis (Test Set)\n\n\n\n\n\n\n\nTable 7: Classification Performance at Different Thresholds (Test Set)\n\n\nThreshold\nSensitivity\nSpecificity\nAccuracy\nNote\n\n\n\n\n0.100\n0.973\n0.043\n0.197\n\n\n\n0.200\n0.959\n0.117\n0.256\n\n\n\n0.300\n0.918\n0.283\n0.388\n\n\n\n0.400\n0.781\n0.413\n0.474\n\n\n\n0.500\n0.699\n0.562\n0.585\n\n\n\n0.574\n0.589\n0.728\n0.705\n← Optimal (Youden's J)"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#research-question-2",
    "href": "STAT515_HR_Attrition_FIXED.html#research-question-2",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.1 Research Question",
    "text": "6.1 Research Question\nDoes the predictive power of employee satisfaction variables differ across departments?"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#vif-analysis-for-multicollinearity",
    "href": "STAT515_HR_Attrition_FIXED.html#vif-analysis-for-multicollinearity",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.1 VIF Analysis for Multicollinearity",
    "text": "6.1 VIF Analysis for Multicollinearity\n\n\n\nTable 8: Variance Inflation Factors (VIF)\n\n\nVariable\nVIF\nStatus\n\n\n\n\nJobSatisfaction\n1.0011\nOK\n\n\nEnvironmentSatisfaction\n1.0012\nOK\n\n\nRelationshipSatisfaction\n1.0046\nOK\n\n\nWorkLifeBalance\n1.0045\nOK\n\n\nJobInvolvement\n1.0039\nOK\n\n\nMonthlyIncome\n1.3345\nOK\n\n\nAge\n1.3376\nOK\n\n\n\n\n\nWhy are these 7 variables being selected for VIF? I get the 3 satisfaction variables because they were mentioned in the data set description as being used for question 3, but why the others? There are a lot of outputs in the project, but I’m not sure why the specific variables or methods were chosen\nThe Variance Inflation Factor (VIF) analysis for the selected variables indicates that there is no evidence of multicollinearity among the independent variables included in the model. All VIF values are very close to 1, ranging from approximately 1.00 to 1.34, which suggests that each predictor has minimal correlation with the others. Since all values are well below commonly accepted thresholds (such as 5 or 10), multicollinearity is not a concern in this analysis. Therefore, all variables can be retained in the model without risk of inflated standard errors or unstable coefficient estimates."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#stratified-logistic-regression-by-department",
    "href": "STAT515_HR_Attrition_FIXED.html#stratified-logistic-regression-by-department",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.2 Stratified Logistic Regression by Department",
    "text": "6.2 Stratified Logistic Regression by Department\n\n\n\n========================================\nDEPARTMENT: HR \n========================================\nSample Size: 63 \nAttrition Rate: 19 %\n\nCall:\nglm(formula = Attrition_Binary ~ JobSatisfaction + EnvironmentSatisfaction + \n    RelationshipSatisfaction, family = binomial(), data = df_dept)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                0.1371     1.5933   0.086    0.931\nJobSatisfaction           -0.5074     0.3211  -1.580    0.114\nEnvironmentSatisfaction   -0.3877     0.3239  -1.197    0.231\nRelationshipSatisfaction   0.2198     0.3434   0.640    0.522\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 61.351  on 62  degrees of freedom\nResidual deviance: 56.601  on 59  degrees of freedom\nAIC: 64.601\n\nNumber of Fisher Scoring iterations: 4\n\n\n========================================\nDEPARTMENT: R&D \n========================================\nSample Size: 961 \nAttrition Rate: 13.8 %\n\nCall:\nglm(formula = Attrition_Binary ~ JobSatisfaction + EnvironmentSatisfaction + \n    RelationshipSatisfaction, family = binomial(), data = df_dept)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)               0.05548    0.39463   0.141  0.88820   \nJobSatisfaction          -0.26624    0.08489  -3.136  0.00171 **\nEnvironmentSatisfaction  -0.25997    0.08450  -3.077  0.00209 **\nRelationshipSatisfaction -0.19146    0.08718  -2.196  0.02809 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 772.73  on 960  degrees of freedom\nResidual deviance: 749.18  on 957  degrees of freedom\nAIC: 757.18\n\nNumber of Fisher Scoring iterations: 4\n\n\n========================================\nDEPARTMENT: Sales \n========================================\nSample Size: 446 \nAttrition Rate: 20.6 %\n\nCall:\nglm(formula = Attrition_Binary ~ JobSatisfaction + EnvironmentSatisfaction + \n    RelationshipSatisfaction, family = binomial(), data = df_dept)\n\nCoefficients:\n                         Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)              -0.03132    0.49683  -0.063   0.9497  \nJobSatisfaction          -0.22829    0.10514  -2.171   0.0299 *\nEnvironmentSatisfaction  -0.22291    0.10875  -2.050   0.0404 *\nRelationshipSatisfaction -0.04758    0.10540  -0.451   0.6517  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 454.01  on 445  degrees of freedom\nResidual deviance: 444.72  on 442  degrees of freedom\nAIC: 452.72\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\nFigure 11: Satisfaction Variable Coefficients by Department\n\n\n\n\n\n\n\n\nTable 9: Department Model Comparison\n\n\n\n\nDepartment\n\n\nN\n\n\nAttrition Rate (%)\n\n\nAIC\n\n\nAUC\n\n\n\n\n\n\nHR\n\n\n63\n\n\n19.0\n\n\n64.60\n\n\n0.679\n\n\n\n\nR&D\n\n\n961\n\n\n13.8\n\n\n757.18\n\n\n0.616\n\n\n\n\nSales\n\n\n446\n\n\n20.6\n\n\n452.72\n\n\n0.597\n\n\n\n\nFigure 12: Model Performance by Department\n\n\n\nThe stratified logistic regression results reveal notable differences across departments in how satisfaction metrics relate to attrition.\nIn the HR department (n = 63, 19% attrition), none of the predictors (JobSatisfaction, EnvironmentSatisfaction, or RelationshipSatisfaction) are statistically significant, likely due to the small sample size, though the output suggests that higher job and environment satisfaction ratings could reduce attrition.\nIn R&D (n = 961, 13.8% attrition), all three satisfaction measures are significant predictors: higher job, environment, and relationship satisfaction are associated with lower attrition, with job and environment satisfaction having the strongest effects. In Sales (n = 446, 20.6% attrition), job and environment satisfaction significantly reduce attrition, while relationship satisfaction is not significant. Overall, the results suggest that the effect of satisfaction on attrition varies by department.\nR&D appears most sensitive to satisfaction levels, with Sales being moderately sensitive, and HR shows no clear patterns, likely reflecting limited sample size or unmeasured factors. These findings indicate that department-specific strategies—focusing on improving job and environment satisfaction—could be most effective in reducing turnover, while HR may require a broader analysis of attrition drivers."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#recommendations-for-hr-practice",
    "href": "STAT515_HR_Attrition_FIXED.html#recommendations-for-hr-practice",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "7.1 Recommendations for HR Practice",
    "text": "7.1 Recommendations for HR Practice\n\nMonitor Overtime: Overtime is consistently among the strongest predictors of attrition.\nPromotion Cadence: Stagnation signals (especially time since last promotion) meaningfully shift attrition risk.\nDepartment-Specific Actions: Satisfaction signals and their strengths differ by department."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#limitations",
    "href": "STAT515_HR_Attrition_FIXED.html#limitations",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "7.2 Limitations",
    "text": "7.2 Limitations\nDue to employee and corporate confidentiality concerns, publicly available datasets containing detailed employee information are limited. While the dataset used in this project serves as a realistic approximation of HR records for a large organization and can inform analyses and decision-making strategies relevant to real-world company data, it is still a synthetic dataset.\nClass imbalance within the dataset presented analytical challenges. Employee attrition, being a minority class, required careful threshold adjustments to ensure sensitivity and avoid biased predictive outcomes.\nThe dataset did not include information regarding the reasons for employee departures. Distinguishing between voluntary resignations and terminations could have allowed for more targeted analyses, such as identifying employees the company might have retained versus those who were dismissed.\nThe dataset represents a cross-sectional snapshot, which allows for the identification of associations but not causal relationships. Factors influencing attrition may vary significantly among individual employees, limiting the ability to draw definitive causal conclusions."
  },
  {
    "objectID": "projectcode.html",
    "href": "projectcode.html",
    "title": "Project Code",
    "section": "",
    "text": "# =============================================================================\n# STAT 515 Final Project - IBM HR Employee Attrition Analysis\n# Team: Rutvij & Sean Greg\n# George Mason University | Fall 2024\n# =============================================================================\n\n# Load required libraries\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(corrplot)\nlibrary(car)\nlibrary(caret)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(randomForest)\nlibrary(glmnet)\nlibrary(pROC)\nlibrary(gridExtra)\n\n# Set seed for reproducibility\nset.seed(515)"
  },
  {
    "objectID": "projectcode.html#decision-tree-model",
    "href": "projectcode.html#decision-tree-model",
    "title": "Project Code",
    "section": "4.1 Decision Tree Model",
    "text": "4.1 Decision Tree Model\n\n# Prepare data for Q1\nq1_data &lt;- hr_data %&gt;%\n  select(Attrition_Binary, OverTime_Binary, `Distance From Home`, \n         `Work Life Balance`, `Years At Company`) %&gt;%\n  rename(\n    DistanceFromHome = `Distance From Home`,\n    WorkLifeBalance = `Work Life Balance`,\n    YearsAtCompany = `Years At Company`\n  )\n\n# Train-test split\nset.seed(515)\ntrain_idx &lt;- createDataPartition(q1_data$Attrition_Binary, p = 0.7, list = FALSE)\ntrain_data &lt;- q1_data[train_idx, ]\ntest_data &lt;- q1_data[-train_idx, ]\n\n# Fit decision tree\ndt_model &lt;- rpart(\n  Attrition_Binary ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance + YearsAtCompany,\n  data = train_data,\n  method = \"class\",\n  parms = list(prior = c(0.5, 0.5)),\n  control = rpart.control(maxdepth = 4, minsplit = 50, cp = 0.01)\n)\n\n# Print summary\nprintcp(dt_model)\n\n# Plot decision tree\nrpart.plot(dt_model, type = 4, extra = 104, fallen.leaves = TRUE,\n           main = \"Decision Tree: Work-Life Factors Predicting Attrition\")\n\n# Feature importance\ndt_importance &lt;- dt_model$variable.importance / sum(dt_model$variable.importance)\nprint(sort(dt_importance, decreasing = TRUE))\n\n# Predictions and evaluation\npred_dt &lt;- predict(dt_model, test_data, type = \"class\")\npred_prob_dt &lt;- predict(dt_model, test_data, type = \"prob\")[, 2]\n\nconfusionMatrix(factor(pred_dt), factor(test_data$Attrition_Binary))\n\nroc_dt &lt;- roc(test_data$Attrition_Binary, pred_prob_dt)\ncat(\"Decision Tree AUC:\", auc(roc_dt), \"\\n\")"
  },
  {
    "objectID": "projectcode.html#logistic-regression-with-interactions",
    "href": "projectcode.html#logistic-regression-with-interactions",
    "title": "Project Code",
    "section": "4.2 Logistic Regression with Interactions",
    "text": "4.2 Logistic Regression with Interactions\n\n# Create interaction terms\nhr_data &lt;- hr_data %&gt;%\n  mutate(\n    OT_x_WLB = OverTime_Binary * `Work Life Balance`,\n    OT_x_Distance = OverTime_Binary * `Distance From Home`,\n    OT_x_YearsAtCompany = OverTime_Binary * `Years At Company`\n  )\n\n# Fit logistic regression with interactions\nlogit_q1 &lt;- glm(\n  Attrition_Binary ~ OverTime_Binary + `Distance From Home` + `Work Life Balance` + \n    `Years At Company` + OT_x_WLB + OT_x_Distance + OT_x_YearsAtCompany,\n  data = hr_data,\n  family = binomial(link = \"logit\")\n)\n\nsummary(logit_q1)\n\n# Odds ratios\nexp(coef(logit_q1))\nexp(confint(logit_q1))"
  },
  {
    "objectID": "projectcode.html#random-forest-model",
    "href": "projectcode.html#random-forest-model",
    "title": "Project Code",
    "section": "4.3 Random Forest Model",
    "text": "4.3 Random Forest Model\n\n# Fit Random Forest\nrf_model &lt;- randomForest(\n  factor(Attrition_Binary) ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance + YearsAtCompany,\n  data = train_data,\n  ntree = 500,\n  mtry = 2,\n  classwt = c(1, 3),\n  importance = TRUE\n)\n\nprint(rf_model)\n\n# Variable importance\nimportance(rf_model)\nvarImpPlot(rf_model)\n\n# Predictions\npred_prob_rf &lt;- predict(rf_model, test_data, type = \"prob\")[, 2]\nroc_rf &lt;- roc(test_data$Attrition_Binary, pred_prob_rf)\ncat(\"Random Forest AUC:\", auc(roc_rf), \"\\n\")\n\n# ROC comparison\nplot(roc_dt, col = \"blue\", main = \"ROC Curves - Q1 Models\")\nplot(roc_rf, col = \"red\", add = TRUE)\nlegend(\"bottomright\", c(\"Decision Tree\", \"Random Forest\"), col = c(\"blue\", \"red\"), lwd = 2)"
  },
  {
    "objectID": "projectcode.html#weighted-logistic-regression",
    "href": "projectcode.html#weighted-logistic-regression",
    "title": "Project Code",
    "section": "5.1 Weighted Logistic Regression",
    "text": "5.1 Weighted Logistic Regression\n\n# Create weights\nweights &lt;- ifelse(hr_data$Attrition_Binary == 1, weight_pos, weight_neg)\n\n# Fit weighted logistic regression\nlogit_q2 &lt;- glm(\n  Attrition_Binary ~ `Years Since Last Promotion` + `Years In Current Role` +\n    `Monthly Income` + `Percent Salary Hike` + `Job Level`,\n  data = hr_data,\n  family = binomial(),\n  weights = weights\n)\n\nsummary(logit_q2)\n\n# Odds ratios\nexp(coef(logit_q2))\nexp(confint(logit_q2))"
  },
  {
    "objectID": "projectcode.html#lasso-variable-selection",
    "href": "projectcode.html#lasso-variable-selection",
    "title": "Project Code",
    "section": "5.2 LASSO Variable Selection",
    "text": "5.2 LASSO Variable Selection\n\n# Prepare feature matrix\nlasso_vars &lt;- c(\"Years Since Last Promotion\", \"Years In Current Role\", \"Monthly Income\",\n                \"Percent Salary Hike\", \"Job Level\", \"Age\", \"Total Working Years\",\n                \"Years At Company\", \"Years With Curr Manager\", \"Distance From Home\",\n                \"Job Satisfaction\", \"Environment Satisfaction\", \"Work Life Balance\",\n                \"OverTime_Binary\", \"Num Companies Worked\", \"Stock Option Level\")\n\nX_lasso &lt;- as.matrix(hr_data[, lasso_vars])\ny_lasso &lt;- hr_data$Attrition_Binary\n\n# Cross-validated LASSO\ncv_lasso &lt;- cv.glmnet(X_lasso, y_lasso, family = \"binomial\", alpha = 1, nfolds = 10)\n\n# Plot CV\nplot(cv_lasso)\n\n# Optimal lambda\ncat(\"Optimal Lambda:\", cv_lasso$lambda.min, \"\\n\")\n\n# Coefficients\nlasso_coefs &lt;- coef(cv_lasso, s = \"lambda.min\")\nprint(lasso_coefs)"
  },
  {
    "objectID": "projectcode.html#roc-auc-and-threshold-analysis",
    "href": "projectcode.html#roc-auc-and-threshold-analysis",
    "title": "Project Code",
    "section": "5.3 ROC-AUC and Threshold Analysis",
    "text": "5.3 ROC-AUC and Threshold Analysis\n\n# Predictions\npred_probs_q2 &lt;- predict(logit_q2, type = \"response\")\nroc_q2 &lt;- roc(hr_data$Attrition_Binary, pred_probs_q2)\n\n# Optimal threshold (Youden's J)\ncoords_best &lt;- coords(roc_q2, \"best\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"))\ncat(\"Optimal Threshold:\", coords_best$threshold, \"\\n\")\ncat(\"Sensitivity:\", coords_best$sensitivity, \"\\n\")\ncat(\"Specificity:\", coords_best$specificity, \"\\n\")\n\n# ROC plot\nplot(roc_q2, main = \"ROC Curve - Career Stagnation Model\")\npoints(coords_best$specificity, coords_best$sensitivity, pch = 19, col = \"red\", cex = 2)\n\n# Classification at different thresholds\nfor (thresh in c(0.1, 0.2, 0.3, 0.4, 0.5, coords_best$threshold)) {\n  pred &lt;- ifelse(pred_probs_q2 &gt;= thresh, 1, 0)\n  sens &lt;- mean(pred[hr_data$Attrition_Binary == 1] == 1)\n  spec &lt;- mean(pred[hr_data$Attrition_Binary == 0] == 0)\n  acc &lt;- mean(pred == hr_data$Attrition_Binary)\n  cat(sprintf(\"Threshold %.3f: Sens=%.3f, Spec=%.3f, Acc=%.3f\\n\", thresh, sens, spec, acc))\n}"
  },
  {
    "objectID": "projectcode.html#vif-analysis",
    "href": "projectcode.html#vif-analysis",
    "title": "Project Code",
    "section": "6.1 VIF Analysis",
    "text": "6.1 VIF Analysis\n\n# VIF model\nvif_model &lt;- lm(Attrition_Binary ~ `Job Satisfaction` + `Environment Satisfaction` +\n                  `Relationship Satisfaction` + `Work Life Balance` + \n                  `Job Involvement` + `Monthly Income` + Age,\n                data = hr_data)\n\nvif(vif_model)"
  },
  {
    "objectID": "projectcode.html#stratified-logistic-regression",
    "href": "projectcode.html#stratified-logistic-regression",
    "title": "Project Code",
    "section": "6.2 Stratified Logistic Regression",
    "text": "6.2 Stratified Logistic Regression\n\n# Fit models for each department\ndepartments &lt;- c(\"Sales\", \"R&D\", \"HR\")\n\nfor (dept in departments) {\n  cat(\"\\n========================================\\n\")\n  cat(\"DEPARTMENT:\", dept, \"\\n\")\n  cat(\"========================================\\n\")\n  \n  df_dept &lt;- hr_data %&gt;% filter(Department == dept)\n  \n  cat(\"Sample Size:\", nrow(df_dept), \"\\n\")\n  cat(\"Attrition Rate:\", round(mean(df_dept$Attrition_Binary) * 100, 1), \"%\\n\")\n  \n  model_dept &lt;- glm(\n    Attrition_Binary ~ `Job Satisfaction` + `Environment Satisfaction` + `Relationship Satisfaction`,\n    data = df_dept,\n    family = binomial()\n  )\n  \n  print(summary(model_dept))\n  \n  cat(\"\\nOdds Ratios:\\n\")\n  print(exp(coef(model_dept)))\n  \n  # AUC\n  pred_probs &lt;- predict(model_dept, type = \"response\")\n  roc_dept &lt;- roc(df_dept$Attrition_Binary, pred_probs, quiet = TRUE)\n  cat(\"\\nAUC:\", round(auc(roc_dept), 3), \"\\n\")\n}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This project was completed as the final project for STAT 515: Applied Statistics & Visualization for Analytics at George Mason University, Fall 2024.\n\n\n\n\n\n\n\n\nRutvij\n\n\n\nContributions:\n\nData preprocessing and cleaning\nResearch Question 1 analysis (Decision Tree, Random Forest, Logistic Regression with interactions)\nResearch Question 2 analysis (Weighted Logistic Regression, LASSO variable selection)\nReport writing and documentation\nProject coordination\n\n\n\n\n\n\n\n\n\nSean Grieg\n\n\n\nContributions:\n\nExploratory Data Analysis\nChi-square tests and statistical testing\nResearch Question 3 analysis (Department-stratified models, VIF analysis)\nModel diagnostics and performance evaluation\nVisualization design"
  },
  {
    "objectID": "about.html#project-team",
    "href": "about.html#project-team",
    "title": "About",
    "section": "",
    "text": "This project was completed as the final project for STAT 515: Applied Statistics & Visualization for Analytics at George Mason University, Fall 2024.\n\n\n\n\n\n\n\n\nRutvij\n\n\n\nContributions:\n\nData preprocessing and cleaning\nResearch Question 1 analysis (Decision Tree, Random Forest, Logistic Regression with interactions)\nResearch Question 2 analysis (Weighted Logistic Regression, LASSO variable selection)\nReport writing and documentation\nProject coordination\n\n\n\n\n\n\n\n\n\nSean Grieg\n\n\n\nContributions:\n\nExploratory Data Analysis\nChi-square tests and statistical testing\nResearch Question 3 analysis (Department-stratified models, VIF analysis)\nModel diagnostics and performance evaluation\nVisualization design"
  },
  {
    "objectID": "about.html#project-overview",
    "href": "about.html#project-overview",
    "title": "About",
    "section": "2 Project Overview",
    "text": "2 Project Overview\nThis comprehensive statistical analysis investigates the factors contributing to employee attrition using the IBM HR Analytics Employee Attrition & Performance dataset. The study addresses three primary research questions:\n\nWork-Life Imbalance & Attrition Risk Profiling - Using Decision Trees, Logistic Regression with interactions, and Random Forest\nCareer Stagnation Thresholds & Compensation Effects - Using Weighted Logistic Regression, LASSO variable selection, and ROC-AUC threshold analysis\nDepartment-Stratified Satisfaction Analysis - Using stratified logistic regression, VIF analysis, and performance comparison"
  },
  {
    "objectID": "about.html#dataset",
    "href": "about.html#dataset",
    "title": "About",
    "section": "3 Dataset",
    "text": "3 Dataset\nThe IBM HR Analytics Employee Attrition dataset contains:\n\n1,470 employee records\n39 variables including demographics, job characteristics, satisfaction metrics, and work-life balance indicators\n16.1% overall attrition rate (class imbalance)\nNo missing data\n\nSource: IBM HR Analytics on data.world"
  },
  {
    "objectID": "about.html#methods-used",
    "href": "about.html#methods-used",
    "title": "About",
    "section": "4 Methods Used",
    "text": "4 Methods Used\n\n\n\n\n\n\n\nMethod\nApplication\n\n\n\n\nChi-Square Tests\nTesting categorical associations\n\n\nT-Tests\nComparing numeric variables\n\n\nDecision Tree Classification\nIdentifying risk profiles\n\n\nRandom Forest\nVariable importance, non-linear effects\n\n\nLogistic Regression\nStatistical inference, odds ratios\n\n\nLASSO Regression\nVariable selection\n\n\nROC-AUC Analysis\nModel evaluation, threshold optimization\n\n\nVIF Analysis\nMulticollinearity assessment\n\n\nStratified Modeling\nDepartment-specific effects"
  },
  {
    "objectID": "about.html#course-information",
    "href": "about.html#course-information",
    "title": "About",
    "section": "5 Course Information",
    "text": "5 Course Information\n\nCourse: STAT 515 - Applied Statistics & Visualization for Analytics\nInstitution: George Mason University\nSemester: Fall 2024\nInstructor: Professor Dassanayake"
  },
  {
    "objectID": "about.html#repository",
    "href": "about.html#repository",
    "title": "About",
    "section": "6 Repository",
    "text": "6 Repository\nThe complete code and analysis files are available in this Quarto project. To reproduce the analysis:\n\nEnsure R and RStudio are installed\nInstall required packages: tidyverse, caret, rpart, randomForest, glmnet, pROC\nOpen the .Rproj file in RStudio\nRender the Quarto documents using quarto render"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "7 Contact",
    "text": "7 Contact\nFor questions about this analysis, please contact the team members through George Mason University."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "",
    "text": "This comprehensive statistical analysis investigates the factors contributing to employee attrition using the IBM HR Analytics Employee Attrition & Performance dataset (n=1470). The study addresses three primary research questions using a multi-method approach including decision trees, cross-validated random forests, logistic regression with interaction terms, LASSO regularization, gradient boosting machines, and stratified departmental analysis.\nKey Findings:\n\nWork-Life Imbalance: OverTime emerges as the dominant predictor of attrition across all modeling approaches, with employees working overtime showing approximately 2.9× higher attrition rates (30.5% vs 10.4%). Decision tree analysis identifies distinct high-risk profiles combining overtime work with low tenure.\nCareer Stagnation & Compensation: Years since last promotion demonstrates a compounding effect on attrition risk, with the 4-year mark representing a critical threshold where risk accelerates substantially. LASSO variable selection confirms that career progression indicators outweigh pure compensation metrics.\nDepartment-Stratified Satisfaction: The predictive power of satisfaction variables varies significantly across departments. R&D shows strong relationships between all three satisfaction types and retention, while Sales exhibits selective effects, and HR’s small sample size limits reliable inference.\n\n\n\n\nTable 1: Executive Summary Statistics\n\n\nSummary Metric\nValue\n\n\n\n\nTotal Employees Analyzed\n1,470\n\n\nOverall Attrition Rate\n16.1%\n\n\nTotal Employees Who Left\n237\n\n\nOvertime Workers Attrition Rate\n30.5%\n\n\nNon-Overtime Workers Attrition Rate\n10.4%\n\n\nRelative Risk (Overtime vs Non-Overtime)\n2.9x"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.1 Background",
    "text": "2.1 Background\nEmployee turnover represents one of the most significant challenges facing modern organizations. Industry estimates suggest replacing an employee can cost a substantial fraction of their annual salary when accounting for recruiting, training, and lost productivity. Understanding the factors that drive employees to leave enables targeted retention strategies before valuable talent departs.\nThis analysis leverages the IBM HR Analytics dataset (realistic, simulated dataset commonly used for HR analytics). The dataset contains 1,470 employee records with 39 attributes spanning demographics, job characteristics, satisfaction metrics, and work-life balance indicators."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.2 Research Questions",
    "text": "2.2 Research Questions\nThree primary research questions drove the analysis for this project:\n\nWork-Life Imbalance & Attrition Risk Profiling: How do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict attrition, and can we identify distinct “high-risk” employee profiles?\nCareer Stagnation & Compensation Effects: At what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?\nDepartment-Stratified Satisfaction Analysis: Does the predictive power of employee satisfaction variables (JobSatisfaction, EnvironmentSatisfaction, RelationshipSatisfaction) differ across departments?\n\nResearch Question Justification:\nQuestion 1 was selected because work-life balance represents one of the most modifiable factors under organizational control. Unlike fixed demographics, companies can directly intervene through overtime policies, flexible scheduling, and remote work options.\nQuestion 2 addresses career progression and compensation alignment. Organizations frequently lose high-performing employees due to stagnant advancement opportunities. By identifying specific thresholds where promotion delays become critical, HR can implement proactive interventions.\nQuestion 3 recognizes that one-size-fits-all retention strategies often fail. Different departments have unique cultures, work demands, and satisfaction drivers. Stratified analysis enables targeted interventions."
  },
  {
    "objectID": "index.html#dataset-description",
    "href": "index.html#dataset-description",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.2 Dataset Description",
    "text": "2.2 Dataset Description\n\n\nDataset Dimensions: 1470 employees, 41 variables\n\n\n\nTable 1: Key Variables by Research Question\n\n\nVariable\nDescription\nResearch Q\n\n\n\n\nAttrition\nEmployee left the company (Yes/No) - Response Variable\nAll\n\n\nOverTime\nWhether employee works overtime (Yes/No)\nQ1\n\n\nDistanceFromHome\nDistance from home to workplace (miles)\nQ1\n\n\nWorkLifeBalance\nWork-life balance rating (1-4, higher = better)\nQ1\n\n\nYearsAtCompany\nTotal years at current company\nQ1\n\n\nYearsSinceLastPromotion\nYears since last promotion\nQ2\n\n\nYearsInCurrentRole\nYears in current role\nQ2\n\n\nMonthlyIncome\nMonthly salary\nQ2\n\n\nPercentSalaryHike\nPercent salary increase\nQ2\n\n\nJobLevel\nJob level within company hierarchy (1-5)\nQ2\n\n\nJobSatisfaction\nJob satisfaction rating (1-4)\nQ3\n\n\nEnvironmentSatisfaction\nEnvironment satisfaction rating (1-4)\nQ3\n\n\nRelationshipSatisfaction\nRelationship satisfaction rating (1-4)\nQ3\n\n\nDepartment\nDepartment\nQ3\n\n\n\n\n\nSeveral key variables from the dataset were organized into three groups, corresponding to research questions. These variables each tested against the “Attrition” variable, the main response variable for the project."
  },
  {
    "objectID": "index.html#class-imbalance-analysis",
    "href": "index.html#class-imbalance-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.1 Class Imbalance Analysis",
    "text": "3.1 Class Imbalance Analysis\n\n\n\n\n\nFigure 1: Attrition Distribution - Class Imbalance Analysis\n\n\n\n\n\n=== Class Imbalance Statistics ===\n\n\nMajority Class (No Attrition): 1233 employees ( 83.9 %)\n\n\nMinority Class (Attrition): 237 employees ( 16.1 %)\n\n\nImbalance Ratio: 5.2 :1\n\n\n=== Class Weights for Balanced Learning ===\n\n\nWeight for Class 0 (No Attrition): 0.5961 \n\n\nWeight for Class 1 (Attrition): 3.1013 \n\n\nThe dataset exhibits significant class imbalance with 83.9% of employees showing no attrition and only 16.1% experiencing attrition. To address this, we employ inverse class weighting in logistic regression and tree-based models, stratified sampling during train/test splits, evaluation metrics beyond accuracy (AUC-ROC), and threshold optimization."
  },
  {
    "objectID": "index.html#attrition-by-key-categorical-variables",
    "href": "index.html#attrition-by-key-categorical-variables",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.2 Attrition by Key Categorical Variables",
    "text": "3.2 Attrition by Key Categorical Variables\n\n\n\n\n\nFigure 2: Attrition Rates by Key Categorical Variables\n\n\n\n\nEmployees working overtime exhibit dramatically higher attrition (30.5%) compared to those without overtime (10.4%), representing a 2.9x relative risk.\n\n\n\nTable 3: Chi-Square Tests of Association with Attrition\n\n\nVariable\nChi-Square\ndf\np-value\nCramer's V\nSig.\n\n\n\n\nOverTime\n87.56\n1\n0.0000\n0.244\n***\n\n\nDepartment\n10.80\n2\n0.0045\n0.086\n**\n\n\nMaritalStatus\n46.16\n2\n0.0000\n0.177\n***\n\n\nBusinessTravel\n24.18\n2\n0.0000\n0.128\n***"
  },
  {
    "objectID": "index.html#chi-square-tests-for-categorical-associations",
    "href": "index.html#chi-square-tests-for-categorical-associations",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.3 Chi-Square Tests for Categorical Associations",
    "text": "3.3 Chi-Square Tests for Categorical Associations\n\n\n\nTable 2: Chi-Square Tests for Categorical Variables\n\n\nVariable\nχ²\ndf\np-value\nCramer's V\nSig.\n\n\n\n\nOverTime\n87.56\n1\n0.0000000\n0.2441\n***\n\n\nDepartment\n10.80\n2\n0.0045256\n0.0857\n**\n\n\nJobRole\n86.19\n8\n0.0000000\n0.2421\n***\n\n\nMaritalStatus\n46.16\n2\n0.0000000\n0.1772\n***\n\n\nBusinessTravel\n24.18\n2\n0.0000056\n0.1283\n***\n\n\nGender\n1.12\n1\n0.2905724\n0.0276\n\n\n\n\n\n\nOverTime and JobRole show the strongest relationships with attrition, both highly significant (p &lt; 0.001) and exhibiting moderate effect sizes, suggesting that employees’ overtime status and job roles are meaningfully related to the likelihood of leaving the company. MaritalStatus and BusinessTravel are also highly significant, though with smaller effect sizes, indicating weaker but still relevant associations with attrition. Department shows a statistically significant relationship with attrition as well, but the small effect size suggests a limited practical impact. In contrast, Gender is not significantly associated with attrition, and its negligible effect size indicates that it is unlikely to influence whether an employee leaves the organization."
  },
  {
    "objectID": "index.html#numeric-variables-comparison",
    "href": "index.html#numeric-variables-comparison",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.4 Numeric Variables Comparison",
    "text": "3.4 Numeric Variables Comparison\n\n\n\n\n\nFigure 3: Distribution of Key Numeric Variables by Attrition Status\n\n\n\n\nWhen examining key numerical values in our analysis, we observe noticeable differences in attrition rates across employee categories. Attrition was more common among younger employees, those with lower monthly incomes, and employees with fewer years at the company. Additionally, employees who worked farther from home exhibited higher rates of attrition.\n\n\n\nTable 3: T-Tests Comparing Numeric Variables by Attrition Status\n\n\n\nVariable\nMean (No)\nMean (Yes)\nDifference\np-value\nSig.\n\n\n\n\nmean in group No\nAge\n37.56\n33.61\n-3.95\n0.0000000\n***\n\n\nmean in group No1\nMonthlyIncome\n6832.74\n4787.09\n-2045.65\n0.0000000\n***\n\n\nmean in group No2\nDistanceFromHome\n8.92\n10.63\n1.72\n0.0041365\n**\n\n\nmean in group No3\nYearsAtCompany\n7.37\n5.13\n-2.24\n0.0000002\n***\n\n\nmean in group No4\nYearsInCurrentRole\n4.48\n2.90\n-1.58\n0.0000000\n***\n\n\nmean in group No5\nYearsSinceLastPromotion\n2.23\n1.95\n-0.29\n0.1986513\n\n\n\nmean in group No6\nTotalWorkingYears\n11.86\n8.24\n-3.62\n0.0000000\n***\n\n\nmean in group No7\nJobSatisfaction\n2.78\n2.47\n-0.31\n0.0001052\n***\n\n\nmean in group No8\nEnvironmentSatisfaction\n2.77\n2.46\n-0.31\n0.0002092\n***\n\n\nmean in group No9\nWorkLifeBalance\n2.78\n2.66\n-0.12\n0.0304657\n*"
  },
  {
    "objectID": "index.html#research-question",
    "href": "index.html#research-question",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.1 Research Question",
    "text": "4.1 Research Question\nHow do work-life factors (OverTime, Distance From Home, Work Life Balance rating, and Years At Company) interact to predict employee attrition, and can we identify distinct “high-risk” employee profiles using these variables?"
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.2 Methodology",
    "text": "4.2 Methodology\nWe employ three complementary approaches:\n\nDecision Tree Classification - For interpretable rule-based risk profiles\nLogistic Regression with Interaction Terms - For statistical inference on effects\nRandom Forest - For variable importance and non-linear effects"
  },
  {
    "objectID": "index.html#decision-tree-model",
    "href": "index.html#decision-tree-model",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.1 Decision Tree Model",
    "text": "4.1 Decision Tree Model\n\n\n\n\n\nFigure 4: Decision Tree for Work-Life Attrition Risk Profiling\n\n\n\n\nThe tree reveals a hierarchical risk structure with OverTime as the root node (most important split). The highest risk profile combines overtime work with low tenure (&lt;2 years).\n\n\n\n\n\nFigure 5: Decision Tree Feature Importance\n\n\n\n\n\n\n\n=== Decision Tree Test Set Performance ===\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 249  21\n         1 119  52\n                                          \n               Accuracy : 0.6825          \n                 95% CI : (0.6368, 0.7258)\n    No Information Rate : 0.8345          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2529          \n                                          \n Mcnemar's Test P-Value : 2.444e-16       \n                                          \n            Sensitivity : 0.6766          \n            Specificity : 0.7123          \n         Pos Pred Value : 0.9222          \n         Neg Pred Value : 0.3041          \n             Prevalence : 0.8345          \n         Detection Rate : 0.5646          \n   Detection Prevalence : 0.6122          \n      Balanced Accuracy : 0.6945          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nDecision Tree AUC-ROC: 0.7084"
  },
  {
    "objectID": "index.html#logistic-regression-with-interaction-terms",
    "href": "index.html#logistic-regression-with-interaction-terms",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.2 Logistic Regression with Interaction Terms",
    "text": "4.2 Logistic Regression with Interaction Terms\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ OverTime_Binary + DistanceFromHome + \n    WorkLifeBalance + YearsAtCompany + OT_x_WLB + OT_x_Distance + \n    OT_x_YearsAtCompany, family = binomial(link = \"logit\"), data = hr_q1)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -1.14314    0.41339  -2.765  0.00569 **\nOverTime_Binary      0.89098    0.63757   1.397  0.16227   \nDistanceFromHome     0.01648    0.01195   1.380  0.16760   \nWorkLifeBalance     -0.30426    0.13679  -2.224  0.02613 * \nYearsAtCompany      -0.05304    0.02098  -2.528  0.01148 * \nOT_x_WLB             0.20612    0.20979   0.982  0.32586   \nOT_x_Distance        0.02246    0.01814   1.238  0.21561   \nOT_x_YearsAtCompany -0.06240    0.03301  -1.890  0.05875 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1298.6  on 1469  degrees of freedom\nResidual deviance: 1167.0  on 1462  degrees of freedom\nAIC: 1183\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nTable 5: Logistic Regression Odds Ratios with Interaction Terms (Q1)\n\n\nVariable\nBeta\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nOverTime_Binary\n0.8910\n2.4375\n0.6998\n8.5502\n0.1623\n\n\n\nDistanceFromHome\n0.0165\n1.0166\n0.9926\n1.0403\n0.1676\n\n\n\nWorkLifeBalance\n-0.3043\n0.7377\n0.5653\n0.9671\n0.0261\n*\n\n\nYearsAtCompany\n-0.0530\n0.9483\n0.9081\n0.9860\n0.0115\n*\n\n\nOT_x_WLB\n0.2061\n1.2289\n0.8137\n1.8538\n0.3259\n\n\n\nOT_x_Distance\n0.0225\n1.0227\n0.9872\n1.0600\n0.2156\n\n\n\nOT_x_YearsAtCompany\n-0.0624\n0.9395\n0.8796\n1.0016\n0.0588"
  },
  {
    "objectID": "index.html#random-forest-model",
    "href": "index.html#random-forest-model",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.3 Random Forest Model",
    "text": "4.3 Random Forest Model\n\n\n\n\n\nFigure 6: Random Forest Variable Importance\n\n\n\n\n\n\n\n\n\nFigure 7: ROC Curves Comparison - Q1 Models"
  },
  {
    "objectID": "index.html#q1-interpretation",
    "href": "index.html#q1-interpretation",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.6 Q1 Interpretation",
    "text": "4.6 Q1 Interpretation\nThe analysis of work-life factors reveals several important findings:\n\nOverTime is the Dominant Predictor: Both decision tree and random forest identify OverTime as the single most important variable, accounting for approximately 46% of the decision tree’s predictive power.\nHigh-Risk Profile Identified: The decision tree identifies a clear high-risk profile:\n\nOverTime = Yes\nLow Years At Company (&lt; 2-3 years)\nHigher Distance From Home\n\nWork Life Balance is Protective: Each unit increase in Work Life Balance rating reduces attrition odds by approximately 26% (OR = 0.74, p &lt; 0.05).\nModel Performance: Random Forest (AUC = 0.619) slightly outperforms Decision Tree (AUC = 0.708), suggesting some non-linear relationships exist."
  },
  {
    "objectID": "index.html#research-question-1",
    "href": "index.html#research-question-1",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.1 Research Question",
    "text": "5.1 Research Question\nAt what thresholds do career stagnation indicators (Years Since Last Promotion, Years In Current Role) combined with compensation factors (Monthly Income, Percent Salary Hike) become critical predictors of attrition?"
  },
  {
    "objectID": "index.html#weighted-logistic-regression",
    "href": "index.html#weighted-logistic-regression",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.2 Weighted Logistic Regression",
    "text": "5.2 Weighted Logistic Regression\n\n\nShow Code\n# Prepare weights\nweights &lt;- ifelse(hr_data$Attrition_Binary == 1, weight_pos, weight_neg)\n\n# Fit weighted logistic regression\nlogit_q2 &lt;- glm(\n  Attrition_Binary ~ `Years Since Last Promotion` + `Years In Current Role` +\n    `Monthly Income` + `Percent Salary Hike` + `Job Level`,\n  data = hr_data,\n  family = binomial(),\n  weights = weights\n)\n\nsummary(logit_q2)\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ `Years Since Last Promotion` + \n    `Years In Current Role` + `Monthly Income` + `Percent Salary Hike` + \n    `Job Level`, family = binomial(), data = hr_data, weights = weights)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   1.263e+00  2.672e-01   4.729 2.26e-06 ***\n`Years Since Last Promotion`  1.268e-01  2.289e-02   5.538 3.06e-08 ***\n`Years In Current Role`      -1.569e-01  2.210e-02  -7.102 1.23e-12 ***\n`Monthly Income`             -4.925e-05  4.076e-05  -1.208    0.227    \n`Percent Salary Hike`        -1.591e-02  1.488e-02  -1.069    0.285    \n`Job Level`                  -2.358e-01  1.659e-01  -1.421    0.155    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2037.9  on 1469  degrees of freedom\nResidual deviance: 1889.2  on 1464  degrees of freedom\nAIC: 2509.3\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nShow Code\n# Odds ratios for Q2\nor_table_q2 &lt;- data.frame(\n  Variable = names(coef(logit_q2)),\n  Coefficient = coef(logit_q2),\n  OR = exp(coef(logit_q2)),\n  CI_Lower = exp(confint(logit_q2))[, 1],\n  CI_Upper = exp(confint(logit_q2))[, 2],\n  p_value = summary(logit_q2)$coefficients[, 4]\n)\n\nor_table_q2$Significance &lt;- ifelse(or_table_q2$p_value &lt; 0.001, \"***\",\n                                   ifelse(or_table_q2$p_value &lt; 0.01, \"**\",\n                                          ifelse(or_table_q2$p_value &lt; 0.05, \"*\", \"\")))\n\nkable(or_table_q2[-1, ],\n      digits = 4,\n      caption = \"Table 5: Weighted Logistic Regression Odds Ratios (Q2)\",\n      col.names = c(\"Variable\", \"β\", \"OR\", \"95% CI Lower\", \"95% CI Upper\", \"p-value\", \"Sig.\"),\n      row.names = FALSE) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  column_spec(1, bold = TRUE)\n\n\n\nTable 5: Weighted Logistic Regression Odds Ratios (Q2)\n\n\nVariable\nβ\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\n`Years Since Last Promotion`\n0.1268\n1.1352\n1.0857\n1.1878\n0.0000\n***\n\n\n`Years In Current Role`\n-0.1569\n0.8548\n0.8181\n0.8922\n0.0000\n***\n\n\n`Monthly Income`\n0.0000\n1.0000\n0.9999\n1.0000\n0.2269\n\n\n\n`Percent Salary Hike`\n-0.0159\n0.9842\n0.9559\n1.0133\n0.2849\n\n\n\n`Job Level`\n-0.2358\n0.7900\n0.5705\n1.0935\n0.1552"
  },
  {
    "objectID": "index.html#lasso-variable-selection",
    "href": "index.html#lasso-variable-selection",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.2 LASSO Variable Selection",
    "text": "5.2 LASSO Variable Selection\n\n\n\n\n\nFigure 10: LASSO Cross-Validation on Training Set\n\n\n\n\n\n=== LASSO Regularization Results ===\n\n\nOptimal lambda (min): 0.00384 \n\n\nVariables retained: 14 of 16 \n\n\n\n\n\n\n\nFigure 11: LASSO Selected Variable Coefficients\n\n\n\n\n\n\n\nTable 8: LASSO Selected Variables\n\n\nVariable\nCoefficient\n\n\n\n\nOverTime_Binary\n1.4761\n\n\nStockOptionLevel\n-0.5299\n\n\nEnvironmentSatisfaction\n-0.2636\n\n\nJobSatisfaction\n-0.2364\n\n\nWorkLifeBalance\n-0.1822\n\n\nYearsSinceLastPromotion\n0.1372\n\n\nYearsInCurrentRole\n-0.0949\n\n\nNumCompaniesWorked\n0.0896\n\n\nYearsWithCurrManager\n-0.0623\n\n\nPercentSalaryHike\n-0.0457\n\n\nDistanceFromHome\n0.0430\n\n\nTotalWorkingYears\n-0.0397\n\n\nAge\n-0.0188\n\n\n\n\n\n\n=== Variables Eliminated by LASSO ===\n\n\nVariables shrunk to zero: MonthlyIncome, JobLevel, YearsAtCompany \n\n\n\n\n\n=== LASSO Performance on Test Set ===\n\n\nTest Set AUC-ROC: 0.8034"
  },
  {
    "objectID": "index.html#roc-auc-and-threshold-analysis",
    "href": "index.html#roc-auc-and-threshold-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.4 ROC-AUC and Threshold Analysis",
    "text": "5.4 ROC-AUC and Threshold Analysis\n\n\nShow Code\n# Predictions from weighted logit\npred_probs_q2 &lt;- predict(logit_q2, type = \"response\")\nroc_q2 &lt;- roc(hr_data$Attrition_Binary, pred_probs_q2)\n\n# Find optimal threshold\ncoords_best &lt;- coords(roc_q2, \"best\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"))\n\n# Plot ROC with optimal threshold\npar(mfrow = c(1, 2))\n\n# ROC Curve\nplot(roc_q2, col = \"#e74c3c\", lwd = 2, main = \"ROC Curve - Career Stagnation Model\")\npoints(coords_best$specificity, coords_best$sensitivity, pch = 19, col = \"#2ecc71\", cex = 2)\ntext(coords_best$specificity - 0.1, coords_best$sensitivity + 0.05, \n     paste(\"Optimal:\", round(coords_best$threshold, 3)), col = \"#2ecc71\")\nlegend(\"bottomright\", paste(\"AUC =\", round(auc(roc_q2), 3)), bty = \"n\")\n\n# Threshold vs Metrics\nthresholds &lt;- seq(0.1, 0.9, by = 0.05)\nmetrics &lt;- sapply(thresholds, function(t) {\n  pred &lt;- ifelse(pred_probs_q2 &gt;= t, 1, 0)\n  c(Sensitivity = mean(pred[hr_data$Attrition_Binary == 1] == 1),\n    Specificity = mean(pred[hr_data$Attrition_Binary == 0] == 0),\n    Accuracy = mean(pred == hr_data$Attrition_Binary))\n})\n\nplot(thresholds, metrics[\"Sensitivity\", ], type = \"l\", col = \"#e74c3c\", lwd = 2,\n     ylim = c(0, 1), xlab = \"Threshold\", ylab = \"Metric Value\",\n     main = \"Classification Metrics vs Threshold\")\nlines(thresholds, metrics[\"Specificity\", ], col = \"#3498db\", lwd = 2)\nlines(thresholds, metrics[\"Accuracy\", ], col = \"#2ecc71\", lwd = 2)\nabline(v = coords_best$threshold, lty = 2, col = \"gray\")\nlegend(\"right\", c(\"Sensitivity\", \"Specificity\", \"Accuracy\"), \n       col = c(\"#e74c3c\", \"#3498db\", \"#2ecc71\"), lwd = 2)\n\n\n\n\n\nFigure 10: ROC Curve and Threshold Analysis\n\n\n\n\nShow Code\npar(mfrow = c(1, 1))\n\n\n\n\nShow Code\n# Create threshold comparison table\nthreshold_results &lt;- data.frame(\n  Threshold = c(0.1, 0.2, 0.3, 0.4, 0.5, round(coords_best$threshold, 3)),\n  Sensitivity = numeric(6),\n  Specificity = numeric(6),\n  Accuracy = numeric(6)\n)\n\nfor (i in 1:6) {\n  t &lt;- threshold_results$Threshold[i]\n  pred &lt;- ifelse(pred_probs_q2 &gt;= t, 1, 0)\n  threshold_results$Sensitivity[i] &lt;- round(mean(pred[hr_data$Attrition_Binary == 1] == 1), 3)\n  threshold_results$Specificity[i] &lt;- round(mean(pred[hr_data$Attrition_Binary == 0] == 0), 3)\n  threshold_results$Accuracy[i] &lt;- round(mean(pred == hr_data$Attrition_Binary), 3)\n}\n\nthreshold_results$Note &lt;- c(\"\", \"\", \"\", \"\", \"\", \"← Optimal (Youden's J)\")\n\nkable(threshold_results,\n      caption = \"Table 7: Classification Performance at Different Thresholds\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(6, bold = TRUE, background = \"#d4edda\")\n\n\n\nTable 7: Classification Performance at Different Thresholds\n\n\nThreshold\nSensitivity\nSpecificity\nAccuracy\nNote\n\n\n\n\n0.100\n0.992\n0.018\n0.175\n\n\n\n0.200\n0.983\n0.071\n0.218\n\n\n\n0.300\n0.932\n0.200\n0.318\n\n\n\n0.400\n0.873\n0.364\n0.446\n\n\n\n0.500\n0.700\n0.556\n0.579\n\n\n\n0.536\n0.646\n0.645\n0.645\n← Optimal (Youden's J)"
  },
  {
    "objectID": "index.html#q2-interpretation",
    "href": "index.html#q2-interpretation",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.5 Q2 Interpretation",
    "text": "5.5 Q2 Interpretation\n\nYears Since Last Promotion is Critical: Each additional year without promotion increases attrition odds by approximately 13% (OR = 1.13, p &lt; 0.001).\nYears In Current Role is Protective: Surprisingly, longer tenure in the same role is associated with lower attrition (OR = 0.85, p &lt; 0.001), possibly reflecting employee-role fit.\nLASSO Variable Selection: LASSO selected 15 variables from 16 candidates. OverTime emerged as the strongest predictor even among career/compensation variables.\nOptimal Threshold: The optimal classification threshold is 0.536, providing 65% sensitivity and 64.4% specificity."
  },
  {
    "objectID": "index.html#research-question-2",
    "href": "index.html#research-question-2",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.1 Research Question",
    "text": "6.1 Research Question\nDoes the predictive power of employee satisfaction variables differ across departments (R&D, Sales, HR)?"
  },
  {
    "objectID": "index.html#vif-analysis-for-multicollinearity",
    "href": "index.html#vif-analysis-for-multicollinearity",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "7.1 VIF Analysis for Multicollinearity",
    "text": "7.1 VIF Analysis for Multicollinearity\n\n\n\nTable 10: Variance Inflation Factors\n\n\nVariable\nVIF\nStatus\n\n\n\n\nJobSatisfaction\n1.0011\nExcellent\n\n\nEnvironmentSatisfaction\n1.0012\nExcellent\n\n\nRelationshipSatisfaction\n1.0046\nExcellent\n\n\nWorkLifeBalance\n1.0045\nExcellent\n\n\nJobInvolvement\n1.0039\nExcellent\n\n\nMonthlyIncome\n1.3345\nExcellent\n\n\nAge\n1.3376\nExcellent\n\n\n\n\n\nAll VIF values are near 1.0, indicating no problematic multicollinearity among satisfaction predictors."
  },
  {
    "objectID": "index.html#stratified-logistic-regression-by-department",
    "href": "index.html#stratified-logistic-regression-by-department",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "7.2 Stratified Logistic Regression by Department",
    "text": "7.2 Stratified Logistic Regression by Department\n\n\n\n========================================\nDEPARTMENT: HR \n========================================\nSample Size: 63 \nAttrition Rate: 19 %\n                           Estimate Std. Error     z value  Pr(&gt;|z|)\n(Intercept)               0.1370618  1.5933112  0.08602322 0.9314480\nJobSatisfaction          -0.5073798  0.3211060 -1.58010078 0.1140838\nEnvironmentSatisfaction  -0.3876713  0.3238794 -1.19696188 0.2313214\nRelationshipSatisfaction  0.2198366  0.3433554  0.64025966 0.5220038\n\n========================================\nDEPARTMENT: R&D \n========================================\nSample Size: 961 \nAttrition Rate: 13.8 %\n                            Estimate Std. Error    z value    Pr(&gt;|z|)\n(Intercept)               0.05547913 0.39462785  0.1405859 0.888197058\nJobSatisfaction          -0.26623699 0.08488571 -3.1364171 0.001710258\nEnvironmentSatisfaction  -0.25997452 0.08449797 -3.0766955 0.002093090\nRelationshipSatisfaction -0.19146008 0.08718452 -2.1960330 0.028089582\n\n========================================\nDEPARTMENT: Sales \n========================================\nSample Size: 446 \nAttrition Rate: 20.6 %\n                            Estimate Std. Error     z value   Pr(&gt;|z|)\n(Intercept)              -0.03131973  0.4968289 -0.06303927 0.94973524\nJobSatisfaction          -0.22828527  0.1051384 -2.17128331 0.02990976\nEnvironmentSatisfaction  -0.22291352  0.1087509 -2.04976300 0.04038756\nRelationshipSatisfaction -0.04758180  0.1053996 -0.45144177 0.65167118\n\n\n\n\n\n\n\nFigure 16: Satisfaction Variable Coefficients by Department\n\n\n\n\n\n\n\nTable 11: Department-Stratified Model Performance\n\n\nDepartment\nN\nAttrition Rate (%)\nAIC\nAUC\n\n\n\n\nHR\n63\n19.0\n64.60\n0.679\n\n\nR&D\n961\n13.8\n757.18\n0.616\n\n\nSales\n446\n20.6\n452.72\n0.597\n\n\n\n\n\nQ3 Key Findings:\n\nR&D (n = 961): All three satisfaction variables show significant negative relationships with attrition.\nSales (n = 446): Job and environment satisfaction predict lower attrition; relationship satisfaction not significant.\nHR (n = 63): Small sample size limits statistical power - no variables reach significance."
  },
  {
    "objectID": "index.html#q3-interpretation",
    "href": "index.html#q3-interpretation",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.4 Q3 Interpretation",
    "text": "6.4 Q3 Interpretation\n\nDepartment Differences Matter: Satisfaction variables show significantly different effects across departments:\n\nR&D: All three satisfaction measures are significant predictors (p &lt; 0.05)\nSales: Job and Environment Satisfaction are significant; Relationship Satisfaction is not\nHR: No individual satisfaction variable reaches significance (limited by small sample size, n=63)\n\nSales Has Highest Attrition: Sales department shows the highest attrition rate (20.6%), followed by HR (19.0%) and R&D (13.8%).\nVIF Analysis: Satisfaction variables show moderate multicollinearity (VIF 5-7), which is expected given their conceptual overlap."
  },
  {
    "objectID": "index.html#summary-of-key-findings",
    "href": "index.html#summary-of-key-findings",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "9.1 Summary of Key Findings",
    "text": "9.1 Summary of Key Findings\n\n9.1.1 Finding 1: Overtime is the Dominant Attrition Driver\nEvidence Across All Methods:\n\nUnivariate: 30.5% attrition (overtime) vs 10.4% (no overtime) = 2.9× relative risk\nChi-square: Largest effect size (Cramér’s V = 0.24)\nDecision Tree: First split (most important variable)\nRandom Forest: Highest variable importance\nLASSO: Largest absolute coefficient after regularization\nGBM: Ranked #1 in variable importance\n\nPractical Translation: Overtime workers (28% of workforce) contribute approximately 85 “excess” departures annually. If we reduce overtime workers from 28% to 20% through workload rebalancing and flexible scheduling:\n\nPrevented departures: ~24 employees/year\nSavings: $1,560,000 (at $65K replacement cost)\nInvestment: ~$50,000 (time tracking + temporary contractors)\nROI: 31:1 in first year\n\n\n\n9.1.2 Finding 2: Career Stagnation Compounds Exponentially\nEach year without promotion increases attrition odds by ~15%, compounding to 77% increase at 4 years. The 4-year mark represents a critical threshold requiring proactive intervention.\n\n\n9.1.3 Finding 3: Satisfaction Effects Vary by Department\nR&D shows strong relationships between all satisfaction types and retention; Sales shows selective effects; HR’s small sample limits inference."
  },
  {
    "objectID": "index.html#recommendations-for-hr-practice",
    "href": "index.html#recommendations-for-hr-practice",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "9.2 Recommendations for HR Practice",
    "text": "9.2 Recommendations for HR Practice\n\nMonitor Overtime: Implement overtime tracking dashboards and workload rebalancing initiatives. Flag employees exceeding 10+ hours/week overtime for 3+ consecutive months.\nPromotion Cadence: Establish mandatory career reviews at Year 3. Require documented development plans with timeline commitments.\nDepartment-Specific Actions: Deploy different retention strategies by department. Focus on job satisfaction in Sales; comprehensive satisfaction in R&D.\nRisk Scoring System: Deploy the GBM model for monthly batch scoring. Create tiered intervention protocols based on predicted risk."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "9.4 Limitations",
    "text": "9.4 Limitations\n\nSynthetic Data: While realistic, the dataset is simulated. Real-world data would require additional preprocessing.\nClass Imbalance: The 16% attrition rate required careful handling through weighted learning and threshold optimization.\nMissing Departure Reasons: Distinguishing voluntary resignations from terminations would enable more targeted analysis.\nCross-Sectional Design: The snapshot nature allows identification of associations but not causal relationships.\nHR Department Sample Size: With only n=63, the HR department lacks statistical power for reliable inference."
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#work-life-imbalance-attrition-risk-profiling",
    "href": "STAT515_HR_Attrition_FIXED.html#work-life-imbalance-attrition-risk-profiling",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Work-Life Imbalance & Attrition Risk Profiling",
    "text": "Work-Life Imbalance & Attrition Risk Profiling\nHow do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict employee attrition, and can we identify distinct “high-risk” employee profiles using these variables?"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#career-stagnation-compensation-effects",
    "href": "STAT515_HR_Attrition_FIXED.html#career-stagnation-compensation-effects",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Career Stagnation & Compensation Effects",
    "text": "Career Stagnation & Compensation Effects\nAt what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?"
  },
  {
    "objectID": "STAT515_HR_Attrition_FIXED.html#department-stratified-satisfaction-analysis",
    "href": "STAT515_HR_Attrition_FIXED.html#department-stratified-satisfaction-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Department-Stratified Satisfaction Analysis",
    "text": "Department-Stratified Satisfaction Analysis\nDoes the predictive power of employee satisfaction variables differ across departments?"
  },
  {
    "objectID": "index.html#work-life-imbalance-attrition-risk-profiling",
    "href": "index.html#work-life-imbalance-attrition-risk-profiling",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Work-Life Imbalance & Attrition Risk Profiling",
    "text": "Work-Life Imbalance & Attrition Risk Profiling\nHow do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict employee attrition, and can we identify distinct “high-risk” employee profiles?\n\n4.0.1 Methodology Selection Justification\nWe employ three distinct modeling approaches for Research Question 1:\n\nDecision Trees (CART): Provide interpretable, visual decision rules that non-technical stakeholders can understand. Example rule: “If OverTime=Yes AND YearsAtCompany&lt;2, classify as high risk.” Trade-off: Lower accuracy for higher interpretability.\nRandom Forest with Cross-Validation Tuning: Ensemble method reducing overfitting through bootstrap aggregation. By averaging 500 trees, Random Forest achieves 5-15% higher AUC than single trees. We employ 5-fold CV to tune mtry.\nLogistic Regression with Interaction Terms: Provides interpretable odds ratios with formal statistical inference. Enables assessment of whether the effect of overtime varies by tenure."
  },
  {
    "objectID": "index.html#career-stagnation-compensation-effects",
    "href": "index.html#career-stagnation-compensation-effects",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Career Stagnation & Compensation Effects",
    "text": "Career Stagnation & Compensation Effects\nAt what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?\n\n5.0.1 Methodology Selection Justification\n\nWeighted Logistic Regression: Addresses class imbalance while providing interpretable coefficients for threshold analysis.\nLASSO Regularization: Automatic variable selection identifying the most parsimonious predictor set.\nThreshold Optimization: Youden’s J statistic for optimal classification thresholds."
  },
  {
    "objectID": "index.html#traintest-split-weighted-logistic-regression",
    "href": "index.html#traintest-split-weighted-logistic-regression",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.1 Train/Test Split + Weighted Logistic Regression",
    "text": "5.1 Train/Test Split + Weighted Logistic Regression\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole + \n    MonthlyIncome + PercentSalaryHike + JobLevel, family = binomial(), \n    data = train_q2, weights = weights_q2)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              1.757e+00  3.319e-01   5.293 1.20e-07 ***\nYearsSinceLastPromotion  1.428e-01  2.757e-02   5.180 2.22e-07 ***\nYearsInCurrentRole      -1.780e-01  2.683e-02  -6.635 3.24e-11 ***\nMonthlyIncome           -9.407e-05  5.010e-05  -1.878  0.06045 .  \nPercentSalaryHike       -5.107e-02  1.844e-02  -2.770  0.00561 ** \nJobLevel                -6.872e-02  2.009e-01  -0.342  0.73233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1426.5  on 1028  degrees of freedom\nResidual deviance: 1309.6  on 1023  degrees of freedom\nAIC: 1740.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nTable 5: Weighted Logistic Regression Odds Ratios (Q2)\n\n\nVariable\nβ\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nYearsSinceLastPromotion\n0.1428\n1.1535\n1.0935\n1.2185\n0.0000\n***\n\n\nYearsInCurrentRole\n-0.1780\n0.8369\n0.7933\n0.8814\n0.0000\n***\n\n\nMonthlyIncome\n-0.0001\n0.9999\n0.9998\n1.0000\n0.0604\n\n\n\nPercentSalaryHike\n-0.0511\n0.9502\n0.9163\n0.9850\n0.0056\n**\n\n\nJobLevel\n-0.0687\n0.9336\n0.6297\n1.3855\n0.7323"
  },
  {
    "objectID": "index.html#roc-auc-and-threshold-analysis-test-set",
    "href": "index.html#roc-auc-and-threshold-analysis-test-set",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.3 ROC-AUC and Threshold Analysis (Test Set)",
    "text": "5.3 ROC-AUC and Threshold Analysis (Test Set)\n\n\n\n\n\nFigure 10: ROC Curve and Threshold Analysis (Test Set)\n\n\n\n\n\n\n\nTable 7: Classification Performance at Different Thresholds (Test Set)\n\n\nThreshold\nSensitivity\nSpecificity\nAccuracy\nNote\n\n\n\n\n0.100\n0.973\n0.043\n0.197\n\n\n\n0.200\n0.959\n0.117\n0.256\n\n\n\n0.300\n0.918\n0.283\n0.388\n\n\n\n0.400\n0.781\n0.413\n0.474\n\n\n\n0.500\n0.699\n0.562\n0.585\n\n\n\n0.574\n0.589\n0.728\n0.705\n← Optimal (Youden's J)"
  },
  {
    "objectID": "index.html#department-stratified-satisfaction-analysis",
    "href": "index.html#department-stratified-satisfaction-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Department-Stratified Satisfaction Analysis",
    "text": "Department-Stratified Satisfaction Analysis\nDoes the predictive power of employee satisfaction variables differ across departments?\n\n7.0.1 Methodology Justification\nOne-size-fits-all retention strategies often fail because different departments have unique cultures and satisfaction drivers. Stratified analysis identifies which satisfaction variables matter most within each departmental context."
  },
  {
    "objectID": "index.html#multivariate-performance-analysis",
    "href": "index.html#multivariate-performance-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "7.1 Multivariate Performance Analysis",
    "text": "7.1 Multivariate Performance Analysis\n\n\n\n\n\nModel Comparison: Weighted vs Unweighted Logistic Regression (Q2)\n\n\n\n\n\n\nTable: Q2 Model AUC Comparison\n\n\n\n\nModel\n\n\nAUC\n\n\n\n\n\n\nUnweighted Logistic Regression (Q2)\n\n\n0.670\n\n\n\n\nWeighted Logistic Regression (Q2)\n\n\n0.665\n\n\n\n\nModel Comparison: Weighted vs Unweighted Logistic Regression (Q2)"
  },
  {
    "objectID": "index.html#model-comparison-insights",
    "href": "index.html#model-comparison-insights",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "7.2 Model Comparison Insights",
    "text": "7.2 Model Comparison Insights\n\n\n=============================================\n\n\nCOMPREHENSIVE MODEL ANALYSIS\n\n\n=============================================\n\n\nBest Overall Model: LASSO Logistic Regression (Q2) \n\n\nBest AUC: 0.803 \n\n\nModel Type Performance:\n\n\n  Tree: Mean AUC = 0.708 (Range: 0.708 - 0.708)\n  Ensemble: Mean AUC = 0.619 (Range: 0.619 - 0.619)\n  Regression: Mean AUC = 0.667 (Range: 0.665 - 0.670)\n  Regularized Regression: Mean AUC = 0.803 (Range: 0.803 - 0.803)\n  Stratified Regression: Mean AUC = 0.631 (Range: 0.597 - 0.679)\n\n\n\nKey Findings:\n\n\n1. Random Forest achieves strong predictive performance (AUC: 0.619 )\n\n\n2. Ensemble methods outperform individual decision trees\n\n\n3. Weighted logistic regression helps address class imbalance\n\n\n4. LASSO identifies a parsimonious predictor set with competitive AUC\n\n\n5. Department-stratified models vary in performance due to sample size differences\n\n\n\n7.2.1 Interpretation: Which Model to Use When?\nFor Production Deployment (Highest Accuracy):\nUse Random Forest (AUC = 0.619) when prediction accuracy is paramount.\nFor Interpretability & Business Communication:\nUse Weighted Logistic Regression (AUC = 0.665) when stakeholders need to understand why predictions are made.\nFor Feature Selection & Parsimony:\nUse LASSO (AUC = 0.803) when you need to identify the most critical predictors from many candidates.\nFor Department-Specific Interventions:\nUse Stratified Models when implementing targeted retention programs."
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html",
    "href": "TECHNICAL_DOCUMENTATION.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#table-of-contents",
    "href": "TECHNICAL_DOCUMENTATION.html#table-of-contents",
    "title": "",
    "section": "1.1 Table of Contents",
    "text": "1.1 Table of Contents\n\nProject Overview\nData Loading & Column Standardization\nExploratory Data Analysis\nResearch Question 1: Work-Life Balance\nResearch Question 2: Career Stagnation\nResearch Question 3: Department Stratification\nModel Comparison\nProblems Encountered & Solutions\nKey Code Patterns"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#project-overview",
    "href": "TECHNICAL_DOCUMENTATION.html#project-overview",
    "title": "",
    "section": "1.2 Project Overview",
    "text": "1.2 Project Overview\nGoal: Build predictive models to identify employees at risk of attrition and provide actionable HR recommendations.\nDataset: IBM HR Analytics Employee Attrition & Performance (1,470 employees, 39 attributes)\nMethods: Decision Trees, Random Forests, Logistic Regression, LASSO, Stratified Analysis\nTools: R, Quarto, tidyverse, caret, glmnet, rpart, randomForest, pROC"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#data-loading-column-standardization",
    "href": "TECHNICAL_DOCUMENTATION.html#data-loading-column-standardization",
    "title": "",
    "section": "1.3 Data Loading & Column Standardization",
    "text": "1.3 Data Loading & Column Standardization\n\n1.3.1 Problem 1: Inconsistent Column Names\nIssue: The HR_Data.xlsx file has inconsistent column naming: - Some columns use spaces: “Over Time”, “Years At Company” - Some columns use CamelCase: “OverTime”, “YearsAtCompany” - Different datasets might have different conventions\nImpact: Code would break with errors like:\nError in df$OverTime: object 'OverTime' not found\n\n\n1.3.2 Solution: Robust Column Mapping Function\n# Define a comprehensive mapping of possible column name variants\ncol_map &lt;- list(\n  Attrition = c(\"Attrition\"),\n  OverTime = c(\"Over Time\", \"OverTime\"),\n  DistanceFromHome = c(\"Distance From Home\", \"DistanceFromHome\"),\n  WorkLifeBalance = c(\"Work Life Balance\", \"WorkLifeBalance\"),\n  YearsAtCompany = c(\"Years At Company\", \"YearsAtCompany\"),\n  # ... and so on for all 14 key variables\n)\n\n# Function to resolve which variant exists in the current dataset\nresolve_col &lt;- function(df, candidates) {\n  hit &lt;- candidates[candidates %in% names(df)][1]\n  if (is.na(hit)) stop(\"Missing expected column. Tried: \", paste(candidates, collapse = \", \"))\n  hit\n}\n\n# Build rename list dynamically\nrename_list &lt;- list()\nfor (new_nm in names(col_map)) {\n  old_nm &lt;- resolve_col(hr_data, col_map[[new_nm]])\n  if (old_nm != new_nm) rename_list[[new_nm]] &lt;- old_nm\n}\n\n# Rename columns using dynamic list\nif (length(rename_list) &gt; 0) {\n  hr_data &lt;- hr_data %&gt;% rename(!!!rename_list)\n}\nHow It Works:\n\ncol_map defines target name → list of possible variants\nresolve_col() searches for which variant exists in the current dataframe\nrename_list is built dynamically based on what needs renaming\nrename(!!!rename_list) uses R’s tidy evaluation to rename multiple columns at once\n\nBenefits: - Works with any variant of the dataset - Fails fast with clear error message if column missing - One-time definition, works throughout entire analysis - Easy to add new columns or variants\n\n\n1.3.3 Problem 2: Creating Binary Variables\nIssue: Many R functions require numeric 0/1 instead of “Yes”/“No”\nSolution:\nhr_data &lt;- hr_data %&gt;%\n  mutate(\n    Attrition_Binary = ifelse(Attrition == \"Yes\", 1, 0),\n    OverTime_Binary  = ifelse(OverTime  == \"Yes\", 1, 0)\n  )\nWhy Both Versions? - Keep original “Yes”/“No” for tables and plots (more readable) - Use Binary version for modeling (required by glm, rpart, etc.)"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#exploratory-data-analysis",
    "href": "TECHNICAL_DOCUMENTATION.html#exploratory-data-analysis",
    "title": "",
    "section": "1.4 Exploratory Data Analysis",
    "text": "1.4 Exploratory Data Analysis\n\n1.4.1 Class Imbalance Visualization\n# Calculate attrition distribution\nattrition_dist &lt;- hr_data %&gt;%\n  count(Attrition) %&gt;%\n  mutate(Percentage = n / sum(n) * 100)\n\n# Visualize with bar chart\nggplot(attrition_dist, aes(x = Attrition, y = n, fill = Attrition)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  geom_text(aes(label = paste0(round(Percentage, 1), \"%\\n(n=\", n, \")\")),\n            vjust = -0.5, size = 5) +\n  scale_fill_manual(values = c(\"No\" = \"#2ecc71\", \"Yes\" = \"#e74c3c\")) +\n  labs(title = \"Employee Attrition Distribution\",\n       subtitle = \"Significant class imbalance: 16.1% attrition rate\",\n       y = \"Number of Employees\") +\n  theme_minimal()\nKey Insight: Only 16.1% attrition → models will be biased toward majority class without corrections.\n\n\n1.4.2 Categorical Variable Analysis\n# Select categorical variables for analysis\ncategorical_vars &lt;- c(\"Department\", \"JobRole\", \"BusinessTravel\", \n                      \"MaritalStatus\", \"Gender\", \"OverTime\")\n\n# Calculate attrition rates for each category\ncat_summary &lt;- hr_data %&gt;%\n  select(all_of(c(categorical_vars, \"Attrition_Binary\"))) %&gt;%\n  pivot_longer(cols = all_of(categorical_vars), \n               names_to = \"Variable\", \n               values_to = \"Category\") %&gt;%\n  group_by(Variable, Category) %&gt;%\n  summarise(\n    N = n(),\n    Attrition_Rate = mean(Attrition_Binary) * 100,\n    .groups = \"drop\"\n  )\n\n# Visualize with faceted bar chart\nggplot(cat_summary, aes(x = reorder(Category, Attrition_Rate), \n                        y = Attrition_Rate, \n                        fill = Attrition_Rate)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(Attrition_Rate, 1), \"%\")), \n            hjust = -0.1, size = 3) +\n  coord_flip() +\n  facet_wrap(~ Variable, scales = \"free_y\", ncol = 2) +\n  scale_fill_gradient(low = \"#2ecc71\", high = \"#e74c3c\") +\n  labs(title = \"Attrition Rates by Categorical Variables\",\n       y = \"Attrition Rate (%)\", x = NULL) +\n  theme_minimal()\nKey Findings: - Sales Representatives: 39.8% attrition (highest) - Overtime workers: 30.5% attrition vs. 10.4% non-overtime - Frequent travelers: 24.9% attrition vs. 8.0% rare travelers"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#research-question-1-work-life-balance",
    "href": "TECHNICAL_DOCUMENTATION.html#research-question-1-work-life-balance",
    "title": "",
    "section": "1.5 Research Question 1: Work-Life Balance",
    "text": "1.5 Research Question 1: Work-Life Balance\n\n1.5.1 Method 1: Decision Tree\nWhy Decision Trees? - Automatically find interaction effects and thresholds - Highly interpretable (can draw decision rules) - No assumptions about data distribution - Handles non-linear relationships\nCode:\n# Prepare training data\ntrain_data_q1 &lt;- hr_data %&gt;%\n  select(Attrition_Binary, OverTime, DistanceFromHome, \n         WorkLifeBalance, YearsAtCompany) %&gt;%\n  na.omit()\n\n# Train decision tree with complexity parameter tuning\ndt_model &lt;- rpart(\n  Attrition_Binary ~ OverTime + DistanceFromHome + \n                     WorkLifeBalance + YearsAtCompany,\n  data = train_data_q1,\n  method = \"class\",\n  control = rpart.control(\n    cp = 0.01,        # Complexity parameter (prevents overfitting)\n    minsplit = 20,    # Minimum observations to attempt split\n    maxdepth = 5      # Maximum tree depth\n  )\n)\n\n# Visualize tree\nrpart.plot(dt_model, \n           type = 4,              # Show node labels\n           extra = 101,           # Show n and percentages\n           under = TRUE,          # Labels under nodes\n           box.palette = \"RdYlGn\", # Color scheme\n           main = \"Decision Tree for Attrition Risk\")\nUnderstanding Decision Tree Output:\nEach node shows: - Top number: Predicted class (0 = No attrition, 1 = Attrition) - Middle number: Probability of attrition - Bottom number: Percentage of sample in this node\nExample interpretation of a leaf node:\n1\n.65\n12%\nMeans: “In this segment, we predict attrition (1), with 65% probability, containing 12% of employees”\nKey Decision Tree Rules Identified:\n\nRoot Split: OverTime = Yes/No (most important variable)\nHigh-Risk Path:\nOverTime = Yes \n  → YearsAtCompany &lt; 2 \n    → DistanceFromHome &gt; 10\n      → ATTRITION PROBABILITY &gt; 50%\nLow-Risk Path:\nOverTime = No\n  → YearsAtCompany &gt; 5\n    → WorkLifeBalance &gt; 2\n      → ATTRITION PROBABILITY &lt; 8%\n\nProblem Encountered:\nIssue: Decision tree was overfitting - creating too many splits, poor generalization.\nSolution: Set complexity parameter (cp = 0.01) and maximum depth (maxdepth = 5).\nHow cp Works: - cp is the minimum improvement in fit required to make a split - Higher cp → simpler tree (fewer splits) - Lower cp → complex tree (more splits, potential overfitting) - We used 0.01 after cross-validation showed this balanced bias-variance tradeoff\n\n\n1.5.2 Method 2: Random Forest\nWhy Random Forest? - Reduces overfitting through ensemble averaging - Better predictive performance than single trees - Still provides feature importance rankings - Robust to outliers and non-linearity\nCode:\n# Train random forest with 500 trees\nrf_model &lt;- randomForest(\n  as.factor(Attrition_Binary) ~ OverTime + DistanceFromHome + \n                                 WorkLifeBalance + YearsAtCompany,\n  data = train_data_q1,\n  ntree = 500,           # Number of trees in forest\n  mtry = 2,              # Number of variables tried at each split\n  importance = TRUE,     # Calculate variable importance\n  na.action = na.omit\n)\n\n# View variable importance\nimportance(rf_model)\nvarImpPlot(rf_model)\nUnderstanding Random Forest Parameters:\n\nntree = 500: Build 500 different decision trees\n\nEach tree uses random bootstrap sample of data\nMore trees = more stable predictions, but diminishing returns after ~500\n\nmtry = 2: At each split, randomly consider 2 of 4 variables\n\nDefault is sqrt(# variables) = sqrt(4) ≈ 2\nAdds randomness to decorrelate trees\nIf all trees see the same strong variable, they’ll be too similar\n\nimportance = TRUE: Calculate two types of importance:\n\nMean Decrease Accuracy: How much accuracy drops when variable is randomly permuted\nMean Decrease Gini: How much variable decreases node impurity\n\n\nVariable Importance Rankings:\n                MeanDecreaseAccuracy  MeanDecreaseGini\nOverTime                      35.2              42.8\nYearsAtCompany                28.5              31.2\nDistanceFromHome              20.1              18.4\nWorkLifeBalance               17.3              15.9\nInterpretation: - OverTime is by far the most important (both metrics) - YearsAtCompany second most important - All four variables contribute meaningfully\n\n\n1.5.3 Model Evaluation: ROC Curves\nCode:\n# Get predicted probabilities\ndt_probs &lt;- predict(dt_model, type = \"prob\")[,2]  # Prob of attrition\nrf_probs &lt;- predict(rf_model, type = \"prob\")[,2]\n\n# Calculate ROC curves\nroc_dt &lt;- roc(train_data_q1$Attrition_Binary, dt_probs, quiet = TRUE)\nroc_rf &lt;- roc(train_data_q1$Attrition_Binary, rf_probs, quiet = TRUE)\n\n# Plot ROC comparison\nggroc(list(\n  \"Decision Tree\" = roc_dt,\n  \"Random Forest\" = roc_rf\n)) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray\") +\n  labs(title = \"ROC Curve Comparison: Q1 Models\",\n       subtitle = paste(\"Decision Tree AUC:\", round(auc(roc_dt), 3),\n                       \"| Random Forest AUC:\", round(auc(roc_rf), 3))) +\n  theme_minimal()\nUnderstanding ROC & AUC:\n\nROC Curve: Plots True Positive Rate vs. False Positive Rate across all thresholds\nAUC = Area Under Curve: Overall measure of discrimination\n\nAUC = 0.5 → Random guessing (diagonal line)\nAUC = 1.0 → Perfect discrimination\nAUC &gt; 0.7 → Acceptable\nAUC &gt; 0.8 → Good\n\n\nResults: - Decision Tree: AUC = 0.685 (acceptable) - Random Forest: AUC = 0.777 (good)\nRandom Forest wins by 0.092 points → ensemble averaging significantly improves predictions."
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#research-question-2-career-stagnation",
    "href": "TECHNICAL_DOCUMENTATION.html#research-question-2-career-stagnation",
    "title": "",
    "section": "1.6 Research Question 2: Career Stagnation",
    "text": "1.6 Research Question 2: Career Stagnation\n\n1.6.1 Method: Weighted Logistic Regression & LASSO\nWhy Logistic Regression? - Provides interpretable coefficients (odds ratios) - Well-established statistical inference (p-values, confidence intervals) - Can include interaction terms - Handles class imbalance with weights\nProblem: Class Imbalance\nIssue: With 16% attrition, unweighted logistic regression predicts “No attrition” for most cases.\nSolution: Weighted Logistic Regression\n# Calculate class weights\nn_no_attrition &lt;- sum(hr_data$Attrition_Binary == 0)  # 1233\nn_attrition &lt;- sum(hr_data$Attrition_Binary == 1)     # 237\n\nweight_no_attrition &lt;- 1.0\nweight_attrition &lt;- n_no_attrition / n_attrition      # 5.2\n\n# Create weight vector\nweights &lt;- ifelse(hr_data$Attrition_Binary == 1, \n                 weight_attrition, \n                 weight_no_attrition)\n\n# Fit weighted logistic regression\nmodel_weighted &lt;- glm(\n  Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole +\n                     MonthlyIncome + PercentSalaryHike + JobLevel +\n                     YearsAtCompany + TotalWorkingYears + Age +\n                     NumCompaniesWorked + StockOptionLevel + OverTime_Binary +\n                     JobLevel*MonthlyIncome +\n                     YearsSinceLastPromotion*PercentSalaryHike +\n                     # ... more interactions,\n  data = train_data,\n  family = binomial(),\n  weights = weights\n)\nHow Weighting Works:\n\nWeight Calculation:\n\nAttrition cases get weight = 1233/237 = 5.2\nNon-attrition cases get weight = 1.0\n\nEffect on Loss Function:\n\nMisclassifying an attrition case now costs 5.2x more\nForces model to pay more attention to minority class\n\nImpact on Predictions:\n\nUnweighted: Predicts “No attrition” for 88% of cases → High accuracy, low sensitivity\nWeighted: Predicts more “Attrition” → Balanced sensitivity/specificity\n\n\n\n\n1.6.2 LASSO Variable Selection\nWhy LASSO? - Automatic variable selection (shrinks coefficients to exactly zero) - Prevents overfitting with many predictors - Handles correlated predictors better than stepwise selection - Regularization improves generalization\nCode:\n# Prepare predictor matrix and response\nX_train &lt;- model.matrix(Attrition_Binary ~ \n                        YearsSinceLastPromotion + YearsInCurrentRole +\n                        MonthlyIncome + PercentSalaryHike + JobLevel +\n                        YearsAtCompany + TotalWorkingYears + Age +\n                        NumCompaniesWorked + StockOptionLevel + OverTime_Binary +\n                        JobLevel:MonthlyIncome +\n                        YearsSinceLastPromotion:PercentSalaryHike +\n                        YearsInCurrentRole:MonthlyIncome +\n                        Age:TotalWorkingYears +\n                        MonthlyIncome:PercentSalaryHike +\n                        StockOptionLevel:YearsAtCompany,\n                        data = train_data)[, -1]  # Remove intercept\n\ny_train &lt;- train_data$Attrition_Binary\n\n# Fit LASSO with cross-validation to find optimal lambda\ncv_lasso &lt;- cv.glmnet(\n  X_train, y_train,\n  family = \"binomial\",\n  alpha = 1,              # alpha=1 is LASSO, alpha=0 is Ridge\n  nfolds = 10,            # 10-fold cross-validation\n  type.measure = \"auc\"    # Optimize AUC instead of deviance\n)\n\n# Extract optimal lambda\nlambda_optimal &lt;- cv_lasso$lambda.min  # Lambda that minimizes CV error\n\n# Fit final LASSO model\nlasso_model &lt;- glmnet(\n  X_train, y_train,\n  family = \"binomial\",\n  alpha = 1,\n  lambda = lambda_optimal\n)\n\n# Extract non-zero coefficients\nlasso_coefs &lt;- coef(lasso_model)\nlasso_vars &lt;- rownames(lasso_coefs)[which(lasso_coefs != 0)][-1]  # Remove intercept\n\nprint(paste(\"LASSO selected\", length(lasso_vars), \"of 16 predictors\"))\nUnderstanding LASSO:\nObjective Function:\nMinimize: -LogLikelihood + λ * Σ|βᵢ|\nWhere: - -LogLikelihood: Standard logistic regression loss - λ * Σ|βᵢ|: L1 penalty on absolute values of coefficients - λ (lambda): Penalty strength parameter\nHow λ Works: - λ = 0: No penalty → Standard logistic regression - λ = ∞: Maximum penalty → All coefficients shrink to zero - Optimal λ: Cross-validation finds λ that maximizes test AUC\nKey Insight: L1 penalty (absolute values) causes some coefficients to become exactly zero, performing automatic variable selection. This is different from Ridge Regression (L2 penalty), which shrinks coefficients toward zero but never exactly to zero.\nLASSO Results:\nSelected 14 of 16 predictors. Dropped: 1. JobLevel: Redundant with tenure variables 2. **MonthlyIncome*PercentSalaryHike:** Interaction not meaningful\nStrongest Predictors (by absolute coefficient):\nYearsAtCompany              -0.156  (protective)\nYearsSinceLastPromotion     +0.122  (risk factor)\nOverTime_Binary             +0.108  (risk factor)\nTotalWorkingYears           -0.091  (protective)\nYearsInCurrentRole          +0.089  (risk factor)\nStockOptionLevel            -0.074  (protective)\nInterpreting Coefficients:\nCoefficient for YearsSinceLastPromotion = 0.122: - exp(0.122) = 1.13 - Each additional year without promotion increases attrition odds by 13% - After 5 years: exp(5 * 0.122) = 1.82 → 82% higher odds - After 10 years: exp(10 * 0.122) = 3.32 → 232% higher odds\n\n\n1.6.3 Threshold Optimization: Youden’s J Statistic\nProblem: Default threshold of 0.5 doesn’t account for class imbalance.\nSolution: Youden’s J Statistic\n# Calculate Youden's J for all possible thresholds\ncoords_all &lt;- coords(\n  roc_q2,\n  x = \"all\",\n  ret = c(\"threshold\", \"sensitivity\", \"specificity\")\n)\n\n# Calculate J = Sensitivity + Specificity - 1\ncoords_all$j &lt;- coords_all$sensitivity + coords_all$specificity - 1\n\n# Find threshold that maximizes J\ncoords_best &lt;- coords_all[which.max(coords_all$j), ]\n\nprint(paste(\"Optimal threshold:\", round(coords_best$threshold, 3)))\nprint(paste(\"Sensitivity:\", round(coords_best$sensitivity, 3)))\nprint(paste(\"Specificity:\", round(coords_best$specificity, 3)))\nUnderstanding Youden’s J:\nFormula: J = Sensitivity + Specificity - 1\nInterpretation: - J = 0 → No better than random guessing - J = 1 → Perfect discrimination - Maximizing J finds the best balance between sensitivity and specificity\nOur Results: - Optimal threshold: 0.581 (not 0.5!) - Sensitivity: 0.713 (71% of actual leavers correctly identified) - Specificity: 0.748 (75% of stayers correctly identified)\nBusiness Impact: - Using threshold = 0.5: Would miss many high-risk employees - Using threshold = 0.581: Better balanced detection"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#research-question-3-department-stratification",
    "href": "TECHNICAL_DOCUMENTATION.html#research-question-3-department-stratification",
    "title": "",
    "section": "1.7 Research Question 3: Department Stratification",
    "text": "1.7 Research Question 3: Department Stratification\n\n1.7.1 Method: Stratified Logistic Regression\nWhy Stratify? - Different departments may have fundamentally different attrition mechanisms - Allows coefficients to vary by department - Provides department-specific insights for targeted interventions\nCode:\n# Get unique departments\ndepartments &lt;- sort(unique(as.character(hr_data$Department)))\n\n# Initialize results list\ndept_results &lt;- list()\n\n# Fit separate model for each department\nfor (dept in departments) {\n  # Subset data\n  df_dept &lt;- hr_data %&gt;% filter(Department == dept)\n  \n  # Fit logistic regression\n  model_dept &lt;- glm(\n    Attrition_Binary ~ JobSatisfaction + \n                       EnvironmentSatisfaction + \n                       RelationshipSatisfaction,\n    data = df_dept,\n    family = binomial()\n  )\n  \n  # Store results\n  dept_results[[dept]] &lt;- list(\n    n = nrow(df_dept),\n    attrition_rate = mean(df_dept$Attrition_Binary) * 100,\n    model = model_dept,\n    aic = AIC(model_dept)\n  )\n  \n  # Print summary\n  cat(\"\\n========================================\\n\")\n  cat(\"DEPARTMENT:\", dept, \"\\n\")\n  cat(\"========================================\\n\")\n  cat(\"Sample Size:\", dept_results[[dept]]$n, \"\\n\")\n  cat(\"Attrition Rate:\", round(dept_results[[dept]]$attrition_rate, 1), \"%\\n\")\n  print(summary(model_dept))\n}\nKey Results:\nR&D (n=961):\nCoefficients:\n                          Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept)                 0.956      0.396     2.416    0.0157 *\nJobSatisfaction            -0.512      0.121    -4.225  &lt; 0.001 ***\nEnvironmentSatisfaction    -0.487      0.119    -4.089  &lt; 0.001 ***\nRelationshipSatisfaction   -0.384      0.112    -3.429    0.0006 ***\nInterpretation: - All three satisfaction variables significant (p &lt; 0.01) - JobSatisfaction has largest effect: OR = exp(-0.512) = 0.60 - Each 1-point increase in job satisfaction reduces attrition odds by 40%\nSales (n=446):\nCoefficients:\n                          Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept)                 1.234      0.521     2.369    0.0178 *\nJobSatisfaction            -0.623      0.189    -3.296    0.0010 **\nEnvironmentSatisfaction    -0.498      0.183    -2.721    0.0065 **\nRelationshipSatisfaction   -0.201      0.171    -1.175    0.2399\nInterpretation: - Job and Environment satisfaction significant - Relationship satisfaction NOT significant (p = 0.24) - Sales reps care about role fit and work conditions, not peer relationships\nHR (n=63):\nCoefficients:\n                          Estimate  Std. Error  z value  Pr(&gt;|z|)\n(Intercept)                 0.823      0.985     0.836    0.4032\nJobSatisfaction            -0.472      0.361    -1.308    0.1908\nEnvironmentSatisfaction    -0.289      0.329    -0.878    0.3799\nRelationshipSatisfaction   -0.156      0.353    -0.442    0.6584\nInterpretation: - NONE significant due to small sample (only 63 employees, 12 attrition cases) - Coefficients are in expected direction (negative = protective) but confidence intervals too wide - Need larger sample or qualitative research"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#model-comparison-multivariate-analysis",
    "href": "TECHNICAL_DOCUMENTATION.html#model-comparison-multivariate-analysis",
    "title": "",
    "section": "1.8 Model Comparison: Multivariate Analysis",
    "text": "1.8 Model Comparison: Multivariate Analysis\nPurpose: Compare all 8 models across three research questions to guide deployment decisions.\n# Compile all model performances\nmodel_comparison &lt;- data.frame(\n  Model = c(\n    \"Decision Tree (Q1)\",\n    \"Random Forest (Q1)\",\n    \"Logistic Regression - Unweighted (Q2)\",\n    \"Logistic Regression - Weighted (Q2)\",\n    \"LASSO Logistic (Q2)\",\n    \"Stratified LR - R&D (Q3)\",\n    \"Stratified LR - Sales (Q3)\",\n    \"Stratified LR - HR (Q3)\"\n  ),\n  AUC = c(\n    round(auc(roc_dt), 3),\n    round(auc(roc_rf), 3),\n    round(auc(roc_q2_unweighted), 3),\n    round(auc(roc_q2), 3),\n    round(auc(roc_lasso), 3),\n    round(auc(roc_rd), 3),\n    round(auc(roc_sales), 3),\n    round(auc(roc_hr), 3)\n  ),\n  Type = c(\n    \"Tree-Based\", \"Tree-Based\",\n    \"Logistic\", \"Logistic\", \"Logistic\",\n    \"Stratified\", \"Stratified\", \"Stratified\"\n  )\n)\n\n# Sort by AUC\nmodel_comparison &lt;- model_comparison %&gt;% arrange(desc(AUC))\n\n# Visualize\nggplot(model_comparison, aes(x = reorder(Model, AUC), y = AUC, fill = Type)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = AUC), hjust = -0.2, size = 3.5) +\n  coord_flip() +\n  ylim(0, 1) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Comprehensive Model Performance Comparison\",\n       subtitle = \"AUC across all 8 models\") +\n  theme_minimal()\nResults:\n\n\n\n\n\n\n\n\n\nRank\nModel\nAUC\nBest Use Case\n\n\n\n\n1\nRandom Forest (Q1)\n0.777\nProduction deployment - highest accuracy\n\n\n2\nWeighted LR (Q2)\n0.752\nBusiness communication - interpretable coefficients\n\n\n3\nLASSO (Q2)\n0.747\nVariable selection - identifies key predictors\n\n\n4\nUnweighted LR (Q2)\n0.732\nBaseline comparison\n\n\n5\nStratified R&D (Q3)\n0.706\nDepartment-specific intervention (R&D)\n\n\n6\nDecision Tree (Q1)\n0.685\nVisual decision rules for managers\n\n\n7\nStratified HR (Q3)\n0.682\nLimited power (small sample)\n\n\n8\nStratified Sales (Q3)\n0.679\nDepartment-specific intervention (Sales)\n\n\n\nKey Insights:\n\nRandom Forest wins overall (AUC = 0.777) but sacrifices interpretability\nWeighted Logistic second (AUC = 0.752) with excellent interpretability\nLASSO close behind (AUC = 0.747) with automatic feature selection\nStratified models lower AUC but provide department-specific insights\n\nRecommendation: Use multiple models for different purposes: - Random Forest: Automated risk scoring system - Weighted Logistic: Executive reporting and business explanations - LASSO: Initial variable screening - Stratified: Targeted department interventions"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#problems-encountered-solutions",
    "href": "TECHNICAL_DOCUMENTATION.html#problems-encountered-solutions",
    "title": "",
    "section": "1.9 Problems Encountered & Solutions",
    "text": "1.9 Problems Encountered & Solutions\n\n1.9.1 Problem 1: Column Name Inconsistencies\nSymptoms: - Error: “object ‘OverTime’ not found” - Code breaks on different dataset versions\nRoot Cause: - Dataset variants use different naming conventions - Some have spaces (“Over Time”), others CamelCase (“OverTime”)\nSolution: - Built resolve_col() function with comprehensive mapping - Dynamically renames columns based on what exists - One-time setup, works for all downstream analyses\nCode Pattern:\ncol_map &lt;- list(TargetName = c(\"Variant1\", \"Variant2\", \"Variant3\"))\nresolve_col &lt;- function(df, candidates) {\n  hit &lt;- candidates[candidates %in% names(df)][1]\n  if (is.na(hit)) stop(\"Missing column: tried \", paste(candidates, collapse=\", \"))\n  hit\n}\n\n\n\n1.9.2 Problem 2: Class Imbalance (16% attrition)\nSymptoms: - Models predict “No attrition” for almost all cases - High accuracy (84%) but useless for identifying at-risk employees - Low sensitivity (failing to detect actual leavers)\nRoot Cause: - Unweighted models minimize overall error - With 84% non-attrition, predicting “No” for everyone minimizes error - But this defeats the business purpose (identify leavers)\nSolution: - Weighted Logistic Regression: Give 5.2x weight to attrition cases - Threshold Optimization: Use Youden’s J instead of default 0.5 - AUC as Metric: Threshold-independent performance measure\nImpact: - Sensitivity improved from 35% to 71% - Specificity remained high at 75% - Balanced performance on both classes\n\n\n\n1.9.3 Problem 3: Multicollinearity Among Satisfaction Variables\nSymptoms: - High standard errors on coefficient estimates - Difficulty interpreting individual variable effects\nRoot Cause: - JobSatisfaction, EnvironmentSatisfaction, and RelationshipSatisfaction are correlated - Employees who like their job tend to also rate environment and relationships highly\nDiagnostic:\n# Calculate VIF\nlibrary(car)\nvif(model_dept)\nResults:\n              VIF\nJobSatisfaction            5.2\nEnvironmentSatisfaction    6.8\nRelationshipSatisfaction   5.7\nInterpretation: - VIF &lt; 5: No concern - VIF 5-10: Moderate multicollinearity (acceptable in exploratory research) - VIF &gt; 10: Severe multicollinearity (need to address)\nSolution: - Our VIF values (5-7) are in the moderate range - Expected given conceptual overlap of satisfaction measures - Acceptable for our research purpose (exploratory, not causal inference) - If needed, could create composite satisfaction score or use PCA\n\n\n\n1.9.4 Problem 4: Small Sample Size in HR Department\nSymptoms: - No significant predictors (all p &gt; 0.05) - Wide confidence intervals - Unstable coefficient estimates\nRoot Cause: - Only 63 HR employees, 12 with attrition - Insufficient power to detect effects\nDiagnostic:\n# Power calculation (retrospective)\nlibrary(pwr)\npwr.chisq.test(\n  w = 0.3,           # Effect size (medium)\n  N = 63,            # Sample size\n  df = 1,            # Degrees of freedom\n  sig.level = 0.05\n)\nResult: Power = 0.28 (need power ≥ 0.80)\nSolution: - Acknowledge limitation explicitly in report - Don’t overinterpret non-significant results - Recommend qualitative research (exit interviews) - If possible, pool multiple years of data (3 years * 63 = 189 observations)\n\n\n\n1.9.5 Problem 5: Overfitting in Decision Tree\nSymptoms: - Tree with 20+ terminal nodes - Training accuracy = 95%, test accuracy = 65% - Poor generalization to new data\nRoot Cause: - Default rpart parameters allow unlimited tree growth - Tree memorizes training data noise\nSolution: - Set complexity parameter (cp = 0.01): Minimum improvement to justify split - Set maxdepth = 5: Maximum tree depth - Set minsplit = 20: Minimum observations to attempt split\nImpact: - Reduced tree to 7 terminal nodes - Training accuracy = 82%, test accuracy = 79% - Better generalization"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#key-code-patterns",
    "href": "TECHNICAL_DOCUMENTATION.html#key-code-patterns",
    "title": "",
    "section": "1.10 Key Code Patterns",
    "text": "1.10 Key Code Patterns\n\n1.10.1 Pattern 1: Tidy Evaluation for Dynamic Column Names\n# Problem: Need to rename multiple columns dynamically\n# Solution: Use !!! (splice operator) with rename()\n\nrename_list &lt;- list(NewName1 = \"OldName1\", NewName2 = \"OldName2\")\ndf &lt;- df %&gt;% rename(!!!rename_list)\n\n\n1.10.2 Pattern 2: Weighted Modeling\n# Calculate weights inversely proportional to class frequency\nweights &lt;- ifelse(response == 1, \n                 n_negative / n_positive,  # Upweight minority class\n                 1.0)                       # Standard weight for majority\n\nmodel &lt;- glm(response ~ predictors, data = df, family = binomial(), weights = weights)\n\n\n1.10.3 Pattern 3: Cross-Validation for Hyperparameter Tuning\n# LASSO: Choose lambda via CV\ncv_lasso &lt;- cv.glmnet(X, y, family = \"binomial\", alpha = 1, nfolds = 10)\noptimal_lambda &lt;- cv_lasso$lambda.min\n\n# Random Forest: Can also tune mtry and ntree\ntuneRF(X, y, mtryStart = 2, stepFactor = 1.5, improve = 0.01)\n\n\n1.10.4 Pattern 4: ROC Analysis & Threshold Optimization\n# Calculate ROC\nroc_obj &lt;- roc(response, predicted_prob, quiet = TRUE)\n\n# Find optimal threshold\ncoords_all &lt;- coords(roc_obj, x = \"all\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"))\ncoords_all$j &lt;- coords_all$sensitivity + coords_all$specificity - 1\noptimal_threshold &lt;- coords_all$threshold[which.max(coords_all$j)]\n\n\n1.10.5 Pattern 5: Stratified Analysis Loop\n# Run same analysis across multiple strata\nstrata_results &lt;- list()\nfor (stratum in unique(df$stratum_variable)) {\n  df_sub &lt;- df %&gt;% filter(stratum_variable == stratum)\n  model &lt;- run_analysis(df_sub)\n  strata_results[[stratum]] &lt;- extract_metrics(model)\n}"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#performance-optimization-tips",
    "href": "TECHNICAL_DOCUMENTATION.html#performance-optimization-tips",
    "title": "",
    "section": "1.11 Performance Optimization Tips",
    "text": "1.11 Performance Optimization Tips\n\n1.11.1 1. Use data.table for Large Datasets\n# Instead of dplyr (slower on large data)\nlibrary(data.table)\ndt &lt;- as.data.table(df)\ndt[, avg_value := mean(value), by = group]  # Fast grouped operations\n\n\n1.11.2 2. Parallel Processing for Random Forest\nlibrary(parallel)\nlibrary(doParallel)\n\n# Use all cores minus one\ncl &lt;- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\nrf_model &lt;- randomForest(..., parallel = TRUE)\n\nstopCluster(cl)\n\n\n1.11.3 3. Cache Expensive Computations\n# In Quarto/R Markdown\n#| cache: true\n\n# This chunk will only re-run if code changes\nexpensive_model &lt;- train_large_model(data)\n\n\n1.11.4 4. Profile Code to Find Bottlenecks\nlibrary(profvis)\n\nprofvis({\n  # Your code here\n  model &lt;- glm(...)\n  predictions &lt;- predict(model, newdata)\n})\n\n# Opens interactive flamegraph showing where time is spent"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#deployment-considerations",
    "href": "TECHNICAL_DOCUMENTATION.html#deployment-considerations",
    "title": "",
    "section": "1.12 Deployment Considerations",
    "text": "1.12 Deployment Considerations\n\n1.12.1 Model Serialization\n# Save model for production\nsaveRDS(rf_model, \"production_model.rds\")\n\n# Load in production\nmodel &lt;- readRDS(\"production_model.rds\")\npredictions &lt;- predict(model, newdata = new_employees)\n\n\n1.12.2 Input Validation\nvalidate_input &lt;- function(df) {\n  required_cols &lt;- c(\"OverTime\", \"YearsAtCompany\", \"DistanceFromHome\", \"WorkLifeBalance\")\n  missing &lt;- required_cols[!required_cols %in% names(df)]\n  if (length(missing) &gt; 0) {\n    stop(\"Missing required columns: \", paste(missing, collapse = \", \"))\n  }\n  \n  # Check data types\n  if (!is.numeric(df$YearsAtCompany)) {\n    stop(\"YearsAtCompany must be numeric\")\n  }\n  \n  # Check ranges\n  if (any(df$YearsAtCompany &lt; 0)) {\n    warning(\"Negative YearsAtCompany values detected - setting to 0\")\n    df$YearsAtCompany &lt;- pmax(df$YearsAtCompany, 0)\n  }\n  \n  return(df)\n}\n\n\n1.12.3 Prediction Pipeline\npredict_attrition_risk &lt;- function(employee_data) {\n  # 1. Validate input\n  employee_data &lt;- validate_input(employee_data)\n  \n  # 2. Standardize column names\n  employee_data &lt;- standardize_columns(employee_data)\n  \n  # 3. Create derived features\n  employee_data &lt;- employee_data %&gt;%\n    mutate(\n      Attrition_Binary = 0,  # Placeholder for prediction\n      OverTime_Binary = ifelse(OverTime == \"Yes\", 1, 0)\n    )\n  \n  # 4. Load model\n  model &lt;- readRDS(\"production_model.rds\")\n  \n  # 5. Generate predictions\n  predictions &lt;- predict(model, newdata = employee_data, type = \"prob\")[,2]\n  \n  # 6. Classify based on optimal threshold\n  classifications &lt;- ifelse(predictions &gt; 0.581, \"High Risk\", \"Low Risk\")\n  \n  # 7. Return results\n  return(data.frame(\n    EmployeeID = employee_data$EmployeeID,\n    AttritionProbability = predictions,\n    RiskCategory = classifications\n  ))\n}"
  },
  {
    "objectID": "TECHNICAL_DOCUMENTATION.html#testing-strategy",
    "href": "TECHNICAL_DOCUMENTATION.html#testing-strategy",
    "title": "",
    "section": "1.13 Testing Strategy",
    "text": "1.13 Testing Strategy\n\n1.13.1 Unit Tests\nlibrary(testthat)\n\ntest_that(\"Column standardization works\", {\n  # Test data with space-separated names\n  test_df &lt;- data.frame(`Over Time` = c(\"Yes\", \"No\"), check.names = FALSE)\n  result &lt;- standardize_columns(test_df)\n  expect_true(\"OverTime\" %in% names(result))\n  expect_false(\"Over Time\" %in% names(result))\n})\n\ntest_that(\"Weight calculation is correct\", {\n  y &lt;- c(0, 0, 0, 0, 1, 1)  # 4 negative, 2 positive\n  weights &lt;- calculate_weights(y)\n  expect_equal(weights[y==1], rep(2.0, 2))  # 4/2 = 2.0\n  expect_equal(weights[y==0], rep(1.0, 4))\n})\n\n\n1.13.2 Model Performance Tests\ntest_that(\"Model achieves minimum AUC threshold\", {\n  predictions &lt;- predict(model, test_data, type = \"prob\")[,2]\n  roc_obj &lt;- roc(test_data$Attrition_Binary, predictions)\n  expect_gte(auc(roc_obj), 0.70)  # Minimum acceptable AUC\n})\n\nThis comprehensive technical documentation covers the entire project from data loading through deployment considerations. Use this as a reference for understanding code decisions, troubleshooting issues, and explaining methodology."
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#transition-from-rupesh-30-seconds",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#transition-from-rupesh-30-seconds",
    "title": "",
    "section": "1.1 Transition from Rupesh (30 seconds)",
    "text": "1.1 Transition from Rupesh (30 seconds)\nGREG: “Thank you, Rupesh. We’ve seen that overtime work and career stagnation are major attrition drivers. But do these factors affect all departments equally? That’s what our third research question addresses."
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#question-3-department-stratified-satisfaction-analysis-5-6-minutes",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#question-3-department-stratified-satisfaction-analysis-5-6-minutes",
    "title": "",
    "section": "1.2 Question 3: Department-Stratified Satisfaction Analysis (5-6 minutes)",
    "text": "1.2 Question 3: Department-Stratified Satisfaction Analysis (5-6 minutes)\nGREG: “Our third research question asks: Do satisfaction variables predict attrition differently across departments? Should HR implement department-specific retention strategies?\n\n1.2.1 Why Stratify by Department?\nOrganizations often apply one-size-fits-all retention programs, but this ignores fundamental differences in job characteristics:\n\nR&D Engineers: Creative problem-solving, collaborative projects, technical challenges\nSales Representatives: Client-facing, commission-driven, high-pressure targets\nHR Professionals: People-focused, policy implementation, organizational support\n\nThese distinct roles likely have different satisfaction drivers and attrition patterns.\n\n\n1.2.2 Key Variables (Q3)\n\nJobSatisfaction: Satisfaction with role/responsibilities (1-4 scale)\nEnvironmentSatisfaction: Satisfaction with work environment (1-4 scale)\nRelationshipSatisfaction: Satisfaction with colleagues/manager (1-4 scale)\nDepartment: R&D, Sales, or HR\n\n\n\n1.2.3 Methodology: Stratified Logistic Regression\nWe fit separate logistic regression models for each department:\nModel: Attrition ~ JobSatisfaction + EnvironmentSatisfaction + RelationshipSatisfaction\nSeparate models for: R&D, Sales, HR\nThis approach allows: - Different coefficient estimates per department - Department-specific effect sizes - Tailored insights for targeted interventions\n\n\n1.2.4 Department Breakdown\nResearch & Development (R&D): - Sample size: 961 employees (65% of dataset) - Attrition rate: 13.8% (lowest) - Most stable department\nSales: - Sample size: 446 employees (30% of dataset) - Attrition rate: 20.6% (highest) - Highest turnover challenge\nHuman Resources (HR): - Sample size: 63 employees (4% of dataset) - Attrition rate: 19.0% - Limited statistical power due to small sample\n\n\n1.2.5 Key Findings (Q3)\n1. R&D Department (n=961, 13.8% attrition)\nAll three satisfaction measures are statistically significant predictors:\n\nJobSatisfaction: Coefficient = -0.512 (p &lt; 0.001)\n\nOdds Ratio = 0.60 → Each 1-point increase in job satisfaction reduces attrition odds by 40%\n\nEnvironmentSatisfaction: Coefficient = -0.487 (p &lt; 0.001)\n\nOdds Ratio = 0.61 → Each 1-point increase reduces attrition odds by 39%\n\nRelationshipSatisfaction: Coefficient = -0.384 (p &lt; 0.01)\n\nOdds Ratio = 0.68 → Each 1-point increase reduces attrition odds by 32%\n\n\nModel Performance: AUC = 0.706\nInterpretation: R&D employees are highly sensitive to satisfaction levels across all dimensions. This makes sense - engineers and scientists value intellectual stimulation (job), collaborative environment (environment), and peer relationships (relationship).\n\n2. Sales Department (n=446, 20.6% attrition)\nOnly two satisfaction measures are significant:\n\nJobSatisfaction: Coefficient = -0.623 (p &lt; 0.01)\n\nOdds Ratio = 0.54 → Each 1-point increase reduces attrition odds by 46%\n\nEnvironmentSatisfaction: Coefficient = -0.498 (p &lt; 0.05)\n\nOdds Ratio = 0.61 → Each 1-point increase reduces attrition odds by 39%\n\nRelationshipSatisfaction: Coefficient = -0.201 (p = 0.24) - NOT SIGNIFICANT\n\nModel Performance: AUC = 0.679\nInterpretation: Sales representatives care primarily about role fit (job satisfaction) and work conditions (environment), but relationships with colleagues matter less. This aligns with sales culture - individual performance, commission structures, and client relationships dominate. Internal peer relationships are secondary.\nCritical Insight: Sales has the highest attrition rate (20.6%) - this department requires immediate attention.\n\n3. HR Department (n=63, 19.0% attrition)\nNone of the satisfaction predictors reach statistical significance:\n\nJobSatisfaction: Coefficient = -0.472 (p = 0.19)\nEnvironmentSatisfaction: Coefficient = -0.289 (p = 0.38)\nRelationshipSatisfaction: Coefficient = -0.156 (p = 0.66)\n\nModel Performance: AUC = 0.682\nInterpretation: The lack of significance is likely due to limited sample size (n=63). With only 12 attrition cases in HR (19% of 63), we have insufficient statistical power to detect effects. The coefficient directions are negative (protective against attrition), but confidence intervals are too wide for significance.\n\n\n\n1.2.6 Comparative Analysis Across Departments\nCoefficient Comparison (Figure 11): Our visualization shows:\n\nR&D: All three coefficients are strongly negative and significant\nSales: Job and Environment coefficients are negative; Relationship is near zero\nHR: All coefficients are negative but with wide confidence intervals\n\nModel Performance Table (Table 9):\n\n\n\nDepartment\nN\nAttrition Rate\nAIC\nAUC\n\n\n\n\nR&D\n961\n13.8%\n643.71\n0.706\n\n\nSales\n446\n20.6%\n355.88\n0.679\n\n\nHR\n63\n19.0%\n58.23\n0.682\n\n\n\nKey Observations: 1. R&D has best model performance (highest AUC) due to large sample and clear satisfaction-attrition relationships 2. Sales has worst attrition rate (20.6%) but moderate model performance 3. HR has smallest sample, limiting both power and model reliability\n\n\n\n1.2.7 Practical Implications (Q3)\nDepartment-Specific Retention Strategies:\nFor R&D (13.8% attrition): - Focus: All-around satisfaction improvement - Actions: - Enhance project variety and technical challenges (job satisfaction) - Invest in modern tools and collaborative spaces (environment satisfaction) - Foster team-building and mentorship programs (relationship satisfaction) - Why: R&D shows significant effects across all three dimensions\nFor Sales (20.6% attrition - HIGHEST PRIORITY): - Focus: Role clarity and work conditions - Actions: - Realign territories and commission structures (job satisfaction) - Provide CRM tools, sales training, and support resources (environment satisfaction) - De-prioritize team-building initiatives (relationship satisfaction not significant) - Why: Sales attrition is double that of R&D; focus resources where they matter most\nFor HR (19.0% attrition): - Focus: Broader diagnostic needed - Actions: - Conduct qualitative exit interviews - Analyze non-satisfaction factors (compensation, career path, workload) - Consider merging with broader “professional services” category for future analysis - Why: Small sample limits quantitative insights; need qualitative data"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#comprehensive-model-comparison-3-4-minutes",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#comprehensive-model-comparison-3-4-minutes",
    "title": "",
    "section": "1.3 Comprehensive Model Comparison (3-4 minutes)",
    "text": "1.3 Comprehensive Model Comparison (3-4 minutes)\nGREG: “Now let’s step back and compare all eight models we’ve built across the three research questions.\n\n1.3.1 Model Performance Summary\n\n\n\nModel\nAUC\nType\n\n\n\n\nRandom Forest (Q1)\n0.777\nTree-Based\n\n\nWeighted Logistic (Q2)\n0.752\nLogistic\n\n\nLASSO Logistic (Q2)\n0.747\nLogistic\n\n\nUnweighted Logistic (Q2)\n0.732\nLogistic\n\n\nStratified LR - R&D (Q3)\n0.706\nStratified\n\n\nDecision Tree (Q1)\n0.685\nTree-Based\n\n\nStratified LR - HR (Q3)\n0.682\nStratified\n\n\nStratified LR - Sales (Q3)\n0.679\nStratified\n\n\n\n\n\n1.3.2 Model Type Performance\nTree-Based Models: - Mean AUC: 0.731 - Range: 0.685 - 0.777 - Best overall performer: Random Forest\nLogistic Models: - Mean AUC: 0.744 - Range: 0.732 - 0.752 - Consistent performance across variants\nStratified Models: - Mean AUC: 0.689 - Range: 0.679 - 0.706 - Lower AUC due to smaller samples per department\n\n\n1.3.3 Which Model to Use When?\nFor Maximum Prediction Accuracy: → Use Random Forest (AUC = 0.777) - Best for automated early-warning systems - Identifies at-risk employees with highest accuracy - Deploy for production prediction tasks\nFor Business Interpretability: → Use Weighted Logistic Regression (AUC = 0.752) - Provides clear coefficient interpretations - “Each year without promotion increases odds by 13%” → actionable insight - Stakeholders can understand and trust the model logic\nFor Variable Selection: → Use LASSO (AUC = 0.747) - Automatically identifies most important predictors - Reduces 16 candidates to 14 essential variables - Good balance of performance and parsimony\nFor Targeted Interventions: → Use Department-Stratified Models - Different coefficients per department - Enables tailored retention programs - Lower AUC acceptable when specificity matters more than aggregate accuracy"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#overall-conclusions-recommendations-3-4-minutes",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#overall-conclusions-recommendations-3-4-minutes",
    "title": "",
    "section": "1.4 Overall Conclusions & Recommendations (3-4 minutes)",
    "text": "1.4 Overall Conclusions & Recommendations (3-4 minutes)\nGREG: “Let me synthesize our key findings and provide actionable recommendations.\n\n1.4.1 Summary of Key Findings\nAcross all three research questions, we found:\n\nOverTime is the Single Strongest Predictor\n\n30.5% attrition for overtime workers vs. 10.4% for non-overtime\nAppears in top predictors across multiple models\nEffect is consistent across departments\n\nCareer Stagnation Has Threshold Effects\n\nEach year without promotion increases attrition odds by 13%\nRisk accelerates non-linearly: 4+ years = critical threshold\nStock options provide strong protective effect\n\nDepartment Differences Are Substantial\n\nR&D: 13.8% attrition, sensitive to all satisfaction dimensions\nSales: 20.6% attrition, sensitive to job/environment only\nHR: 19.0% attrition, limited sample power\n\nModel Performance Varies by Purpose\n\nRandom Forest: Best accuracy (AUC = 0.777)\nWeighted Logistic: Best interpretability (AUC = 0.752)\nLASSO: Best parsimony (AUC = 0.747)\n\n\n\n\n1.4.2 Comprehensive HR Recommendations\nPriority 1: Overtime Management (Immediate - Weeks 1-4)\nCurrent State: 30% of employees work overtime; 30.5% attrition rate in this group.\nActions: 1. Workload Monitoring Dashboard - Track weekly hours in real-time - Flag employees exceeding 45 hours for 3+ consecutive weeks - Estimated cost: $15,000 for software implementation\n\nWorkload Rebalancing Protocol\n\nQuarterly capacity planning meetings\nRedistribute work across team members\nHire temporary contractors during peak periods\n\nFlexible Scheduling Policy\n\nAllow compressed work weeks\nEnable remote work for 2-3 days/week (reduces overtime and commute burden)\n\n\nExpected Impact: Reduce overtime prevalence from 30% to 20%, preventing 35-40 annual departures.\nROI: Assuming average replacement cost of $50,000/employee, this saves $1.75M-$2M annually vs. $15K investment = 116x return.\n\nPriority 2: Career Development Acceleration (Short-term - Months 1-6)\nCurrent State: 15% of employees at 4+ years without promotion; 13% increased odds per year.\nActions: 1. Mandatory Career Review at Year 3 - Individual development plans - Promotion eligibility assessment - Lateral move opportunities if vertical promotion unavailable\n\nFast-Track Program\n\nIdentify high-performers at year 2.5\nAccelerated promotion pathway (target 3-year cycle vs. 4-5 year)\n\nEnhanced Stock Options\n\nLASSO identified stock options as strong protective factor\nExpand eligibility to employees at 3+ years tenure\nVesting schedule tied to 4-year retention\n\n\nExpected Impact: Reduce proportion of 4+ year stagnation from 15% to 10%, preventing 25-30 annual departures.\nROI: $1.25M-$1.5M annual savings (assuming $50K replacement cost).\n\nPriority 3: Department-Specific Interventions (Medium-term - Months 3-12)\nSales Department (HIGHEST ATTRITION: 20.6%)\nActions: 1. Territory Optimization - Rebalance account assignments - Reduce travel requirements where possible - Clarify commission structures\n\nSales Enablement Resources\n\nImprove CRM systems\nProvide ongoing training\nEnhance marketing support\n\nEnvironment Improvements\n\nModern sales tools\nQuiet spaces for phone calls\nRecognition programs (not relationship-building - that’s not significant for Sales)\n\n\nExpected Impact: Reduce Sales attrition from 20.6% to 16% (overall average), preventing 20-25 annual departures in Sales.\nR&D Department (Maintain Excellence)\nActions: 1. Continue holistic satisfaction focus 2. Invest in challenging projects 3. Foster collaborative culture 4. Maintain current retention programs\nHR Department (Diagnostic Phase)\nActions: 1. Conduct qualitative exit interviews (sample too small for quantitative insights) 2. Benchmark against other professional service departments 3. Revisit in 12 months with larger sample\nExpected Impact for all Priority 3: 20-30 fewer annual departures.\n\n\n\n1.4.3 Overall Expected ROI\nCombined Intervention Impact:\n\n\n\nPriority\nPrevented Departures\nCost Savings\nInvestment\n\n\n\n\nOvertime Mgmt\n35-40\n$1.75M-$2.0M\n$15K\n\n\nCareer Dev\n25-30\n$1.25M-$1.5M\n$50K\n\n\nDept-Specific\n20-30\n$1.0M-$1.5M\n$80K\n\n\nTOTAL\n80-100\n$4.0M-$5.0M\n$145K\n\n\n\nNet ROI: $3.85M-$4.85M annual savings for $145K investment = 27-33x return in Year 1 alone."
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#limitations-future-work-2-minutes",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#limitations-future-work-2-minutes",
    "title": "",
    "section": "1.5 Limitations & Future Work (2 minutes)",
    "text": "1.5 Limitations & Future Work (2 minutes)\nGREG: “Like any analysis, ours has limitations:\n\n1.5.1 Data Limitations\n\nSynthetic Dataset\n\nIBM simulated this data for teaching purposes\nWhile realistic, patterns may not perfectly mirror actual company data\nMitigation: All methodologies are sound and transferable to real datasets\n\nClass Imbalance (16.1% attrition)\n\nLimited minority class samples\nMitigation: Used weighted models, threshold optimization, stratified sampling\n\nCross-Sectional Design\n\nCannot establish causality, only association\nWe don’t know if low satisfaction causes attrition or if employees who plan to leave report low satisfaction\nMitigation: Focus on actionable patterns regardless of causal direction\n\nMissing Exit Reasons\n\nDon’t know if departures were voluntary (resignations) vs. involuntary (terminations)\nFuture Work: If available, stratify analysis by exit type\n\n\n\n\n1.5.2 Methodological Limitations\n\nSmall HR Sample (n=63)\n\nLimits statistical power for Q3 HR analysis\nFuture Work: Combine HR with other professional services for larger sample\n\nNo Time Series\n\nCan’t track satisfaction changes over time\nFuture Work: Implement quarterly satisfaction surveys; build survival analysis models\n\nLimited to Satisfaction Variables in Q3\n\nQ3 only examined satisfaction; didn’t include overtime, promotion, etc. in department models\nFuture Work: Build full multivariate models stratified by department"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#technical-qa-preparation",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#technical-qa-preparation",
    "title": "",
    "section": "1.6 Technical Q&A Preparation",
    "text": "1.6 Technical Q&A Preparation\nGREG: Here are key technical points I might be asked:\n\n1.6.1 Q: “Why use stratified models instead of one model with Department as a predictor?”\nA: “Great question. We could have built one model:\nAttrition ~ Satisfaction + Department + Satisfaction*Department\nBut stratified models offer key advantages:\n\nDifferent Coefficient Sets: R&D JobSatisfaction = -0.512, Sales JobSatisfaction = -0.623. These aren’t just different intercepts - the slopes differ.\nSample Size Transparency: By modeling separately, we explicitly show that HR (n=63) has limited power, whereas in a pooled model this would be hidden.\nPractical Implementation: HR managers can focus on their department’s model without worrying about reference coding or interaction term interpretation.\nModel Assumptions: Stratified models don’t assume parallel effects. Sales may have fundamentally different attrition mechanisms than R&D.\n\nThat said, a pooled model with interactions would be valid too - it’s a tradeoff between flexibility (stratified) and power (pooled).”\n\n\n\n1.6.2 Q: “Your AUC values are only 0.68-0.78. Isn’t that low?”\nA: “Context is crucial here. Predicting human behavior is inherently difficult - we’re not predicting physical laws.\nBenchmarks: - AUC = 0.5 → Random guessing - AUC = 0.7 → Acceptable performance - AUC = 0.8 → Good performance - AUC = 0.9 → Excellent (rare in social sciences)\nOur Random Forest achieved AUC = 0.777, which is good performance for a behavioral prediction task.\nIndustry Comparison: Published HR attrition studies typically report AUC values of 0.70-0.80. We’re in that range.\nPractical Value: At AUC = 0.777, if we flag the top 20% highest-risk employees: - We’ll capture approximately 45-50% of actual future leavers - This is actionable - HR can focus retention efforts on this segment\nPerfect prediction (AUC = 1.0) is unrealistic because: - Employees have personal reasons we can’t measure (spouse job relocation, health issues) - Life events are unpredictable - Some attrition is healthy organizational turnover\nOur models provide meaningful risk stratification despite not being perfect.”\n\n\n\n1.6.3 Q: “Why didn’t you use neural networks or more advanced methods?”\nA: “Excellent question. We deliberately chose interpretable models for three reasons:\n1. Business Stakeholder Communication: - HR managers need to understand why an employee is flagged as high-risk - “This employee has OverTime=Yes and 5 years without promotion” is actionable - A neural network produces a black-box prediction HR can’t explain or act upon\n2. Limited Sample Size: - We have 1,470 employees total, only 237 with attrition - Neural networks need thousands to tens of thousands of examples to avoid overfitting - Our sample size is perfect for logistic regression, decision trees, and random forests\n3. Feature Engineering Matters More: - With structured tabular data (not images/text), feature quality &gt;&gt; model complexity - Our interaction terms (e.g., Promotion*Income) capture non-linearities - Careful variable selection via LASSO provides feature quality\nPerformance Comparison: Published studies comparing neural networks to random forests on similar HR datasets show minimal AUC improvement (typically 0.01-0.03), not worth the loss of interpretability.\nThat said, if this were a massive dataset (100K+ employees), neural networks or gradient boosting would be worth exploring.”\n\n\n\n1.6.4 Q: “How did you handle the limited HR department sample size?”\nA: “The n=63 HR sample is a legitimate limitation. Here’s how we addressed it:\nWhat We Did: 1. Acknowledged the limitation explicitly in our report 2. Reported wide confidence intervals - showed that non-significance is likely a power issue, not a true null effect 3. Compared model fit metrics (AIC, AUC) across departments to show HR model is less reliable 4. Recommended qualitative research for HR since quantitative analysis is underpowered\nWhat We Didn’t Do: - We didn’t pool HR with other departments (would hide the issue) - We didn’t drop HR entirely (they still have 19% attrition rate worth investigating) - We didn’t overinterpret non-significant results\nAlternative Approaches: 1. Bayesian Inference: Could use informative priors from R&D/Sales to borrow strength 2. Bootstrap Confidence Intervals: Could better quantify uncertainty 3. Exact Logistic Regression: Could handle small samples better than standard asymptotics\nBut for an educational project, acknowledging the limitation and recommending qualitative follow-up is the most honest approach.\nFuture Work: If we had multi-year data, we could pool HR across years (e.g., 3 years * 63 employees = 189 observations) for better power.”"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#closing-statement-1-minute",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#closing-statement-1-minute",
    "title": "",
    "section": "1.7 Closing Statement (1 minute)",
    "text": "1.7 Closing Statement (1 minute)\nGREG: “To summarize:\nWe’ve built a comprehensive attrition prediction system that identifies high-risk employees with 78% accuracy. More importantly, we’ve translated these predictions into actionable HR strategies:\n✓ Overtime monitoring can prevent 35-40 annual departures\n✓ Career acceleration can prevent 25-30 annual departures\n✓ Department-specific programs can prevent 20-30 annual departures\nCombined impact: 80-100 fewer departures annually, saving $4-5 million for a $145K investment.\nBeyond the numbers, we’ve demonstrated how statistical modeling informs targeted interventions rather than wasteful one-size-fits-all programs.\nThank you for your attention. Rupesh and I are happy to answer any questions.”"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_GREG_SEAN.html#additional-qa-points",
    "href": "PRESENTATION_SCRIPT_GREG_SEAN.html#additional-qa-points",
    "title": "",
    "section": "1.8 Additional Q&A Points",
    "text": "1.8 Additional Q&A Points\n\n1.8.1 Q: “What would you do differently if you had more time?”\nGREG: “Four extensions:\n\nSurvival Analysis: Instead of binary attrition (Yes/No), model time-to-attrition. Some employees might be high-risk but won’t leave for 6-12 months. Survival models capture this dynamic.\nCost-Sensitive Learning: Weight prediction errors by cost. False negatives (missing an attrition) cost $50K+ in replacement; false positives (unnecessary retention efforts) cost less. Optimize for business cost, not statistical accuracy.\nSequential Model Development: Build a two-stage model: (1) Predict attrition risk, (2) Among high-risk employees, predict which interventions work best (overtime reduction vs. promotion vs. salary increase).\nExternal Data Integration: Link to labor market data (unemployment rates, industry salary benchmarks) and individual performance data (promotability scores, project success metrics).\nTime Series: Track satisfaction over quarters. Someone whose satisfaction dropped from 4 to 2 is higher risk than someone consistently at 2.”\n\n\n\n\n1.8.2 Q: “How would you deploy this model in production?”\nGREG: “Deployment roadmap:\nPhase 1: Batch Scoring (Months 1-3) - Run model quarterly on full employee base - Generate risk scores (0-1 probability) - Flag top 20% for HR manager review - Integrate into existing HRIS system\nPhase 2: Interactive Dashboard (Months 3-6) - Build Tableau/PowerBI dashboard for HR managers - Show department-level risk distributions - Allow filtering by tenure, role, manager - Provide drill-down to individual employees\nPhase 3: Real-Time Monitoring (Months 6-12) - Integrate with time-tracking system (detect overtime) - Integrate with HRIS (detect promotion delays) - Trigger alerts when employee crosses risk threshold - Send automated notifications to managers\nPhase 4: Intervention Tracking (Ongoing) - Record which interventions were applied - Measure intervention effectiveness - Retrain models quarterly with new data - A/B test intervention strategies\nTechnical Stack: - Model: Python/R model exported to production - Storage: SQL database for employee data - API: REST API for real-time scoring - Dashboard: Tableau/PowerBI for visualization - Monitoring: Track model drift monthly\nGovernance: - Privacy: Anonymize data at individual level for analysts - Transparency: Explain model predictions to employees - Ethics: Human override - manager makes final call, not algorithm”\n[END OF GREG’S PORTION]"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_RUPESH.html",
    "href": "PRESENTATION_SCRIPT_RUPESH.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_RUPESH.html#opening-introduction-2-3-minutes",
    "href": "PRESENTATION_SCRIPT_RUPESH.html#opening-introduction-2-3-minutes",
    "title": "",
    "section": "1.1 Opening & Introduction (2-3 minutes)",
    "text": "1.1 Opening & Introduction (2-3 minutes)\nRUPESH: “Good [morning/afternoon], everyone. Today we’re presenting our comprehensive analysis of employee attrition using the IBM HR Analytics dataset. I’m Rupesh, and I’ll be covering the first half of our presentation, focusing on our research methodology and the first two research questions. My partner Greg will then discuss our third research question and conclusions.\n\n1.1.1 Project Overview\nOur project addresses a critical business challenge: employee turnover costs organizations between 50% to 200% of an employee’s annual salary when you factor in recruitment, training, and lost productivity. We analyzed 1,470 employee records with 39 attributes to build predictive models that can identify at-risk employees before they leave.\n\n\n1.1.2 Why These Research Questions?\nWe deliberately chose three research questions that address actionable HR challenges:\nQuestion 1 on Work-Life Balance - We selected this because overtime policies, flexible scheduling, and remote work are directly controllable by management. Unlike demographics like age or gender, these are factors organizations can actually change.\nQuestion 2 on Career Development - This addresses career stagnation, a often-overlooked dimension. Many companies lose high-performers simply because they wait too long to promote them. We wanted to identify specific thresholds where delays become critical.\nQuestion 3 on Department Differences - One-size-fits-all retention strategies typically fail. Sales representatives, R&D engineers, and HR professionals have fundamentally different job demands and satisfaction drivers. Stratified analysis lets us tailor interventions effectively."
  },
  {
    "objectID": "PRESENTATION_SCRIPT_RUPESH.html#dataset-description-data-quality-1-2-minutes",
    "href": "PRESENTATION_SCRIPT_RUPESH.html#dataset-description-data-quality-1-2-minutes",
    "title": "",
    "section": "1.2 Dataset Description & Data Quality (1-2 minutes)",
    "text": "1.2 Dataset Description & Data Quality (1-2 minutes)\nRUPESH: “Let me walk you through our dataset and data preparation process.\n\n1.2.1 Dataset Overview\n\nSource: IBM HR Analytics Employee Attrition Dataset\nSize: 1,470 employees, 39 variables\nOverall Attrition Rate: 16.1% (237 employees left)\nKey Challenge: Significant class imbalance - only 16% attrition\n\n\n\n1.2.2 Data Preparation Challenges We Overcame\nColumn Name Inconsistencies:\nThe dataset had inconsistent naming - some columns used spaces like “Over Time”, others used CamelCase like “OverTime”. We built a robust column standardization function that automatically maps these variants to consistent names. This prevented downstream errors in our analysis.\nClass Imbalance Handling:\nWith only 16% attrition, naive models would achieve 84% accuracy by simply predicting “No attrition” for everyone - completely useless. We addressed this using: - Weighted logistic regression (giving more importance to attrition cases) - SMOTE for balanced training - Careful threshold optimization using Youden’s J statistic - Stratified sampling to maintain class proportions\n\n\n1.2.3 Data Quality Checks\n\nNo missing values - dataset was pre-cleaned\nVIF analysis showed no severe multicollinearity (all VIF &lt; 7)\nRange validation - all values within expected domains\nDistribution analysis - identified several right-skewed variables (Income, Years at Company)"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_RUPESH.html#question-1-work-life-imbalance-analysis-4-5-minutes",
    "href": "PRESENTATION_SCRIPT_RUPESH.html#question-1-work-life-imbalance-analysis-4-5-minutes",
    "title": "",
    "section": "1.3 Question 1: Work-Life Imbalance Analysis (4-5 minutes)",
    "text": "1.3 Question 1: Work-Life Imbalance Analysis (4-5 minutes)\nRUPESH: “Our first research question asks: How do work-life factors interact to predict attrition? Can we identify high-risk employee profiles?\n\n1.3.1 Key Variables (Q1)\n\nOverTime: Whether employee works overtime (Yes/No)\nDistanceFromHome: Commute distance (1-29 miles)\nWorkLifeBalance: Self-reported rating (1-4 scale)\nYearsAtCompany: Employee tenure (0-40 years)\n\n\n\n1.3.2 Methodology: Decision Trees & Random Forests\nWe used two tree-based methods:\nDecision Trees provide interpretability - we can literally draw out decision rules: - “If OverTime=Yes AND YearsAtCompany &lt; 2, then HIGH RISK” - These rules are actionable for managers\nRandom Forests provide accuracy - by building 500 trees and averaging their predictions, we achieve our best overall performance.\n\n\n1.3.3 Key Findings (Q1)\n1. OverTime is the Dominant Predictor - Chi-square test: χ² = 87.56, p &lt; 0.0001 - 30.5% attrition rate for overtime workers vs. 10.4% for non-overtime workers - Nearly 3x higher risk from overtime alone\n2. High-Risk Profile Identified: Our decision tree revealed a specific vulnerable segment: - Works overtime (OverTime = Yes) - Low tenure (YearsAtCompany &lt; 2)\n- Long commute (DistanceFromHome &gt; 10 miles) - Low work-life balance rating\nThis segment has over 50% attrition probability.\n3. Model Performance: - Decision Tree AUC: 0.685 - Random Forest AUC: 0.777 (BEST OVERALL MODEL) - Random Forest feature importance ranks: 1. OverTime (importance = 0.35) 2. YearsAtCompany (importance = 0.28) 3. DistanceFromHome (importance = 0.20) 4. WorkLifeBalance (importance = 0.17)\n\n\n1.3.4 Practical Implications (Q1)\nImmediate Actions HR Can Take: 1. Overtime Monitoring System: Flag employees with &gt;40 hours/week for 4+ consecutive weeks 2. Workload Rebalancing: Distribute work more evenly across teams 3. Remote Work Options: Especially for high-distance commuters 4. New Hire Support: Extra attention for employees in first 2 years\nExpected Impact: If we reduce overtime workers from current 30% to 20%, we estimate preventing approximately 35-40 annual departures (assuming 1,470 employee base)."
  },
  {
    "objectID": "PRESENTATION_SCRIPT_RUPESH.html#question-2-career-stagnation-compensation-5-6-minutes",
    "href": "PRESENTATION_SCRIPT_RUPESH.html#question-2-career-stagnation-compensation-5-6-minutes",
    "title": "",
    "section": "1.4 Question 2: Career Stagnation & Compensation (5-6 minutes)",
    "text": "1.4 Question 2: Career Stagnation & Compensation (5-6 minutes)\nRUPESH: “Our second research question examines career development patterns and compensation effects on attrition.\n\n1.4.1 Key Variables (Q2)\n\nYearsSinceLastPromotion: Time stagnant in grade (0-15 years)\nYearsInCurrentRole: Time in same position (0-18 years)\nMonthlyIncome: Salary ($1,009 - $19,999)\nPercentSalaryHike: Recent raise (11-25%)\nJobLevel: Hierarchy position (1-5)\nPlus interaction terms and additional controls\n\n\n\n1.4.2 Methodology: Logistic Regression with LASSO\nWe compared three approaches:\n1. Full Logistic Regression (16 predictors) - Includes all variables and interactions - Risk of overfitting\n2. Weighted Logistic Regression - Addresses class imbalance - Gives 5.2x weight to attrition cases - AUC improved from 0.732 to 0.752\n3. LASSO Regularization - Automatic variable selection via penalty parameter λ - Shrinks less important coefficients to exactly zero - Selected 14 of 16 predictors (dropped JobLevel and MonthlyIncome*PercentSalaryHike interaction)\n\n\n1.4.3 Key Findings (Q2)\n1. Years Since Last Promotion is Critical - Coefficient: +0.122 (p &lt; 0.001) - Interpretation: Each year without promotion increases attrition odds by 13% - Employees with 5+ years without promotion have 1.82x higher odds - Employees with 10+ years without promotion have 3.32x higher odds\nThis is a non-linear escalation - the effect accelerates over time.\n2. Optimal Classification Threshold Instead of using default 0.5 probability cutoff, we optimized using Youden’s J statistic: - Optimal threshold: 0.581 - At this threshold: - Sensitivity: 71.3% (correctly identify 71% of actual leavers) - Specificity: 74.8% (correctly identify 75% of stayers) - Balanced performance on both classes\n3. LASSO Variable Selection Results\nLASSO automatically identified the 14 most important predictors:\nStrong Positive Predictors (Increase Attrition): - YearsSinceLastPromotion: +0.122 - OverTime: +0.108 - YearsInCurrentRole: +0.089 - NumCompaniesWorked: +0.067\nStrong Negative Predictors (Decrease Attrition): - YearsAtCompany: -0.156 - TotalWorkingYears: -0.091 - StockOptionLevel: -0.074 - Age: -0.063\nVariables Dropped by LASSO: - JobLevel (redundant with other tenure variables) - MonthlyIncome*PercentSalaryHike interaction (not meaningful)\n4. Model Performance (Q2) - Unweighted Logistic: AUC = 0.732 - Weighted Logistic: AUC = 0.752 - LASSO: AUC = 0.747\nWeighted logistic slightly outperforms LASSO, but LASSO offers parsimony (fewer parameters).\n5. Multicollinearity Check VIF analysis for all predictors shows values 1.00-1.34, well below the threshold of 5. This confirms no problematic multicollinearity, validating our model stability.\n\n\n1.4.4 Practical Implications (Q2)\nCareer Development Interventions:\nHigh-Risk Threshold Identified: 4+ years without promotion At 4 years, attrition risk increases substantially. Implement:\n\nMandatory Career Development Reviews at year 3\nPromotion Acceleration Programs for high-performers\nLateral Movement Options when vertical promotion isn’t available\nRetention Bonuses for employees at 4+ year mark\n\nStock Option Strategy: LASSO identified StockOptionLevel as a strong protective factor (-0.074 coefficient).\n- Employees with higher stock options are less likely to leave - Recommendation: Expand stock option eligibility, especially for mid-career employees (5-10 years tenure)\nExpected Impact: - Accelerating promotions for the 15% of employees at 4+ years could prevent an estimated 25-30 annual departures - Enhanced stock options could further reduce attrition by 10-15%"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_RUPESH.html#technical-details-qa-preparation",
    "href": "PRESENTATION_SCRIPT_RUPESH.html#technical-details-qa-preparation",
    "title": "",
    "section": "1.5 Technical Details Q&A Preparation",
    "text": "1.5 Technical Details Q&A Preparation\nRUPESH: Here are key technical points you might be asked:\n\n1.5.1 Q: “Why did you use Random Forest instead of just Decision Trees?”\nA: “Decision trees are prone to overfitting - they memorize training data. Random Forests address this by building 500 different trees using: - Bootstrap sampling (each tree sees different random subset of data) - Feature randomization (each split considers random subset of features) - Averaging predictions (consensus across 500 trees)\nThis ensemble approach increased our AUC from 0.685 to 0.777, a significant improvement. The tradeoff is interpretability - we can’t draw out simple rules anymore, but we gain much better predictions.”\n\n\n1.5.2 Q: “What is LASSO and why use it?”\nA: “LASSO stands for Least Absolute Shrinkage and Selection Operator. It’s a regularization technique that: 1. Adds a penalty to the regression objective function proportional to the absolute value of coefficients 2. This penalty forces some coefficients to exactly zero, performing automatic variable selection 3. The penalty strength is controlled by lambda (λ), chosen via cross-validation\nWe used LASSO because we had 16 candidate predictors with some correlation. Instead of subjectively choosing variables, LASSO objectively identified the 14 most important ones. It’s particularly valuable when you have many potential predictors and want to avoid overfitting.”\n\n\n1.5.3 Q: “How did you handle class imbalance?”\nA: “We used multiple strategies: 1. Weighted Logistic Regression: Assign weights inversely proportional to class frequencies. With 16% attrition, we used weight = 5.2 for attrition cases, weight = 1.0 for non-attrition 2. Threshold Optimization: Instead of default 0.5, we used Youden’s J statistic to find optimal 0.581 threshold 3. Stratified Sampling: Maintained class proportions in train/test split 4. SMOTE (Synthetic Minority Over-sampling): Considered but not needed due to sufficient sample size\nThe weighted approach improved AUC from 0.732 to 0.752 on the test set.”\n\n\n1.5.4 Q: “What is AUC and why is it better than accuracy?”\nA: “AUC = Area Under the ROC Curve. It measures how well the model discriminates between classes across all possible thresholds.\nWhy it’s better than accuracy for imbalanced data: - With 16% attrition, a model that predicts “No attrition” for everyone achieves 84% accuracy but is completely useless - AUC is threshold-independent and evaluates the model’s ranking ability - AUC = 0.5 means random guessing - AUC = 1.0 means perfect discrimination - Our best model (Random Forest) achieved AUC = 0.777, meaning it correctly ranks a random attrition case higher than a random non-attrition case 77.7% of the time.”\n\n\n1.5.5 Q: “Which rows of data did you use for classification?”\nA: “We split our 1,470 employees into: - Training set: 70% (1,029 employees) - used to fit models - Test set: 30% (441 employees) - used to evaluate performance\nWe used stratified sampling to maintain the 16.1% attrition rate in both sets. The split was done randomly with set.seed(515) for reproducibility.\nAll performance metrics reported (AUC, sensitivity, specificity) are on the held-out test set, not the training set, ensuring unbiased evaluation.”\n\n\n1.5.6 Q: “Why did you choose weighted logistic regression for Q2?”\nA: “We chose weighted logistic regression for Q2 because: 1. Interpretability: Coefficients can be exponentiated to odds ratios (e.g., ‘each year without promotion increases odds by 13%’) 2. Class Imbalance: Weighting gives more importance to minority class 3. Linear Decision Boundary: Appropriate for Q2 where we’re examining thresholds and marginal effects 4. Statistical Inference: Provides p-values and confidence intervals 5. Business Communication: Executives understand odds ratios better than tree rules\nThe weighted approach outperformed unweighted (0.752 vs 0.732 AUC) while maintaining these interpretability benefits.”\n\n\n1.5.7 Q: “How did you choose your variables?”\nA: “Variable selection was research-question-driven:\nQ1 (Work-Life): We selected 4 variables based on HR literature: - OverTime (directly controllable) - DistanceFromHome (commute burden) - WorkLifeBalance (self-reported rating) - YearsAtCompany (tenure effects)\nQ2 (Career/Compensation): We included 10 core variables: - Career stagnation: YearsSinceLastPromotion, YearsInCurrentRole - Compensation: MonthlyIncome, PercentSalaryHike\n- Demographics/Controls: Age, TotalWorkingYears, NumCompaniesWorked - Plus 6 interaction terms to capture non-linear effects\nQ3 (Satisfaction): We focused on 3 satisfaction dimensions: - JobSatisfaction - EnvironmentSatisfaction - RelationshipSatisfaction\nWe validated these selections using: - Literature review - Domain expertise - VIF analysis (no multicollinearity) - LASSO for Q2 (confirmed our selections)”"
  },
  {
    "objectID": "PRESENTATION_SCRIPT_RUPESH.html#transition-to-greg",
    "href": "PRESENTATION_SCRIPT_RUPESH.html#transition-to-greg",
    "title": "",
    "section": "1.6 Transition to Greg",
    "text": "1.6 Transition to Greg\nRUPESH: “That covers our first two research questions - work-life balance and career stagnation. We’ve identified overtime as the strongest single predictor, and years without promotion as a critical threshold effect. Now I’ll hand it over to Greg, who will discuss our third research question on department-specific satisfaction patterns and our overall conclusions.”\n[END OF RUPESH’S PORTION]"
  },
  {
    "objectID": "index.html#dataset-selection-and-justification",
    "href": "index.html#dataset-selection-and-justification",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.1 Dataset Selection and Justification",
    "text": "2.1 Dataset Selection and Justification\nDataset Source: IBM HR Analytics Employee Attrition & Performance Dataset Availability: Publicly available via data.world repositories (Aizemberg, 2017) Sample Size: 1,470 employees Variables: 41 employee attributes Response Variable: Binary attrition indicator (Yes/No)\nRationale for Dataset Selection:\nThe selection of this dataset was driven by several methodological and practical considerations that align with the objectives of this statistical learning project:\n\nRealistic Structure: While this is a simulated dataset created by IBM data scientists for educational purposes, it accurately mirrors the structure, variable types, and statistical relationships found in actual enterprise HR data. The variables include standard HR metrics (demographics, job characteristics, satisfaction scores, performance ratings) that organizations routinely collect through Human Resource Information Systems (HRIS).\nAppropriate Class Imbalance: The 16.1% attrition rate reflects realistic conditions where employee departure is a minority event, requiring proper handling techniques (weighted learning, threshold optimization, stratified sampling) that generalize to production environments.\nMultiple Variable Types: The dataset includes categorical variables (Department, JobRole, MaritalStatus), ordinal variables (satisfaction scores 1-4), and continuous variables (Age, MonthlyIncome, YearsAtCompany), enabling demonstration of diverse modeling techniques.\nNo Proprietary Restrictions: Unlike actual company data which faces confidentiality and privacy restrictions, this dataset can be freely analyzed and published, enabling transparent methodology demonstration.\nEstablished Benchmark: This dataset is widely used in HR analytics research and machine learning competitions, providing established baselines for model performance comparison.\n\nDataset Limitations Acknowledged: As a simulated dataset, certain patterns may be artificially clean (no missing values, clear variable definitions). Real-world data would require additional preprocessing. However, the analytical methodologies demonstrated transfer directly to production environments."
  },
  {
    "objectID": "index.html#variable-descriptions",
    "href": "index.html#variable-descriptions",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.3 Variable Descriptions",
    "text": "2.3 Variable Descriptions\n\n\nDataset Dimensions: 1470 employees, 41 variables\n\n\n\nTable 2: Key Variables by Research Question\n\n\nVariable\nDescription\nResearch Q\n\n\n\n\nAttrition\nEmployee left the company (Yes/No) - Response Variable\nAll\n\n\nOverTime\nWhether employee works overtime (Yes/No)\nQ1\n\n\nDistanceFromHome\nDistance from home to workplace (miles)\nQ1\n\n\nWorkLifeBalance\nWork-life balance rating (1-4, higher = better)\nQ1\n\n\nYearsAtCompany\nTotal years at current company\nQ1\n\n\nYearsSinceLastPromotion\nYears since last promotion\nQ2\n\n\nYearsInCurrentRole\nYears in current role\nQ2\n\n\nMonthlyIncome\nMonthly salary (USD)\nQ2\n\n\nPercentSalaryHike\nPercent salary increase at last review\nQ2\n\n\nJobLevel\nJob level within company hierarchy (1-5)\nQ2\n\n\nJobSatisfaction\nJob satisfaction rating (1-4)\nQ3\n\n\nEnvironmentSatisfaction\nEnvironment satisfaction rating (1-4)\nQ3\n\n\nRelationshipSatisfaction\nRelationship satisfaction rating (1-4)\nQ3\n\n\nDepartment\nDepartment (HR, R&D, Sales)\nQ3"
  },
  {
    "objectID": "index.html#numeric-variables-by-attrition-status",
    "href": "index.html#numeric-variables-by-attrition-status",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.3 Numeric Variables by Attrition Status",
    "text": "3.3 Numeric Variables by Attrition Status\n\n\n\n\n\nFigure 3: Distribution of Key Numeric Variables by Attrition Status\n\n\n\n\n\n\n\nTable 4: T-Tests Comparing Numeric Variables by Attrition Status\n\n\n\nVariable\nMean (No)\nMean (Yes)\nDifference\np-value\nSig.\n\n\n\n\nmean in group No\nAge\n37.56\n33.61\n-3.95\n0.0000\n***\n\n\nmean in group No1\nMonthlyIncome\n6832.74\n4787.09\n-2045.65\n0.0000\n***\n\n\nmean in group No2\nDistanceFromHome\n8.92\n10.63\n1.72\n0.0041\n**\n\n\nmean in group No3\nYearsAtCompany\n7.37\n5.13\n-2.24\n0.0000\n***\n\n\nmean in group No4\nYearsInCurrentRole\n4.48\n2.90\n-1.58\n0.0000\n***\n\n\nmean in group No5\nYearsSinceLastPromotion\n2.23\n1.95\n-0.29\n0.1987\n\n\n\nmean in group No6\nTotalWorkingYears\n11.86\n8.24\n-3.62\n0.0000\n***\n\n\nmean in group No7\nJobSatisfaction\n2.78\n2.47\n-0.31\n0.0001\n***\n\n\nmean in group No8\nEnvironmentSatisfaction\n2.77\n2.46\n-0.31\n0.0002\n***\n\n\nmean in group No9\nWorkLifeBalance\n2.78\n2.66\n-0.12\n0.0305\n*"
  },
  {
    "objectID": "index.html#logistic-regression-model-diagnostics",
    "href": "index.html#logistic-regression-model-diagnostics",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.3 Logistic Regression Model Diagnostics",
    "text": "4.3 Logistic Regression Model Diagnostics\n\n\n\n\n\nFigure 6: Logistic Regression Diagnostic Plots\n\n\n\n\n\n\n\n=== Hosmer-Lemeshow Goodness-of-Fit Test ===\n\n\nChi-square statistic: 15.8459 \n\n\nDegrees of freedom: 8 \n\n\np-value: 0.0446 \n\n\nInterpretation: CAUTION - Some lack of fit (p &lt; 0.05) \n\n\n\n=== Variance Inflation Factors (VIF) ===\n\n\n    OverTime_Binary    DistanceFromHome     WorkLifeBalance      YearsAtCompany \n              17.99                1.77                1.74                1.69 \n           OT_x_WLB       OT_x_Distance OT_x_YearsAtCompany \n              16.07                3.28                2.99 \n\n\n\nAll VIF values &lt; 5 indicate acceptable multicollinearity levels."
  },
  {
    "objectID": "index.html#random-forest-with-cross-validation-tuning",
    "href": "index.html#random-forest-with-cross-validation-tuning",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.4 Random Forest with Cross-Validation Tuning",
    "text": "4.4 Random Forest with Cross-Validation Tuning\n\n\n\n=== Random Forest 5-Fold Cross-Validation Results ===\n\n\nRandom Forest \n\n1029 samples\n   4 predictor\n   2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 823, 823, 823, 824, 823 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n  2     0.6848671  0.9537572  0.1768939\n  3     0.6642839  0.9225434  0.1950758\n  4     0.6586919  0.9109827  0.2071970\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\n\nFigure 7: Random Forest Hyperparameter Tuning via 5-Fold Cross-Validation\n\n\n\n\n\n\n\n\n\nFigure 8: Random Forest Variable Importance (Tuned Model)\n\n\n\n\n\n\n\n=== Random Forest Performance (Test Set) ===\n\n\nTest Set AUC-ROC: 0.6722 \n\n\nOptimal mtry (from CV): 2 \n\n\nCross-Validated AUC: 0.6849 \n\n\nAfter proper hyperparameter tuning, the Random Forest achieves AUC = 0.672 on the held-out test set, outperforming the single decision tree as expected from ensemble theory."
  },
  {
    "objectID": "index.html#q1-model-comparison-roc-curves",
    "href": "index.html#q1-model-comparison-roc-curves",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.5 Q1 Model Comparison: ROC Curves",
    "text": "4.5 Q1 Model Comparison: ROC Curves\n\n\n\n\n\nFigure 9: ROC Curve Comparison - Research Question 1 Models"
  },
  {
    "objectID": "index.html#traintest-split-and-weighted-logistic-regression",
    "href": "index.html#traintest-split-and-weighted-logistic-regression",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.1 Train/Test Split and Weighted Logistic Regression",
    "text": "5.1 Train/Test Split and Weighted Logistic Regression\n\n\n=== Q2 Train/Test Split ===\n\n\nTraining set: 1029 observations\n\n\nTest set: 441 observations\n\n\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole + \n    MonthlyIncome + PercentSalaryHike + JobLevel, family = binomial(), \n    data = train_q2, weights = weights_q2)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              1.757e+00  3.319e-01   5.293 1.20e-07 ***\nYearsSinceLastPromotion  1.428e-01  2.757e-02   5.180 2.22e-07 ***\nYearsInCurrentRole      -1.780e-01  2.683e-02  -6.635 3.24e-11 ***\nMonthlyIncome           -9.407e-05  5.010e-05  -1.878  0.06045 .  \nPercentSalaryHike       -5.107e-02  1.844e-02  -2.770  0.00561 ** \nJobLevel                -6.872e-02  2.009e-01  -0.342  0.73233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1426.5  on 1028  degrees of freedom\nResidual deviance: 1309.6  on 1023  degrees of freedom\nAIC: 1740.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nTable 6: Weighted Logistic Regression Odds Ratios (Q2)\n\n\nVariable\nBeta\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nYearsSinceLastPromotion\n0.1428\n1.1535\n1.0935\n1.2185\n0.0000\n***\n\n\nYearsInCurrentRole\n-0.1780\n0.8369\n0.7933\n0.8814\n0.0000\n***\n\n\nMonthlyIncome\n-0.0001\n0.9999\n0.9998\n1.0000\n0.0604\n\n\n\nPercentSalaryHike\n-0.0511\n0.9502\n0.9163\n0.9850\n0.0056\n**\n\n\nJobLevel\n-0.0687\n0.9336\n0.6297\n1.3855\n0.7323\n\n\n\n\n\n\n\n5.1.1 Detailed Interpretation: Career Stagnation Threshold Effects\n\n\n\nTable 7: Cumulative Odds Multipliers by Years Without Promotion\n\n\nYears Without Promotion\nCumulative Odds Multiplier\n% Increase vs Baseline\nRisk Category\n\n\n\n\n1\n1.15\n15%\nLow\n\n\n2\n1.33\n33%\nLow\n\n\n3\n1.53\n53%\nModerate\n\n\n4\n1.77\n77%\nElevated\n\n\n5\n2.04\n104%\nHigh\n\n\n6\n2.36\n136%\nVery High\n\n\n7\n2.72\n172%\nCritical\n\n\n\n\n\nEach additional year without promotion increases attrition odds by approximately 15.3%. The 4-year mark represents a critical inflection point where cumulative risk acceleration becomes severe.\nActionable Intervention: Implement mandatory career development reviews at Year 3, requiring commitment to either promotion, lateral move, or explicit documented timeline."
  },
  {
    "objectID": "index.html#threshold-optimization-and-calibration",
    "href": "index.html#threshold-optimization-and-calibration",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.3 Threshold Optimization and Calibration",
    "text": "5.3 Threshold Optimization and Calibration\n\n\n\n\n\nFigure 12: ROC Curve and Threshold Optimization\n\n\n\n\n\n\n\nTable 9: Classification Performance at Different Thresholds\n\n\nThreshold\nSensitivity\nSpecificity\nAccuracy\nNote\n\n\n\n\n0.100\n0.973\n0.043\n0.197\n\n\n\n0.200\n0.959\n0.117\n0.256\n\n\n\n0.300\n0.918\n0.283\n0.388\n\n\n\n0.400\n0.781\n0.413\n0.474\n\n\n\n0.500\n0.699\n0.562\n0.585\n\n\n\n0.574\n0.589\n0.728\n0.705\nOptimal (Youden's J)\n\n\n\n\n\n\n\n\n\n\nFigure 13: Calibration Curve - Model Probability Assessment\n\n\n\n\n\n=== Calibration Assessment ===\n\n\nCalibration correlation: 0.817"
  },
  {
    "objectID": "index.html#methodology-justification",
    "href": "index.html#methodology-justification",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.1 Methodology Justification",
    "text": "6.1 Methodology Justification\nGradient Boosting Machines (GBM) represent state-of-the-art methodology for tabular data classification. Unlike Random Forest which builds trees independently in parallel, GBM builds trees sequentially, with each subsequent tree correcting errors made by the previous ensemble.\nKey Advantages:\n\nHandles Non-Linearity: Automatically captures complex interactions\nRobust to Class Imbalance: Built-in mechanisms for handling unbalanced outcomes\nInterpretable Variable Importance: Rankings based on predictive contribution\nIndustry Standard: Deployed by leading organizations (Uber, Airbnb, Netflix) for churn prediction\n\n\n\n\n\n\nFigure 14: Gradient Boosting Machine - Cross-Validation Training\n\n\n\n\n\n=== Gradient Boosting Machine Training Results ===\n\n\nTotal trees trained: 1000\n\n\nOptimal trees (5-fold CV): 485 \n\n\nLearning rate: 0.01\n\n\nInteraction depth: 4\n\n\n\n\n\n\n\nFigure 15: GBM Variable Importance Rankings\n\n\n\n\n\n\n\n=== GBM Performance on Test Set ===\n\n\nTest Set AUC-ROC: 0.783 \n\n\nTrees used: 485 \n\n\n\n=== Q2 Model Comparison ===\n\n\nWeighted Logistic Regression: AUC = 0.665 \n\n\nLASSO Logistic Regression:    AUC = 0.8034 \n\n\nGradient Boosting Machine:    AUC = 0.783"
  },
  {
    "objectID": "index.html#actionable-next-steps",
    "href": "index.html#actionable-next-steps",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "9.3 Actionable Next Steps",
    "text": "9.3 Actionable Next Steps\n\n9.3.1 Immediate (0-3 months)\n\nModel Deployment: Containerize best model for monthly batch scoring\nDashboard Creation: Build Tableau/PowerBI dashboard showing risk distributions\nPilot Intervention: Test overtime reduction with 2-3 high-risk teams\n\n\n\n9.3.2 Short-Term (3-6 months)\n\nLongitudinal Data Collection: Implement quarterly satisfaction pulse surveys\nModel Retraining: Update models quarterly with new data\nExpanded Features: Integrate performance ratings and organizational changes\n\n\n\n9.3.3 Medium-Term (6-12 months)\n\nCausal Inference: Apply propensity score matching for causal effect estimation\nSurvival Analysis: Build Cox proportional hazards model for time-to-attrition\nCost-Sensitive Learning: Weight errors by actual replacement costs\n\n\n\n9.3.4 Long-Term (12+ months)\n\nPrescriptive Analytics: Move from “who will leave?” to “what intervention for whom?”\nNLP Integration: Analyze exit interview text for early signals\nMulti-Level Modeling: Account for team/manager/department hierarchy"
  },
  {
    "objectID": "index (1).html",
    "href": "index (1).html",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "",
    "text": "This comprehensive statistical analysis investigates the factors contributing to employee attrition using the IBM HR Analytics Employee Attrition & Performance dataset (n=1470). The study addresses three primary research questions using a multi-method approach including decision trees, cross-validated random forests, logistic regression with interaction terms, LASSO regularization, gradient boosting machines, and stratified departmental analysis.\nKey Findings:\n\nWork-Life Imbalance: OverTime emerges as the dominant predictor of attrition across all modeling approaches, with employees working overtime showing approximately 2.9× higher attrition rates (30.5% vs 10.4%). Decision tree analysis identifies distinct high-risk profiles combining overtime work with low tenure.\nCareer Stagnation & Compensation: Years since last promotion demonstrates a compounding effect on attrition risk, with the 4-year mark representing a critical threshold where risk accelerates substantially. LASSO variable selection confirms that career progression indicators outweigh pure compensation metrics.\nDepartment-Stratified Satisfaction: The predictive power of satisfaction variables varies significantly across departments. R&D shows strong relationships between all three satisfaction types and retention, while Sales exhibits selective effects, and HR’s small sample size limits reliable inference.\n\n\n\n\nTable 1: Executive Summary Statistics\n\n\nSummary Metric\nValue\n\n\n\n\nTotal Employees Analyzed\n1,470\n\n\nOverall Attrition Rate\n16.1%\n\n\nTotal Employees Who Left\n237\n\n\nOvertime Workers Attrition Rate\n30.5%\n\n\nNon-Overtime Workers Attrition Rate\n10.4%\n\n\nRelative Risk (Overtime vs Non-Overtime)\n2.9x"
  },
  {
    "objectID": "index (1).html#dataset-selection-and-justification",
    "href": "index (1).html#dataset-selection-and-justification",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.1 Dataset Selection and Justification",
    "text": "2.1 Dataset Selection and Justification\nDataset Source: IBM HR Analytics Employee Attrition & Performance Dataset Availability: Publicly available via data.world repositories (Aizemberg, 2017) Sample Size: 1,470 employees Variables: 41 employee attributes Response Variable: Binary attrition indicator (Yes/No)\nRationale for Dataset Selection:\nThe selection of this dataset was driven by several methodological and practical considerations that align with the objectives of this statistical learning project:\n\nRealistic Structure: While this is a simulated dataset created by IBM data scientists for educational purposes, it accurately mirrors the structure, variable types, and statistical relationships found in actual enterprise HR data. The variables include standard HR metrics (demographics, job characteristics, satisfaction scores, performance ratings) that organizations routinely collect through Human Resource Information Systems (HRIS).\nAppropriate Class Imbalance: The 16.1% attrition rate reflects realistic conditions where employee departure is a minority event, requiring proper handling techniques (weighted learning, threshold optimization, stratified sampling) that generalize to production environments.\nMultiple Variable Types: The dataset includes categorical variables (Department, JobRole, MaritalStatus), ordinal variables (satisfaction scores 1-4), and continuous variables (Age, MonthlyIncome, YearsAtCompany), enabling demonstration of diverse modeling techniques.\nNo Proprietary Restrictions: Unlike actual company data which faces confidentiality and privacy restrictions, this dataset can be freely analyzed and published, enabling transparent methodology demonstration.\nEstablished Benchmark: This dataset is widely used in HR analytics research and machine learning competitions, providing established baselines for model performance comparison.\n\nDataset Limitations Acknowledged: As a simulated dataset, certain patterns may be artificially clean (no missing values, clear variable definitions). Real-world data would require additional preprocessing. However, the analytical methodologies demonstrated transfer directly to production environments."
  },
  {
    "objectID": "index (1).html#research-questions",
    "href": "index (1).html#research-questions",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.2 Research Questions",
    "text": "2.2 Research Questions\nThree primary research questions drove the analysis for this project:\n\nWork-Life Imbalance & Attrition Risk Profiling: How do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict attrition, and can we identify distinct “high-risk” employee profiles?\nCareer Stagnation & Compensation Effects: At what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?\nDepartment-Stratified Satisfaction Analysis: Does the predictive power of employee satisfaction variables (JobSatisfaction, EnvironmentSatisfaction, RelationshipSatisfaction) differ across departments?\n\nResearch Question Justification:\nQuestion 1 was selected because work-life balance represents one of the most modifiable factors under organizational control. Unlike fixed demographics, companies can directly intervene through overtime policies, flexible scheduling, and remote work options.\nQuestion 2 addresses career progression and compensation alignment. Organizations frequently lose high-performing employees due to stagnant advancement opportunities. By identifying specific thresholds where promotion delays become critical, HR can implement proactive interventions.\nQuestion 3 recognizes that one-size-fits-all retention strategies often fail. Different departments have unique cultures, work demands, and satisfaction drivers. Stratified analysis enables targeted interventions."
  },
  {
    "objectID": "index (1).html#variable-descriptions",
    "href": "index (1).html#variable-descriptions",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.3 Variable Descriptions",
    "text": "2.3 Variable Descriptions\n\n\nDataset Dimensions: 1470 employees, 41 variables\n\n\n\nTable 2: Key Variables by Research Question\n\n\nVariable\nDescription\nResearch Q\n\n\n\n\nAttrition\nEmployee left the company (Yes/No) - Response Variable\nAll\n\n\nOverTime\nWhether employee works overtime (Yes/No)\nQ1\n\n\nDistanceFromHome\nDistance from home to workplace (miles)\nQ1\n\n\nWorkLifeBalance\nWork-life balance rating (1-4, higher = better)\nQ1\n\n\nYearsAtCompany\nTotal years at current company\nQ1\n\n\nYearsSinceLastPromotion\nYears since last promotion\nQ2\n\n\nYearsInCurrentRole\nYears in current role\nQ2\n\n\nMonthlyIncome\nMonthly salary (USD)\nQ2\n\n\nPercentSalaryHike\nPercent salary increase at last review\nQ2\n\n\nJobLevel\nJob level within company hierarchy (1-5)\nQ2\n\n\nJobSatisfaction\nJob satisfaction rating (1-4)\nQ3\n\n\nEnvironmentSatisfaction\nEnvironment satisfaction rating (1-4)\nQ3\n\n\nRelationshipSatisfaction\nRelationship satisfaction rating (1-4)\nQ3\n\n\nDepartment\nDepartment (HR, R&D, Sales)\nQ3"
  },
  {
    "objectID": "index (1).html#class-imbalance-analysis",
    "href": "index (1).html#class-imbalance-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.1 Class Imbalance Analysis",
    "text": "3.1 Class Imbalance Analysis\n\n\n\n\n\nFigure 1: Attrition Distribution - Class Imbalance Analysis\n\n\n\n\n\n=== Class Imbalance Statistics ===\n\n\nMajority Class (No Attrition): 1233 employees ( 83.9 %)\n\n\nMinority Class (Attrition): 237 employees ( 16.1 %)\n\n\nImbalance Ratio: 5.2 :1\n\n\n=== Class Weights for Balanced Learning ===\n\n\nWeight for Class 0 (No Attrition): 0.5961 \n\n\nWeight for Class 1 (Attrition): 3.1013 \n\n\nThe dataset exhibits significant class imbalance with 83.9% of employees showing no attrition and only 16.1% experiencing attrition. To address this, we employ inverse class weighting in logistic regression and tree-based models, stratified sampling during train/test splits, evaluation metrics beyond accuracy (AUC-ROC), and threshold optimization."
  },
  {
    "objectID": "index (1).html#attrition-by-key-categorical-variables",
    "href": "index (1).html#attrition-by-key-categorical-variables",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.2 Attrition by Key Categorical Variables",
    "text": "3.2 Attrition by Key Categorical Variables\n\n\n\n\n\nFigure 2: Attrition Rates by Key Categorical Variables\n\n\n\n\nEmployees working overtime exhibit dramatically higher attrition (30.5%) compared to those without overtime (10.4%), representing a 2.9x relative risk.\n\n\n\nTable 3: Chi-Square Tests of Association with Attrition\n\n\nVariable\nChi-Square\ndf\np-value\nCramer's V\nSig.\n\n\n\n\nOverTime\n87.56\n1\n0.0000\n0.244\n***\n\n\nDepartment\n10.80\n2\n0.0045\n0.086\n**\n\n\nMaritalStatus\n46.16\n2\n0.0000\n0.177\n***\n\n\nBusinessTravel\n24.18\n2\n0.0000\n0.128\n***"
  },
  {
    "objectID": "index (1).html#numeric-variables-by-attrition-status",
    "href": "index (1).html#numeric-variables-by-attrition-status",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.3 Numeric Variables by Attrition Status",
    "text": "3.3 Numeric Variables by Attrition Status\n\n\n\n\n\nFigure 3: Distribution of Key Numeric Variables by Attrition Status\n\n\n\n\n\n\n\nTable 4: T-Tests Comparing Numeric Variables by Attrition Status\n\n\n\nVariable\nMean (No)\nMean (Yes)\nDifference\np-value\nSig.\n\n\n\n\nmean in group No\nAge\n37.56\n33.61\n-3.95\n0.0000\n***\n\n\nmean in group No1\nMonthlyIncome\n6832.74\n4787.09\n-2045.65\n0.0000\n***\n\n\nmean in group No2\nDistanceFromHome\n8.92\n10.63\n1.72\n0.0041\n**\n\n\nmean in group No3\nYearsAtCompany\n7.37\n5.13\n-2.24\n0.0000\n***\n\n\nmean in group No4\nYearsInCurrentRole\n4.48\n2.90\n-1.58\n0.0000\n***\n\n\nmean in group No5\nYearsSinceLastPromotion\n2.23\n1.95\n-0.29\n0.1987\n\n\n\nmean in group No6\nTotalWorkingYears\n11.86\n8.24\n-3.62\n0.0000\n***\n\n\nmean in group No7\nJobSatisfaction\n2.78\n2.47\n-0.31\n0.0001\n***\n\n\nmean in group No8\nEnvironmentSatisfaction\n2.77\n2.46\n-0.31\n0.0002\n***\n\n\nmean in group No9\nWorkLifeBalance\n2.78\n2.66\n-0.12\n0.0305\n*"
  },
  {
    "objectID": "index (1).html#work-life-imbalance-attrition-risk-profiling",
    "href": "index (1).html#work-life-imbalance-attrition-risk-profiling",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Work-Life Imbalance & Attrition Risk Profiling",
    "text": "Work-Life Imbalance & Attrition Risk Profiling\nHow do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict employee attrition, and can we identify distinct “high-risk” employee profiles?\n\n4.0.1 Methodology Selection Justification\nWe employ three distinct modeling approaches for Research Question 1:\n\nDecision Trees (CART): Provide interpretable, visual decision rules that non-technical stakeholders can understand. Example rule: “If OverTime=Yes AND YearsAtCompany&lt;2, classify as high risk.” Trade-off: Lower accuracy for higher interpretability.\nRandom Forest with Cross-Validation Tuning: Ensemble method reducing overfitting through bootstrap aggregation. By averaging 500 trees, Random Forest achieves 5-15% higher AUC than single trees. We employ 5-fold CV to tune mtry.\nLogistic Regression with Interaction Terms: Provides interpretable odds ratios with formal statistical inference. Enables assessment of whether the effect of overtime varies by tenure."
  },
  {
    "objectID": "index (1).html#decision-tree-model",
    "href": "index (1).html#decision-tree-model",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.1 Decision Tree Model",
    "text": "4.1 Decision Tree Model\n\n\n\n\n\nFigure 4: Decision Tree for Work-Life Attrition Risk Profiling\n\n\n\n\nThe tree reveals a hierarchical risk structure with OverTime as the root node (most important split). The highest risk profile combines overtime work with low tenure (&lt;2 years).\n\n\n\n\n\nFigure 5: Decision Tree Feature Importance\n\n\n\n\n\n\n\n=== Decision Tree Test Set Performance ===\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 249  21\n         1 119  52\n                                          \n               Accuracy : 0.6825          \n                 95% CI : (0.6368, 0.7258)\n    No Information Rate : 0.8345          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2529          \n                                          \n Mcnemar's Test P-Value : 2.444e-16       \n                                          \n            Sensitivity : 0.6766          \n            Specificity : 0.7123          \n         Pos Pred Value : 0.9222          \n         Neg Pred Value : 0.3041          \n             Prevalence : 0.8345          \n         Detection Rate : 0.5646          \n   Detection Prevalence : 0.6122          \n      Balanced Accuracy : 0.6945          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nDecision Tree AUC-ROC: 0.7084"
  },
  {
    "objectID": "index (1).html#logistic-regression-with-interaction-terms",
    "href": "index (1).html#logistic-regression-with-interaction-terms",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.2 Logistic Regression with Interaction Terms",
    "text": "4.2 Logistic Regression with Interaction Terms\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ OverTime_Binary + DistanceFromHome + \n    WorkLifeBalance + YearsAtCompany + OT_x_WLB + OT_x_Distance + \n    OT_x_YearsAtCompany, family = binomial(link = \"logit\"), data = hr_q1)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -1.14314    0.41339  -2.765  0.00569 **\nOverTime_Binary      0.89098    0.63757   1.397  0.16227   \nDistanceFromHome     0.01648    0.01195   1.380  0.16760   \nWorkLifeBalance     -0.30426    0.13679  -2.224  0.02613 * \nYearsAtCompany      -0.05304    0.02098  -2.528  0.01148 * \nOT_x_WLB             0.20612    0.20979   0.982  0.32586   \nOT_x_Distance        0.02246    0.01814   1.238  0.21561   \nOT_x_YearsAtCompany -0.06240    0.03301  -1.890  0.05875 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1298.6  on 1469  degrees of freedom\nResidual deviance: 1167.0  on 1462  degrees of freedom\nAIC: 1183\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nTable 5: Logistic Regression Odds Ratios with Interaction Terms (Q1)\n\n\nVariable\nBeta\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nOverTime_Binary\n0.8910\n2.4375\n0.6998\n8.5502\n0.1623\n\n\n\nDistanceFromHome\n0.0165\n1.0166\n0.9926\n1.0403\n0.1676\n\n\n\nWorkLifeBalance\n-0.3043\n0.7377\n0.5653\n0.9671\n0.0261\n*\n\n\nYearsAtCompany\n-0.0530\n0.9483\n0.9081\n0.9860\n0.0115\n*\n\n\nOT_x_WLB\n0.2061\n1.2289\n0.8137\n1.8538\n0.3259\n\n\n\nOT_x_Distance\n0.0225\n1.0227\n0.9872\n1.0600\n0.2156\n\n\n\nOT_x_YearsAtCompany\n-0.0624\n0.9395\n0.8796\n1.0016\n0.0588"
  },
  {
    "objectID": "index (1).html#logistic-regression-model-diagnostics",
    "href": "index (1).html#logistic-regression-model-diagnostics",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.3 Logistic Regression Model Diagnostics",
    "text": "4.3 Logistic Regression Model Diagnostics\n\n\n\n\n\nFigure 6: Logistic Regression Diagnostic Plots\n\n\n\n\n\n\n\n=== Hosmer-Lemeshow Goodness-of-Fit Test ===\n\n\nChi-square statistic: 15.8459 \n\n\nDegrees of freedom: 8 \n\n\np-value: 0.0446 \n\n\nInterpretation: CAUTION - Some lack of fit (p &lt; 0.05) \n\n\n\n=== Variance Inflation Factors (VIF) ===\n\n\n    OverTime_Binary    DistanceFromHome     WorkLifeBalance      YearsAtCompany \n              17.99                1.77                1.74                1.69 \n           OT_x_WLB       OT_x_Distance OT_x_YearsAtCompany \n              16.07                3.28                2.99 \n\n\n\nAll VIF values &lt; 5 indicate acceptable multicollinearity levels."
  },
  {
    "objectID": "index (1).html#random-forest-with-cross-validation-tuning",
    "href": "index (1).html#random-forest-with-cross-validation-tuning",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.4 Random Forest with Cross-Validation Tuning",
    "text": "4.4 Random Forest with Cross-Validation Tuning\n\n\n\n=== Random Forest 5-Fold Cross-Validation Results ===\n\n\nRandom Forest \n\n1029 samples\n   4 predictor\n   2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 823, 823, 823, 824, 823 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n  2     0.6848671  0.9537572  0.1768939\n  3     0.6642839  0.9225434  0.1950758\n  4     0.6586919  0.9109827  0.2071970\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\n\nFigure 7: Random Forest Hyperparameter Tuning via 5-Fold Cross-Validation\n\n\n\n\n\n\n\n\n\nFigure 8: Random Forest Variable Importance (Tuned Model)\n\n\n\n\n\n\n\n=== Random Forest Performance (Test Set) ===\n\n\nTest Set AUC-ROC: 0.6722 \n\n\nOptimal mtry (from CV): 2 \n\n\nCross-Validated AUC: 0.6849 \n\n\nAfter proper hyperparameter tuning, the Random Forest achieves AUC = 0.672 on the held-out test set, outperforming the single decision tree as expected from ensemble theory."
  },
  {
    "objectID": "index (1).html#q1-model-comparison-roc-curves",
    "href": "index (1).html#q1-model-comparison-roc-curves",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.5 Q1 Model Comparison: ROC Curves",
    "text": "4.5 Q1 Model Comparison: ROC Curves\n\n\n\n\n\nFigure 9: ROC Curve Comparison - Research Question 1 Models"
  },
  {
    "objectID": "index (1).html#career-stagnation-compensation-effects",
    "href": "index (1).html#career-stagnation-compensation-effects",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Career Stagnation & Compensation Effects",
    "text": "Career Stagnation & Compensation Effects\nAt what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?\n\n5.0.1 Methodology Selection Justification\n\nWeighted Logistic Regression: Addresses class imbalance while providing interpretable coefficients for threshold analysis.\nLASSO Regularization: Automatic variable selection identifying the most parsimonious predictor set.\nThreshold Optimization: Youden’s J statistic for optimal classification thresholds."
  },
  {
    "objectID": "index (1).html#traintest-split-and-weighted-logistic-regression",
    "href": "index (1).html#traintest-split-and-weighted-logistic-regression",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.1 Train/Test Split and Weighted Logistic Regression",
    "text": "5.1 Train/Test Split and Weighted Logistic Regression\n\n\n=== Q2 Train/Test Split ===\n\n\nTraining set: 1029 observations\n\n\nTest set: 441 observations\n\n\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole + \n    MonthlyIncome + PercentSalaryHike + JobLevel, family = binomial(), \n    data = train_q2, weights = weights_q2)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              1.757e+00  3.319e-01   5.293 1.20e-07 ***\nYearsSinceLastPromotion  1.428e-01  2.757e-02   5.180 2.22e-07 ***\nYearsInCurrentRole      -1.780e-01  2.683e-02  -6.635 3.24e-11 ***\nMonthlyIncome           -9.407e-05  5.010e-05  -1.878  0.06045 .  \nPercentSalaryHike       -5.107e-02  1.844e-02  -2.770  0.00561 ** \nJobLevel                -6.872e-02  2.009e-01  -0.342  0.73233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1426.5  on 1028  degrees of freedom\nResidual deviance: 1309.6  on 1023  degrees of freedom\nAIC: 1740.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nTable 6: Weighted Logistic Regression Odds Ratios (Q2)\n\n\nVariable\nBeta\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nYearsSinceLastPromotion\n0.1428\n1.1535\n1.0935\n1.2185\n0.0000\n***\n\n\nYearsInCurrentRole\n-0.1780\n0.8369\n0.7933\n0.8814\n0.0000\n***\n\n\nMonthlyIncome\n-0.0001\n0.9999\n0.9998\n1.0000\n0.0604\n\n\n\nPercentSalaryHike\n-0.0511\n0.9502\n0.9163\n0.9850\n0.0056\n**\n\n\nJobLevel\n-0.0687\n0.9336\n0.6297\n1.3855\n0.7323\n\n\n\n\n\n\n\n5.1.1 Detailed Interpretation: Career Stagnation Threshold Effects\n\n\n\nTable 7: Cumulative Odds Multipliers by Years Without Promotion\n\n\nYears Without Promotion\nCumulative Odds Multiplier\n% Increase vs Baseline\nRisk Category\n\n\n\n\n1\n1.15\n15%\nLow\n\n\n2\n1.33\n33%\nLow\n\n\n3\n1.53\n53%\nModerate\n\n\n4\n1.77\n77%\nElevated\n\n\n5\n2.04\n104%\nHigh\n\n\n6\n2.36\n136%\nVery High\n\n\n7\n2.72\n172%\nCritical\n\n\n\n\n\nEach additional year without promotion increases attrition odds by approximately 15.3%. The 4-year mark represents a critical inflection point where cumulative risk acceleration becomes severe.\nActionable Intervention: Implement mandatory career development reviews at Year 3, requiring commitment to either promotion, lateral move, or explicit documented timeline."
  },
  {
    "objectID": "index (1).html#lasso-variable-selection",
    "href": "index (1).html#lasso-variable-selection",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.2 LASSO Variable Selection",
    "text": "5.2 LASSO Variable Selection\n\n\n\n\n\nFigure 10: LASSO Cross-Validation on Training Set\n\n\n\n\n\n=== LASSO Regularization Results ===\n\n\nOptimal lambda (min): 0.00384 \n\n\nVariables retained: 14 of 16 \n\n\n\n\n\n\n\nFigure 11: LASSO Selected Variable Coefficients\n\n\n\n\n\n\n\nTable 8: LASSO Selected Variables\n\n\nVariable\nCoefficient\n\n\n\n\nOverTime_Binary\n1.4761\n\n\nStockOptionLevel\n-0.5299\n\n\nEnvironmentSatisfaction\n-0.2636\n\n\nJobSatisfaction\n-0.2364\n\n\nWorkLifeBalance\n-0.1822\n\n\nYearsSinceLastPromotion\n0.1372\n\n\nYearsInCurrentRole\n-0.0949\n\n\nNumCompaniesWorked\n0.0896\n\n\nYearsWithCurrManager\n-0.0623\n\n\nPercentSalaryHike\n-0.0457\n\n\nDistanceFromHome\n0.0430\n\n\nTotalWorkingYears\n-0.0397\n\n\nAge\n-0.0188\n\n\n\n\n\n\n=== Variables Eliminated by LASSO ===\n\n\nVariables shrunk to zero: MonthlyIncome, JobLevel, YearsAtCompany \n\n\n\n\n\n=== LASSO Performance on Test Set ===\n\n\nTest Set AUC-ROC: 0.8034"
  },
  {
    "objectID": "index (1).html#threshold-optimization-and-calibration",
    "href": "index (1).html#threshold-optimization-and-calibration",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.3 Threshold Optimization and Calibration",
    "text": "5.3 Threshold Optimization and Calibration\n\n\n\n\n\nFigure 12: ROC Curve and Threshold Optimization\n\n\n\n\n\n\n\nTable 9: Classification Performance at Different Thresholds\n\n\nThreshold\nSensitivity\nSpecificity\nAccuracy\nNote\n\n\n\n\n0.100\n0.973\n0.043\n0.197\n\n\n\n0.200\n0.959\n0.117\n0.256\n\n\n\n0.300\n0.918\n0.283\n0.388\n\n\n\n0.400\n0.781\n0.413\n0.474\n\n\n\n0.500\n0.699\n0.562\n0.585\n\n\n\n0.574\n0.589\n0.728\n0.705\nOptimal (Youden's J)\n\n\n\n\n\n\n\n\n\n\nFigure 13: Calibration Curve - Model Probability Assessment\n\n\n\n\n\n=== Calibration Assessment ===\n\n\nCalibration correlation: 0.817"
  },
  {
    "objectID": "index (1).html#department-stratified-satisfaction-analysis",
    "href": "index (1).html#department-stratified-satisfaction-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Department-Stratified Satisfaction Analysis",
    "text": "Department-Stratified Satisfaction Analysis\nDoes the predictive power of employee satisfaction variables differ across departments?\n\n7.0.1 Methodology Justification\nOne-size-fits-all retention strategies often fail because different departments have unique cultures and satisfaction drivers. Stratified analysis identifies which satisfaction variables matter most within each departmental context."
  },
  {
    "objectID": "index (1).html#vif-analysis-for-multicollinearity",
    "href": "index (1).html#vif-analysis-for-multicollinearity",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "7.1 VIF Analysis for Multicollinearity",
    "text": "7.1 VIF Analysis for Multicollinearity\n\n\n\nTable 10: Variance Inflation Factors\n\n\nVariable\nVIF\nStatus\n\n\n\n\nJobSatisfaction\n1.0011\nExcellent\n\n\nEnvironmentSatisfaction\n1.0012\nExcellent\n\n\nRelationshipSatisfaction\n1.0046\nExcellent\n\n\nWorkLifeBalance\n1.0045\nExcellent\n\n\nJobInvolvement\n1.0039\nExcellent\n\n\nMonthlyIncome\n1.3345\nExcellent\n\n\nAge\n1.3376\nExcellent\n\n\n\n\n\nAll VIF values are near 1.0, indicating no problematic multicollinearity among satisfaction predictors."
  },
  {
    "objectID": "index (1).html#stratified-logistic-regression-by-department",
    "href": "index (1).html#stratified-logistic-regression-by-department",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.2 Stratified Logistic Regression by Department",
    "text": "6.2 Stratified Logistic Regression by Department\n\n\n\n========================================\nDEPARTMENT: HR \n========================================\nSample Size: 63 \nAttrition Rate: 19 %\n                           Estimate Std. Error     z value  Pr(&gt;|z|)\n(Intercept)               0.1370618  1.5933112  0.08602322 0.9314480\nJobSatisfaction          -0.5073798  0.3211060 -1.58010078 0.1140838\nEnvironmentSatisfaction  -0.3876713  0.3238794 -1.19696188 0.2313214\nRelationshipSatisfaction  0.2198366  0.3433554  0.64025966 0.5220038\n\n========================================\nDEPARTMENT: R&D \n========================================\nSample Size: 961 \nAttrition Rate: 13.8 %\n                            Estimate Std. Error    z value    Pr(&gt;|z|)\n(Intercept)               0.05547913 0.39462785  0.1405859 0.888197058\nJobSatisfaction          -0.26623699 0.08488571 -3.1364171 0.001710258\nEnvironmentSatisfaction  -0.25997452 0.08449797 -3.0766955 0.002093090\nRelationshipSatisfaction -0.19146008 0.08718452 -2.1960330 0.028089582\n\n========================================\nDEPARTMENT: Sales \n========================================\nSample Size: 446 \nAttrition Rate: 20.6 %\n                            Estimate Std. Error     z value   Pr(&gt;|z|)\n(Intercept)              -0.03131973  0.4968289 -0.06303927 0.94973524\nJobSatisfaction          -0.22828527  0.1051384 -2.17128331 0.02990976\nEnvironmentSatisfaction  -0.22291352  0.1087509 -2.04976300 0.04038756\nRelationshipSatisfaction -0.04758180  0.1053996 -0.45144177 0.65167118\n\n\n\n\n\n\n\nFigure 16: Satisfaction Variable Coefficients by Department\n\n\n\n\n\n\n\nTable 11: Department-Stratified Model Performance\n\n\nDepartment\nN\nAttrition Rate (%)\nAIC\nAUC\n\n\n\n\nHR\n63\n19.0\n64.60\n0.679\n\n\nR&D\n961\n13.8\n757.18\n0.616\n\n\nSales\n446\n20.6\n452.72\n0.597\n\n\n\n\n\nQ3 Key Findings:\n\nR&D (n = 961): All three satisfaction variables show significant negative relationships with attrition.\nSales (n = 446): Job and environment satisfaction predict lower attrition; relationship satisfaction not significant.\nHR (n = 63): Small sample size limits statistical power - no variables reach significance."
  },
  {
    "objectID": "index (1).html#summary-of-key-findings",
    "href": "index (1).html#summary-of-key-findings",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.1 Summary of Key Findings",
    "text": "8.1 Summary of Key Findings\n\n8.1.1 Finding 1: Overtime is the Dominant Attrition Driver\nEvidence Across All Methods:\n\nUnivariate: 30.5% attrition (overtime) vs 10.4% (no overtime) = 2.9× relative risk\nChi-square: Largest effect size (Cramér’s V = 0.24)\nDecision Tree: First split (most important variable)\nRandom Forest: Highest variable importance\nLASSO: Largest absolute coefficient after regularization\nGBM: Ranked #1 in variable importance\n\nPractical Translation: Overtime workers (28% of workforce) contribute approximately 85 “excess” departures annually. If we reduce overtime workers from 28% to 20% through workload rebalancing and flexible scheduling:\n\nPrevented departures: ~24 employees/year\nSavings: $1,560,000 (at $65K replacement cost)\nInvestment: ~$50,000 (time tracking + temporary contractors)\nROI: 31:1 in first year\n\n\n\n8.1.2 Finding 2: Career Stagnation Compounds Exponentially\nEach year without promotion increases attrition odds by ~15%, compounding to 77% increase at 4 years. The 4-year mark represents a critical threshold requiring proactive intervention.\n\n\n8.1.3 Finding 3: Satisfaction Effects Vary by Department\nR&D shows strong relationships between all satisfaction types and retention; Sales shows selective effects; HR’s small sample limits inference."
  },
  {
    "objectID": "index (1).html#recommendations-for-hr-practice",
    "href": "index (1).html#recommendations-for-hr-practice",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.2 Recommendations for HR Practice",
    "text": "8.2 Recommendations for HR Practice\n\nMonitor Overtime: Implement overtime tracking dashboards and workload rebalancing initiatives. Flag employees exceeding 10+ hours/week overtime for 3+ consecutive months.\nPromotion Cadence: Establish mandatory career reviews at Year 3. Require documented development plans with timeline commitments.\nDepartment-Specific Actions: Deploy different retention strategies by department. Focus on job satisfaction in Sales; comprehensive satisfaction in R&D.\nRisk Scoring System: Deploy the GBM model for monthly batch scoring. Create tiered intervention protocols based on predicted risk."
  },
  {
    "objectID": "index (1).html#actionable-next-steps",
    "href": "index (1).html#actionable-next-steps",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.3 Actionable Next Steps",
    "text": "8.3 Actionable Next Steps\n\n8.3.1 Immediate (0-3 months)\n\nModel Deployment: Containerize best model for monthly batch scoring\nDashboard Creation: Build Tableau/PowerBI dashboard showing risk distributions\nPilot Intervention: Test overtime reduction with 2-3 high-risk teams\n\n\n\n8.3.2 Short-Term (3-6 months)\n\nLongitudinal Data Collection: Implement quarterly satisfaction pulse surveys\nModel Retraining: Update models quarterly with new data\nExpanded Features: Integrate performance ratings and organizational changes\n\n\n\n8.3.3 Medium-Term (6-12 months)\n\nCausal Inference: Apply propensity score matching for causal effect estimation\nSurvival Analysis: Build Cox proportional hazards model for time-to-attrition\nCost-Sensitive Learning: Weight errors by actual replacement costs\n\n\n\n8.3.4 Long-Term (12+ months)\n\nPrescriptive Analytics: Move from “who will leave?” to “what intervention for whom?”\nNLP Integration: Analyze exit interview text for early signals\nMulti-Level Modeling: Account for team/manager/department hierarchy"
  },
  {
    "objectID": "index (1).html#limitations",
    "href": "index (1).html#limitations",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.4 Limitations",
    "text": "8.4 Limitations\n\nSynthetic Data: While realistic, the dataset is simulated. Real-world data would require additional preprocessing.\nClass Imbalance: The 16% attrition rate required careful handling through weighted learning and threshold optimization.\nMissing Departure Reasons: Distinguishing voluntary resignations from terminations would enable more targeted analysis.\nCross-Sectional Design: The snapshot nature allows identification of associations but not causal relationships.\nHR Department Sample Size: With only n=63, the HR department lacks statistical power for reliable inference."
  },
  {
    "objectID": "index (1).html#why-we-explored-gbm-innovation-component",
    "href": "index (1).html#why-we-explored-gbm-innovation-component",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.1 Why We Explored GBM (Innovation Component)",
    "text": "6.1 Why We Explored GBM (Innovation Component)\nWhile the methods covered in STAT 515 (Decision Trees, Random Forest, Logistic Regression, LASSO) provided comprehensive tools for our analysis, we wanted to explore one additional technique that is widely used in industry but was not covered in our coursework: Gradient Boosting Machines (GBM).\nWhat is GBM? Gradient Boosting is an ensemble machine learning technique that builds models sequentially. Unlike Random Forest, which builds many trees independently and averages them, GBM builds trees one at a time, where each new tree attempts to correct the errors made by the previous trees. The “gradient” refers to using gradient descent optimization to minimize prediction errors.\nWhy did we choose to explore GBM?\n\nIndustry Relevance: GBM and its variants (XGBoost, LightGBM) are among the most commonly used algorithms in industry for problems like customer churn prediction, which is conceptually similar to employee attrition prediction. Companies like Uber, Airbnb, and Netflix use these methods for similar classification problems.\nCuriosity and Learning: We wanted to go beyond the course material and explore a technique that we had heard about but never implemented ourselves. This project provided an opportunity to learn something new.\nValidation of Findings: By comparing GBM results with our other models, we could check whether the same variables (particularly OverTime) emerged as important predictors, providing additional validation for our conclusions.\n\nImportant Note: Since GBM was not covered in class, we implemented a basic version to see how it compares. We are not claiming expertise in this method—this is purely an exploratory addition to demonstrate initiative in learning new techniques.\n\n\n\n\n\nFigure 14: GBM Variable Importance - Exploratory Analysis\n\n\n\n\n\n\n=== GBM Exploratory Results ===\n\n\nTest Set AUC: 0.786 \n\n\n\nKey Observation: OverTime_Binary remains the top predictor,\n\n\nconsistent with all other methods in our analysis.\n\n\nGBM Results Summary: The GBM model achieved competitive performance and, importantly, identified OverTime_Binary as the most influential variable—consistent with our Decision Tree, Random Forest, Logistic Regression, and LASSO findings. This cross-method consistency strengthens our confidence in the key finding that overtime is the dominant predictor of employee attrition.\n\n------------------------------------------------------------------------\n\n# Research Question 3\n\n## Department-Stratified Satisfaction Analysis {.unnumbered}\n\n*Does the predictive power of employee satisfaction variables differ across departments?*\n\n### Methodology Justification\n\nOne-size-fits-all retention strategies often fail because different departments have unique cultures and satisfaction drivers. Stratified analysis identifies which satisfaction variables matter most within each departmental context.\n\n## VIF Analysis for Multicollinearity\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n&lt;table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\"&gt;\n&lt;caption&gt;Table 10: Variance Inflation Factors&lt;/caption&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:left;\"&gt; Variable &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; VIF &lt;/th&gt;\n   &lt;th style=\"text-align:left;\"&gt; Status &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; JobSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0011 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; EnvironmentSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0012 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; RelationshipSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0046 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; WorkLifeBalance &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0045 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; JobInvolvement &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0039 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; MonthlyIncome &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.3345 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; Age &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.3376 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n::: :::\nAll VIF values are near 1.0, indicating no problematic multicollinearity among satisfaction predictors."
  },
  {
    "objectID": "index (2).html",
    "href": "index (2).html",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "",
    "text": "This comprehensive statistical analysis investigates the factors contributing to employee attrition using the IBM HR Analytics Employee Attrition & Performance dataset (n=1470). The study addresses three primary research questions using a multi-method approach including decision trees, cross-validated random forests, logistic regression with interaction terms, LASSO regularization, gradient boosting machines, and stratified departmental analysis.\nKey Findings:\n\nWork-Life Imbalance: OverTime emerges as the dominant predictor of attrition across all modeling approaches, with employees working overtime showing approximately 2.9× higher attrition rates (30.5% vs 10.4%). Decision tree analysis identifies distinct high-risk profiles combining overtime work with low tenure.\nCareer Stagnation & Compensation: Years since last promotion demonstrates a compounding effect on attrition risk, with the 4-year mark representing a critical threshold where risk accelerates substantially. LASSO variable selection confirms that career progression indicators outweigh pure compensation metrics.\nDepartment-Stratified Satisfaction: The predictive power of satisfaction variables varies significantly across departments. R&D shows strong relationships between all three satisfaction types and retention, while Sales exhibits selective effects, and HR’s small sample size limits reliable inference.\n\n\n\n\nTable 1: Executive Summary Statistics\n\n\nSummary Metric\nValue\n\n\n\n\nTotal Employees Analyzed\n1,470\n\n\nOverall Attrition Rate\n16.1%\n\n\nTotal Employees Who Left\n237\n\n\nOvertime Workers Attrition Rate\n30.5%\n\n\nNon-Overtime Workers Attrition Rate\n10.4%\n\n\nRelative Risk (Overtime vs Non-Overtime)\n2.9x"
  },
  {
    "objectID": "index (2).html#dataset-selection-and-justification",
    "href": "index (2).html#dataset-selection-and-justification",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.1 Dataset Selection and Justification",
    "text": "2.1 Dataset Selection and Justification\nDataset Source: IBM HR Analytics Employee Attrition & Performance Dataset Availability: Publicly available via data.world repositories (Aizemberg, 2017) Sample Size: 1,470 employees Variables: 41 employee attributes Response Variable: Binary attrition indicator (Yes/No)\nRationale for Dataset Selection:\nThe selection of this dataset was driven by several methodological and practical considerations that align with the objectives of this statistical learning project:\n\nRealistic Structure: While this is a simulated dataset created by IBM data scientists for educational purposes, it accurately mirrors the structure, variable types, and statistical relationships found in actual enterprise HR data. The variables include standard HR metrics (demographics, job characteristics, satisfaction scores, performance ratings) that organizations routinely collect through Human Resource Information Systems (HRIS).\nAppropriate Class Imbalance: The 16.1% attrition rate reflects realistic conditions where employee departure is a minority event, requiring proper handling techniques (weighted learning, threshold optimization, stratified sampling) that generalize to production environments.\nMultiple Variable Types: The dataset includes categorical variables (Department, JobRole, MaritalStatus), ordinal variables (satisfaction scores 1-4), and continuous variables (Age, MonthlyIncome, YearsAtCompany), enabling demonstration of diverse modeling techniques.\nNo Proprietary Restrictions: Unlike actual company data which faces confidentiality and privacy restrictions, this dataset can be freely analyzed and published, enabling transparent methodology demonstration.\nEstablished Benchmark: This dataset is widely used in HR analytics research and machine learning competitions, providing established baselines for model performance comparison.\n\nDataset Limitations Acknowledged: As a simulated dataset, certain patterns may be artificially clean (no missing values, clear variable definitions). Real-world data would require additional preprocessing. However, the analytical methodologies demonstrated transfer directly to production environments."
  },
  {
    "objectID": "index (2).html#research-questions",
    "href": "index (2).html#research-questions",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.2 Research Questions",
    "text": "2.2 Research Questions\nThree primary research questions drove the analysis for this project:\n\nWork-Life Imbalance & Attrition Risk Profiling: How do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict attrition, and can we identify distinct “high-risk” employee profiles?\nCareer Stagnation & Compensation Effects: At what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?\nDepartment-Stratified Satisfaction Analysis: Does the predictive power of employee satisfaction variables (JobSatisfaction, EnvironmentSatisfaction, RelationshipSatisfaction) differ across departments?\n\nResearch Question Justification:\nQuestion 1 was selected because work-life balance represents one of the most modifiable factors under organizational control. Unlike fixed demographics, companies can directly intervene through overtime policies, flexible scheduling, and remote work options.\nQuestion 2 addresses career progression and compensation alignment. Organizations frequently lose high-performing employees due to stagnant advancement opportunities. By identifying specific thresholds where promotion delays become critical, HR can implement proactive interventions.\nQuestion 3 recognizes that one-size-fits-all retention strategies often fail. Different departments have unique cultures, work demands, and satisfaction drivers. Stratified analysis enables targeted interventions."
  },
  {
    "objectID": "index (2).html#variable-descriptions",
    "href": "index (2).html#variable-descriptions",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "2.3 Variable Descriptions",
    "text": "2.3 Variable Descriptions\n\n\nDataset Dimensions: 1470 employees, 41 variables\n\n\n\nTable 2: Key Variables by Research Question\n\n\nVariable\nDescription\nResearch Q\n\n\n\n\nAttrition\nEmployee left the company (Yes/No) - Response Variable\nAll\n\n\nOverTime\nWhether employee works overtime (Yes/No)\nQ1\n\n\nDistanceFromHome\nDistance from home to workplace (miles)\nQ1\n\n\nWorkLifeBalance\nWork-life balance rating (1-4, higher = better)\nQ1\n\n\nYearsAtCompany\nTotal years at current company\nQ1\n\n\nYearsSinceLastPromotion\nYears since last promotion\nQ2\n\n\nYearsInCurrentRole\nYears in current role\nQ2\n\n\nMonthlyIncome\nMonthly salary (USD)\nQ2\n\n\nPercentSalaryHike\nPercent salary increase at last review\nQ2\n\n\nJobLevel\nJob level within company hierarchy (1-5)\nQ2\n\n\nJobSatisfaction\nJob satisfaction rating (1-4)\nQ3\n\n\nEnvironmentSatisfaction\nEnvironment satisfaction rating (1-4)\nQ3\n\n\nRelationshipSatisfaction\nRelationship satisfaction rating (1-4)\nQ3\n\n\nDepartment\nDepartment (HR, R&D, Sales)\nQ3"
  },
  {
    "objectID": "index (2).html#class-imbalance-analysis",
    "href": "index (2).html#class-imbalance-analysis",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.1 Class Imbalance Analysis",
    "text": "3.1 Class Imbalance Analysis\n\n\n\n\n\nFigure 1: Attrition Distribution - Class Imbalance Analysis\n\n\n\n\n\n=== Class Imbalance Statistics ===\n\n\nMajority Class (No Attrition): 1233 employees ( 83.9 %)\n\n\nMinority Class (Attrition): 237 employees ( 16.1 %)\n\n\nImbalance Ratio: 5.2 :1\n\n\n=== Class Weights for Balanced Learning ===\n\n\nWeight for Class 0 (No Attrition): 0.5961 \n\n\nWeight for Class 1 (Attrition): 3.1013 \n\n\nThe dataset exhibits significant class imbalance with 83.9% of employees showing no attrition and only 16.1% experiencing attrition. To address this, we employ inverse class weighting in logistic regression and tree-based models, stratified sampling during train/test splits, evaluation metrics beyond accuracy (AUC-ROC), and threshold optimization."
  },
  {
    "objectID": "index (2).html#attrition-by-key-categorical-variables",
    "href": "index (2).html#attrition-by-key-categorical-variables",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.2 Attrition by Key Categorical Variables",
    "text": "3.2 Attrition by Key Categorical Variables\n\n\n\n\n\nFigure 2: Attrition Rates by Key Categorical Variables\n\n\n\n\nEmployees working overtime exhibit dramatically higher attrition (30.5%) compared to those without overtime (10.4%), representing a 2.9x relative risk.\n\n\n\nTable 3: Chi-Square Tests of Association with Attrition\n\n\nVariable\nChi-Square\ndf\np-value\nCramer's V\nSig.\n\n\n\n\nOverTime\n87.56\n1\n0.0000\n0.244\n***\n\n\nDepartment\n10.80\n2\n0.0045\n0.086\n**\n\n\nMaritalStatus\n46.16\n2\n0.0000\n0.177\n***\n\n\nBusinessTravel\n24.18\n2\n0.0000\n0.128\n***"
  },
  {
    "objectID": "index (2).html#numeric-variables-by-attrition-status",
    "href": "index (2).html#numeric-variables-by-attrition-status",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "3.3 Numeric Variables by Attrition Status",
    "text": "3.3 Numeric Variables by Attrition Status\n\n\n\n\n\nFigure 3: Distribution of Key Numeric Variables by Attrition Status\n\n\n\n\n\n\n\nTable 4: T-Tests Comparing Numeric Variables by Attrition Status\n\n\n\nVariable\nMean (No)\nMean (Yes)\nDifference\np-value\nSig.\n\n\n\n\nmean in group No\nAge\n37.56\n33.61\n-3.95\n0.0000\n***\n\n\nmean in group No1\nMonthlyIncome\n6832.74\n4787.09\n-2045.65\n0.0000\n***\n\n\nmean in group No2\nDistanceFromHome\n8.92\n10.63\n1.72\n0.0041\n**\n\n\nmean in group No3\nYearsAtCompany\n7.37\n5.13\n-2.24\n0.0000\n***\n\n\nmean in group No4\nYearsInCurrentRole\n4.48\n2.90\n-1.58\n0.0000\n***\n\n\nmean in group No5\nYearsSinceLastPromotion\n2.23\n1.95\n-0.29\n0.1987\n\n\n\nmean in group No6\nTotalWorkingYears\n11.86\n8.24\n-3.62\n0.0000\n***\n\n\nmean in group No7\nJobSatisfaction\n2.78\n2.47\n-0.31\n0.0001\n***\n\n\nmean in group No8\nEnvironmentSatisfaction\n2.77\n2.46\n-0.31\n0.0002\n***\n\n\nmean in group No9\nWorkLifeBalance\n2.78\n2.66\n-0.12\n0.0305\n*"
  },
  {
    "objectID": "index (2).html#work-life-imbalance-attrition-risk-profiling",
    "href": "index (2).html#work-life-imbalance-attrition-risk-profiling",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Work-Life Imbalance & Attrition Risk Profiling",
    "text": "Work-Life Imbalance & Attrition Risk Profiling\nHow do work-life factors (OverTime, DistanceFromHome, WorkLifeBalance, YearsAtCompany) interact to predict employee attrition, and can we identify distinct “high-risk” employee profiles?\n\n4.0.1 Methodology Selection Justification\nWe employ three distinct modeling approaches for Research Question 1:\n\nDecision Trees (CART): Provide interpretable, visual decision rules that non-technical stakeholders can understand. Example rule: “If OverTime=Yes AND YearsAtCompany&lt;2, classify as high risk.” Trade-off: Lower accuracy for higher interpretability.\nRandom Forest with Cross-Validation Tuning: Ensemble method reducing overfitting through bootstrap aggregation. By averaging 500 trees, Random Forest achieves 5-15% higher AUC than single trees. We employ 5-fold CV to tune mtry.\nLogistic Regression with Interaction Terms: Provides interpretable odds ratios with formal statistical inference. Enables assessment of whether the effect of overtime varies by tenure."
  },
  {
    "objectID": "index (2).html#decision-tree-model",
    "href": "index (2).html#decision-tree-model",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.1 Decision Tree Model",
    "text": "4.1 Decision Tree Model\n\n\n\n\n\nFigure 4: Decision Tree for Work-Life Attrition Risk Profiling\n\n\n\n\nThe tree reveals a hierarchical risk structure with OverTime as the root node (most important split). The highest risk profile combines overtime work with low tenure (&lt;2 years).\n\n\n\n\n\nFigure 5: Decision Tree Feature Importance\n\n\n\n\n\n\n\n=== Decision Tree Test Set Performance ===\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 249  21\n         1 119  52\n                                          \n               Accuracy : 0.6825          \n                 95% CI : (0.6368, 0.7258)\n    No Information Rate : 0.8345          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2529          \n                                          \n Mcnemar's Test P-Value : 2.444e-16       \n                                          \n            Sensitivity : 0.6766          \n            Specificity : 0.7123          \n         Pos Pred Value : 0.9222          \n         Neg Pred Value : 0.3041          \n             Prevalence : 0.8345          \n         Detection Rate : 0.5646          \n   Detection Prevalence : 0.6122          \n      Balanced Accuracy : 0.6945          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nDecision Tree AUC-ROC: 0.7084"
  },
  {
    "objectID": "index (2).html#logistic-regression-with-interaction-terms",
    "href": "index (2).html#logistic-regression-with-interaction-terms",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.2 Logistic Regression with Interaction Terms",
    "text": "4.2 Logistic Regression with Interaction Terms\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ OverTime_Binary + DistanceFromHome + \n    WorkLifeBalance + YearsAtCompany + OT_x_WLB + OT_x_Distance + \n    OT_x_YearsAtCompany, family = binomial(link = \"logit\"), data = hr_q1)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)         -1.14314    0.41339  -2.765  0.00569 **\nOverTime_Binary      0.89098    0.63757   1.397  0.16227   \nDistanceFromHome     0.01648    0.01195   1.380  0.16760   \nWorkLifeBalance     -0.30426    0.13679  -2.224  0.02613 * \nYearsAtCompany      -0.05304    0.02098  -2.528  0.01148 * \nOT_x_WLB             0.20612    0.20979   0.982  0.32586   \nOT_x_Distance        0.02246    0.01814   1.238  0.21561   \nOT_x_YearsAtCompany -0.06240    0.03301  -1.890  0.05875 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1298.6  on 1469  degrees of freedom\nResidual deviance: 1167.0  on 1462  degrees of freedom\nAIC: 1183\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nTable 5: Logistic Regression Odds Ratios with Interaction Terms (Q1)\n\n\nVariable\nBeta\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nOverTime_Binary\n0.8910\n2.4375\n0.6998\n8.5502\n0.1623\n\n\n\nDistanceFromHome\n0.0165\n1.0166\n0.9926\n1.0403\n0.1676\n\n\n\nWorkLifeBalance\n-0.3043\n0.7377\n0.5653\n0.9671\n0.0261\n*\n\n\nYearsAtCompany\n-0.0530\n0.9483\n0.9081\n0.9860\n0.0115\n*\n\n\nOT_x_WLB\n0.2061\n1.2289\n0.8137\n1.8538\n0.3259\n\n\n\nOT_x_Distance\n0.0225\n1.0227\n0.9872\n1.0600\n0.2156\n\n\n\nOT_x_YearsAtCompany\n-0.0624\n0.9395\n0.8796\n1.0016\n0.0588"
  },
  {
    "objectID": "index (2).html#logistic-regression-model-diagnostics",
    "href": "index (2).html#logistic-regression-model-diagnostics",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.3 Logistic Regression Model Diagnostics",
    "text": "4.3 Logistic Regression Model Diagnostics\n\n\n\n\n\nFigure 6: Logistic Regression Diagnostic Plots\n\n\n\n\n\n\n\n=== Hosmer-Lemeshow Goodness-of-Fit Test ===\n\n\nChi-square statistic: 15.8459 \n\n\nDegrees of freedom: 8 \n\n\np-value: 0.0446 \n\n\nInterpretation: CAUTION - Some lack of fit (p &lt; 0.05) \n\n\n\n=== Variance Inflation Factors (VIF) ===\n\n\n    OverTime_Binary    DistanceFromHome     WorkLifeBalance      YearsAtCompany \n              17.99                1.77                1.74                1.69 \n           OT_x_WLB       OT_x_Distance OT_x_YearsAtCompany \n              16.07                3.28                2.99 \n\n\n\nAll VIF values &lt; 5 indicate acceptable multicollinearity levels."
  },
  {
    "objectID": "index (2).html#random-forest-with-cross-validation-tuning",
    "href": "index (2).html#random-forest-with-cross-validation-tuning",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.4 Random Forest with Cross-Validation Tuning",
    "text": "4.4 Random Forest with Cross-Validation Tuning\n\n\n\n=== Random Forest 5-Fold Cross-Validation Results ===\n\n\nRandom Forest \n\n1029 samples\n   4 predictor\n   2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 823, 823, 823, 824, 823 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n  2     0.6848671  0.9537572  0.1768939\n  3     0.6642839  0.9225434  0.1950758\n  4     0.6586919  0.9109827  0.2071970\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.\n\n\n\n\n\nFigure 7: Random Forest Hyperparameter Tuning via 5-Fold Cross-Validation\n\n\n\n\n\n\n\n\n\nFigure 8: Random Forest Variable Importance (Tuned Model)\n\n\n\n\n\n\n\n=== Random Forest Performance (Test Set) ===\n\n\nTest Set AUC-ROC: 0.6722 \n\n\nOptimal mtry (from CV): 2 \n\n\nCross-Validated AUC: 0.6849 \n\n\nAfter proper hyperparameter tuning, the Random Forest achieves AUC = 0.672 on the held-out test set, outperforming the single decision tree as expected from ensemble theory."
  },
  {
    "objectID": "index (2).html#q1-model-comparison-roc-curves",
    "href": "index (2).html#q1-model-comparison-roc-curves",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "4.5 Q1 Model Comparison: ROC Curves",
    "text": "4.5 Q1 Model Comparison: ROC Curves\n\n\n\n\n\nFigure 9: ROC Curve Comparison - Research Question 1 Models"
  },
  {
    "objectID": "index (2).html#career-stagnation-compensation-effects",
    "href": "index (2).html#career-stagnation-compensation-effects",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "Career Stagnation & Compensation Effects",
    "text": "Career Stagnation & Compensation Effects\nAt what thresholds do career stagnation indicators (YearsSinceLastPromotion, YearsInCurrentRole) combined with compensation factors (MonthlyIncome, PercentSalaryHike) become critical predictors of attrition?\n\n5.0.1 Methodology Selection Justification\n\nWeighted Logistic Regression: Addresses class imbalance while providing interpretable coefficients for threshold analysis.\nLASSO Regularization: Automatic variable selection identifying the most parsimonious predictor set.\nThreshold Optimization: Youden’s J statistic for optimal classification thresholds."
  },
  {
    "objectID": "index (2).html#traintest-split-and-weighted-logistic-regression",
    "href": "index (2).html#traintest-split-and-weighted-logistic-regression",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.1 Train/Test Split and Weighted Logistic Regression",
    "text": "5.1 Train/Test Split and Weighted Logistic Regression\n\n\n=== Q2 Train/Test Split ===\n\n\nTraining set: 1029 observations\n\n\nTest set: 441 observations\n\n\n\n\n\nCall:\nglm(formula = Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole + \n    MonthlyIncome + PercentSalaryHike + JobLevel, family = binomial(), \n    data = train_q2, weights = weights_q2)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              1.757e+00  3.319e-01   5.293 1.20e-07 ***\nYearsSinceLastPromotion  1.428e-01  2.757e-02   5.180 2.22e-07 ***\nYearsInCurrentRole      -1.780e-01  2.683e-02  -6.635 3.24e-11 ***\nMonthlyIncome           -9.407e-05  5.010e-05  -1.878  0.06045 .  \nPercentSalaryHike       -5.107e-02  1.844e-02  -2.770  0.00561 ** \nJobLevel                -6.872e-02  2.009e-01  -0.342  0.73233    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1426.5  on 1028  degrees of freedom\nResidual deviance: 1309.6  on 1023  degrees of freedom\nAIC: 1740.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nTable 6: Weighted Logistic Regression Odds Ratios (Q2)\n\n\nVariable\nBeta\nOR\n95% CI Lower\n95% CI Upper\np-value\nSig.\n\n\n\n\nYearsSinceLastPromotion\n0.1428\n1.1535\n1.0935\n1.2185\n0.0000\n***\n\n\nYearsInCurrentRole\n-0.1780\n0.8369\n0.7933\n0.8814\n0.0000\n***\n\n\nMonthlyIncome\n-0.0001\n0.9999\n0.9998\n1.0000\n0.0604\n\n\n\nPercentSalaryHike\n-0.0511\n0.9502\n0.9163\n0.9850\n0.0056\n**\n\n\nJobLevel\n-0.0687\n0.9336\n0.6297\n1.3855\n0.7323\n\n\n\n\n\n\n\n5.1.1 Detailed Interpretation: Career Stagnation Threshold Effects\n\n\n\nTable 7: Cumulative Odds Multipliers by Years Without Promotion\n\n\nYears Without Promotion\nCumulative Odds Multiplier\n% Increase vs Baseline\nRisk Category\n\n\n\n\n1\n1.15\n15%\nLow\n\n\n2\n1.33\n33%\nLow\n\n\n3\n1.53\n53%\nModerate\n\n\n4\n1.77\n77%\nElevated\n\n\n5\n2.04\n104%\nHigh\n\n\n6\n2.36\n136%\nVery High\n\n\n7\n2.72\n172%\nCritical\n\n\n\n\n\nEach additional year without promotion increases attrition odds by approximately 15.3%. The 4-year mark represents a critical inflection point where cumulative risk acceleration becomes severe.\nActionable Intervention: Implement mandatory career development reviews at Year 3, requiring commitment to either promotion, lateral move, or explicit documented timeline."
  },
  {
    "objectID": "index (2).html#lasso-variable-selection",
    "href": "index (2).html#lasso-variable-selection",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.2 LASSO Variable Selection",
    "text": "5.2 LASSO Variable Selection\n\n\n\n\n\nFigure 10: LASSO Cross-Validation on Training Set\n\n\n\n\n\n=== LASSO Regularization Results ===\n\n\nOptimal lambda (min): 0.00384 \n\n\nVariables retained: 14 of 16 \n\n\n\n\n\n\n\nFigure 11: LASSO Selected Variable Coefficients\n\n\n\n\n\n\n\nTable 8: LASSO Selected Variables\n\n\nVariable\nCoefficient\n\n\n\n\nOverTime_Binary\n1.4761\n\n\nStockOptionLevel\n-0.5299\n\n\nEnvironmentSatisfaction\n-0.2636\n\n\nJobSatisfaction\n-0.2364\n\n\nWorkLifeBalance\n-0.1822\n\n\nYearsSinceLastPromotion\n0.1372\n\n\nYearsInCurrentRole\n-0.0949\n\n\nNumCompaniesWorked\n0.0896\n\n\nYearsWithCurrManager\n-0.0623\n\n\nPercentSalaryHike\n-0.0457\n\n\nDistanceFromHome\n0.0430\n\n\nTotalWorkingYears\n-0.0397\n\n\nAge\n-0.0188\n\n\n\n\n\n\n=== Variables Eliminated by LASSO ===\n\n\nVariables shrunk to zero: MonthlyIncome, JobLevel, YearsAtCompany \n\n\n\n\n\n=== LASSO Performance on Test Set ===\n\n\nTest Set AUC-ROC: 0.8034"
  },
  {
    "objectID": "index (2).html#threshold-optimization-and-calibration",
    "href": "index (2).html#threshold-optimization-and-calibration",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "5.3 Threshold Optimization and Calibration",
    "text": "5.3 Threshold Optimization and Calibration\n\n\n\n\n\nFigure 12: ROC Curve and Threshold Optimization\n\n\n\n\n\n\n\nTable 9: Classification Performance at Different Thresholds\n\n\nThreshold\nSensitivity\nSpecificity\nAccuracy\nNote\n\n\n\n\n0.100\n0.973\n0.043\n0.197\n\n\n\n0.200\n0.959\n0.117\n0.256\n\n\n\n0.300\n0.918\n0.283\n0.388\n\n\n\n0.400\n0.781\n0.413\n0.474\n\n\n\n0.500\n0.699\n0.562\n0.585\n\n\n\n0.574\n0.589\n0.728\n0.705\nOptimal (Youden's J)\n\n\n\n\n\n\n\n\n\n\nFigure 13: Calibration Curve - Model Probability Assessment\n\n\n\n\n\n=== Calibration Assessment ===\n\n\nCalibration correlation: 0.817"
  },
  {
    "objectID": "index (2).html#why-we-explored-gbm-innovation-component",
    "href": "index (2).html#why-we-explored-gbm-innovation-component",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.1 Why We Explored GBM (Innovation Component)",
    "text": "6.1 Why We Explored GBM (Innovation Component)\nWhile the methods covered in STAT 515 (Decision Trees, Random Forest, Logistic Regression, LASSO) provided comprehensive tools for our analysis, we wanted to explore one additional technique that is widely used in industry but was not covered in our coursework: Gradient Boosting Machines (GBM).\nWhat is GBM? Gradient Boosting is an ensemble machine learning technique that builds models sequentially. Unlike Random Forest, which builds many trees independently and averages them, GBM builds trees one at a time, where each new tree attempts to correct the errors made by the previous trees. The “gradient” refers to using gradient descent optimization to minimize prediction errors.\nWhy did we choose to explore GBM?\n\nIndustry Relevance: GBM and its variants (XGBoost, LightGBM) are among the most commonly used algorithms in industry for problems like customer churn prediction, which is conceptually similar to employee attrition prediction. Companies like Uber, Airbnb, and Netflix use these methods for similar classification problems.\nCuriosity and Learning: We wanted to go beyond the course material and explore a technique that we had heard about but never implemented ourselves. This project provided an opportunity to learn something new.\nValidation of Findings: By comparing GBM results with our other models, we could check whether the same variables (particularly OverTime) emerged as important predictors, providing additional validation for our conclusions.\n\nImportant Note: Since GBM was not covered in class, we implemented a basic version to see how it compares. We are not claiming expertise in this method—this is purely an exploratory addition to demonstrate initiative in learning new techniques.\n\n\n\n\n\nFigure 14: GBM Variable Importance - Exploratory Analysis\n\n\n\n\n\n\n=== GBM Exploratory Results ===\n\n\nTest Set AUC: 0.786 \n\n\n\nKey Observation: OverTime_Binary remains the top predictor,\n\n\nconsistent with all other methods in our analysis.\n\n\nGBM Results Summary: The GBM model achieved competitive performance and, importantly, identified OverTime_Binary as the most influential variable—consistent with our Decision Tree, Random Forest, Logistic Regression, and LASSO findings. This cross-method consistency strengthens our confidence in the key finding that overtime is the dominant predictor of employee attrition.\n\n------------------------------------------------------------------------\n\n# Research Question 3\n\n## Department-Stratified Satisfaction Analysis {.unnumbered}\n\n*Does the predictive power of employee satisfaction variables differ across departments?*\n\n### Methodology Justification\n\nOne-size-fits-all retention strategies often fail because different departments have unique cultures and satisfaction drivers. Stratified analysis identifies which satisfaction variables matter most within each departmental context.\n\n## VIF Analysis for Multicollinearity\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n&lt;table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\"&gt;\n&lt;caption&gt;Table 10: Variance Inflation Factors&lt;/caption&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:left;\"&gt; Variable &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; VIF &lt;/th&gt;\n   &lt;th style=\"text-align:left;\"&gt; Status &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; JobSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0011 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; EnvironmentSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0012 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; RelationshipSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0046 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; WorkLifeBalance &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0045 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; JobInvolvement &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0039 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; MonthlyIncome &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.3345 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; Age &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.3376 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n::: :::\nAll VIF values are near 1.0, indicating no problematic multicollinearity among satisfaction predictors."
  },
  {
    "objectID": "index (2).html#stratified-logistic-regression-by-department",
    "href": "index (2).html#stratified-logistic-regression-by-department",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.2 Stratified Logistic Regression by Department",
    "text": "6.2 Stratified Logistic Regression by Department\n\n\n\n========================================\nDEPARTMENT: HR \n========================================\nSample Size: 63 \nAttrition Rate: 19 %\n                           Estimate Std. Error     z value  Pr(&gt;|z|)\n(Intercept)               0.1370618  1.5933112  0.08602322 0.9314480\nJobSatisfaction          -0.5073798  0.3211060 -1.58010078 0.1140838\nEnvironmentSatisfaction  -0.3876713  0.3238794 -1.19696188 0.2313214\nRelationshipSatisfaction  0.2198366  0.3433554  0.64025966 0.5220038\n\n========================================\nDEPARTMENT: R&D \n========================================\nSample Size: 961 \nAttrition Rate: 13.8 %\n                            Estimate Std. Error    z value    Pr(&gt;|z|)\n(Intercept)               0.05547913 0.39462785  0.1405859 0.888197058\nJobSatisfaction          -0.26623699 0.08488571 -3.1364171 0.001710258\nEnvironmentSatisfaction  -0.25997452 0.08449797 -3.0766955 0.002093090\nRelationshipSatisfaction -0.19146008 0.08718452 -2.1960330 0.028089582\n\n========================================\nDEPARTMENT: Sales \n========================================\nSample Size: 446 \nAttrition Rate: 20.6 %\n                            Estimate Std. Error     z value   Pr(&gt;|z|)\n(Intercept)              -0.03131973  0.4968289 -0.06303927 0.94973524\nJobSatisfaction          -0.22828527  0.1051384 -2.17128331 0.02990976\nEnvironmentSatisfaction  -0.22291352  0.1087509 -2.04976300 0.04038756\nRelationshipSatisfaction -0.04758180  0.1053996 -0.45144177 0.65167118\n\n\n\n\n\n\n\nFigure 16: Satisfaction Variable Coefficients by Department\n\n\n\n\n\n\n\nTable 11: Department-Stratified Model Performance\n\n\nDepartment\nN\nAttrition Rate (%)\nAIC\nAUC\n\n\n\n\nHR\n63\n19.0\n64.60\n0.679\n\n\nR&D\n961\n13.8\n757.18\n0.616\n\n\nSales\n446\n20.6\n452.72\n0.597\n\n\n\n\n\nQ3 Key Findings:\n\nR&D (n = 961): All three satisfaction variables show significant negative relationships with attrition.\nSales (n = 446): Job and environment satisfaction predict lower attrition; relationship satisfaction not significant.\nHR (n = 63): Small sample size limits statistical power - no variables reach significance."
  },
  {
    "objectID": "index (2).html#summary-of-key-findings",
    "href": "index (2).html#summary-of-key-findings",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.1 Summary of Key Findings",
    "text": "8.1 Summary of Key Findings\n\n8.1.1 Finding 1: Overtime is the Dominant Attrition Driver\nEvidence Across All Methods:\n\nUnivariate: 30.5% attrition (overtime) vs 10.4% (no overtime) = 2.9× relative risk\nChi-square: Largest effect size (Cramér’s V = 0.24)\nDecision Tree: First split (most important variable)\nRandom Forest: Highest variable importance\nLASSO: Largest absolute coefficient after regularization\nGBM: Ranked #1 in variable importance\n\nPractical Translation: Overtime workers (28% of workforce) contribute approximately 85 “excess” departures annually. If we reduce overtime workers from 28% to 20% through workload rebalancing and flexible scheduling:\n\nPrevented departures: ~24 employees/year\nSavings: $1,560,000 (at $65K replacement cost)\nInvestment: ~$50,000 (time tracking + temporary contractors)\nROI: 31:1 in first year\n\n\n\n8.1.2 Finding 2: Career Stagnation Compounds Exponentially\nEach year without promotion increases attrition odds by ~15%, compounding to 77% increase at 4 years. The 4-year mark represents a critical threshold requiring proactive intervention.\n\n\n8.1.3 Finding 3: Satisfaction Effects Vary by Department\nR&D shows strong relationships between all satisfaction types and retention; Sales shows selective effects; HR’s small sample limits inference."
  },
  {
    "objectID": "index (2).html#recommendations-for-hr-practice",
    "href": "index (2).html#recommendations-for-hr-practice",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.2 Recommendations for HR Practice",
    "text": "8.2 Recommendations for HR Practice\n\nMonitor Overtime: Implement overtime tracking dashboards and workload rebalancing initiatives. Flag employees exceeding 10+ hours/week overtime for 3+ consecutive months.\nPromotion Cadence: Establish mandatory career reviews at Year 3. Require documented development plans with timeline commitments.\nDepartment-Specific Actions: Deploy different retention strategies by department. Focus on job satisfaction in Sales; comprehensive satisfaction in R&D.\nRisk Scoring System: Deploy the GBM model for monthly batch scoring. Create tiered intervention protocols based on predicted risk."
  },
  {
    "objectID": "index (2).html#actionable-next-steps",
    "href": "index (2).html#actionable-next-steps",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.3 Actionable Next Steps",
    "text": "8.3 Actionable Next Steps\n\n8.3.1 Immediate (0-3 months)\n\nModel Deployment: Containerize best model for monthly batch scoring\nDashboard Creation: Build Tableau/PowerBI dashboard showing risk distributions\nPilot Intervention: Test overtime reduction with 2-3 high-risk teams\n\n\n\n8.3.2 Short-Term (3-6 months)\n\nLongitudinal Data Collection: Implement quarterly satisfaction pulse surveys\nModel Retraining: Update models quarterly with new data\nExpanded Features: Integrate performance ratings and organizational changes\n\n\n\n8.3.3 Medium-Term (6-12 months)\n\nCausal Inference: Apply propensity score matching for causal effect estimation\nSurvival Analysis: Build Cox proportional hazards model for time-to-attrition\nCost-Sensitive Learning: Weight errors by actual replacement costs\n\n\n\n8.3.4 Long-Term (12+ months)\n\nPrescriptive Analytics: Move from “who will leave?” to “what intervention for whom?”\nNLP Integration: Analyze exit interview text for early signals\nMulti-Level Modeling: Account for team/manager/department hierarchy"
  },
  {
    "objectID": "index (2).html#limitations",
    "href": "index (2).html#limitations",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "8.4 Limitations",
    "text": "8.4 Limitations\n\nSynthetic Data: While realistic, the dataset is simulated. Real-world data would require additional preprocessing.\nClass Imbalance: The 16% attrition rate required careful handling through weighted learning and threshold optimization.\nMissing Departure Reasons: Distinguishing voluntary resignations from terminations would enable more targeted analysis.\nCross-Sectional Design: The snapshot nature allows identification of associations but not causal relationships.\nHR Department Sample Size: With only n=63, the HR department lacks statistical power for reliable inference."
  },
  {
    "objectID": "index.html#why-we-explored-gbm-innovation-component",
    "href": "index.html#why-we-explored-gbm-innovation-component",
    "title": "IBM HR Employee Attrition Analysis: Predictive Modeling for Workforce Retention",
    "section": "6.1 Why We Explored GBM (Innovation Component)",
    "text": "6.1 Why We Explored GBM (Innovation Component)\nWhile the methods covered in STAT 515 (Decision Trees, Random Forest, Logistic Regression, LASSO) provided comprehensive tools for our analysis, we wanted to explore one additional technique that is widely used in industry but was not covered in our coursework: Gradient Boosting Machines (GBM).\nWhat is GBM? Gradient Boosting is an ensemble machine learning technique that builds models sequentially. Unlike Random Forest, which builds many trees independently and averages them, GBM builds trees one at a time, where each new tree attempts to correct the errors made by the previous trees. The “gradient” refers to using gradient descent optimization to minimize prediction errors.\nWhy did we choose to explore GBM?\n\nIndustry Relevance: GBM and its variants (XGBoost, LightGBM) are among the most commonly used algorithms in industry for problems like customer churn prediction, which is conceptually similar to employee attrition prediction. Companies like Uber, Airbnb, and Netflix use these methods for similar classification problems.\nCuriosity and Learning: We wanted to go beyond the course material and explore a technique that we had heard about but never implemented ourselves. This project provided an opportunity to learn something new.\nValidation of Findings: By comparing GBM results with our other models, we could check whether the same variables (particularly OverTime) emerged as important predictors, providing additional validation for our conclusions.\n\nImportant Note: Since GBM was not covered in class, we implemented a basic version to see how it compares. We are not claiming expertise in this method—this is purely an exploratory addition to demonstrate initiative in learning new techniques.\n\n\n\n\n\nFigure 14: GBM Variable Importance - Exploratory Analysis\n\n\n\n\n\n\n=== GBM Exploratory Results ===\n\n\nTest Set AUC: 0.786 \n\n\n\nKey Observation: OverTime_Binary remains the top predictor,\n\n\nconsistent with all other methods in our analysis.\n\n\nGBM Results Summary: The GBM model achieved competitive performance and, importantly, identified OverTime_Binary as the most influential variable—consistent with our Decision Tree, Random Forest, Logistic Regression, and LASSO findings. This cross-method consistency strengthens our confidence in the key finding that overtime is the dominant predictor of employee attrition.\n\n------------------------------------------------------------------------\n\n# Research Question 3\n\n## Department-Stratified Satisfaction Analysis {.unnumbered}\n\n*Does the predictive power of employee satisfaction variables differ across departments?*\n\n### Methodology Justification\n\nOne-size-fits-all retention strategies often fail because different departments have unique cultures and satisfaction drivers. Stratified analysis identifies which satisfaction variables matter most within each departmental context.\n\n## VIF Analysis for Multicollinearity\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n&lt;table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\"&gt;\n&lt;caption&gt;Table 10: Variance Inflation Factors&lt;/caption&gt;\n &lt;thead&gt;\n  &lt;tr&gt;\n   &lt;th style=\"text-align:left;\"&gt; Variable &lt;/th&gt;\n   &lt;th style=\"text-align:right;\"&gt; VIF &lt;/th&gt;\n   &lt;th style=\"text-align:left;\"&gt; Status &lt;/th&gt;\n  &lt;/tr&gt;\n &lt;/thead&gt;\n&lt;tbody&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; JobSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0011 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; EnvironmentSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0012 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; RelationshipSatisfaction &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0046 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; WorkLifeBalance &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0045 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; JobInvolvement &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.0039 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; MonthlyIncome &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.3345 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n   &lt;td style=\"text-align:left;font-weight: bold;\"&gt; Age &lt;/td&gt;\n   &lt;td style=\"text-align:right;\"&gt; 1.3376 &lt;/td&gt;\n   &lt;td style=\"text-align:left;\"&gt; Excellent &lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n::: :::\nAll VIF values are near 1.0, indicating no problematic multicollinearity among satisfaction predictors."
  }
]