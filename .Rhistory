"Relationship satisfaction rating (1-4)",
"Department (HR, R&D, Sales)"),
Question = c("All", "Q1", "Q1", "Q1", "Q1", "Q2", "Q2", "Q2", "Q2", "Q2", "Q3", "Q3", "Q3", "Q3")
)
kable(variable_desc, caption = "Table 2: Key Variables by Research Question",
col.names = c("Variable", "Description", "Research Q")) %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
column_spec(1, bold = TRUE, width = "20%") %>%
column_spec(2, width = "60%") %>%
column_spec(3, width = "20%")
#| label: class-imbalance
#| fig-cap: "Figure 1: Attrition Distribution - Class Imbalance Analysis"
attrition_dist <- hr_data %>%
count(Attrition) %>%
mutate(Percentage = n / sum(n) * 100)
ggplot(attrition_dist, aes(x = Attrition, y = n, fill = Attrition)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = paste0(n, "\n(", round(Percentage, 1), "%)")),
vjust = -0.3, size = 5, fontface = "bold") +
labs(x = "Attrition Status",
y = "Number of Employees",
title = "Employee Attrition Distribution",
subtitle = paste0("Class imbalance ratio: ", round(100 - attrition_rate, 1), "% No vs ", round(attrition_rate, 1), "% Yes")) +
theme_minimal(base_size = 12) +
theme(plot.title = element_text(face = "bold", size = 14),
legend.position = "none") +
scale_fill_manual(values = c("No" = "#6A994E", "Yes" = "#BC4749")) +
ylim(0, max(attrition_dist$n) * 1.20)
n_total <- nrow(hr_data)
n_pos <- sum(hr_data$Attrition_Binary)
n_neg <- n_total - n_pos
weight_pos <- n_total / (2 * n_pos)
weight_neg <- n_total / (2 * n_neg)
cat("\n=== Class Imbalance Statistics ===\n")
cat("Majority Class (No Attrition):", n_neg, "employees (", round(n_neg/n_total*100, 1), "%)\n")
cat("Minority Class (Attrition):", n_pos, "employees (", round(n_pos/n_total*100, 1), "%)\n")
cat("Imbalance Ratio:", round(n_neg/n_pos, 2), ":1\n\n")
cat("=== Class Weights for Balanced Learning ===\n")
cat("Weight for Class 0 (No Attrition):", round(weight_neg, 4), "\n")
cat("Weight for Class 1 (Attrition):", round(weight_pos, 4), "\n")
#| label: categorical-eda
#| fig-cap: "Figure 2: Attrition Rates by Key Categorical Variables"
overtime_summary <- hr_data %>%
group_by(OverTime) %>%
summarise(Total = n(), Attrition = sum(Attrition_Binary), Rate = mean(Attrition_Binary) * 100, .groups = "drop")
p1 <- ggplot(overtime_summary, aes(x = OverTime, y = Rate, fill = OverTime)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = paste0(round(Rate, 1), "%")), vjust = -0.3, size = 4, fontface = "bold") +
labs(x = "OverTime Status", y = "Attrition Rate (%)", title = "By OverTime Status") +
theme_minimal() +
theme(legend.position = "none", plot.title = element_text(face = "bold")) +
scale_fill_manual(values = c("No" = "#6A994E", "Yes" = "#BC4749")) +
ylim(0, max(overtime_summary$Rate) * 1.25)
dept_summary <- hr_data %>%
group_by(Department) %>%
summarise(Total = n(), Attrition = sum(Attrition_Binary), Rate = mean(Attrition_Binary) * 100, .groups = "drop")
p2 <- ggplot(dept_summary, aes(x = reorder(Department, -Rate), y = Rate, fill = Department)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = paste0(round(Rate, 1), "%")), vjust = -0.3, size = 4, fontface = "bold") +
labs(x = "Department", y = "Attrition Rate (%)", title = "By Department") +
theme_minimal() +
theme(legend.position = "none", plot.title = element_text(face = "bold")) +
scale_fill_brewer(palette = "Set2") +
ylim(0, max(dept_summary$Rate) * 1.25)
marital_summary <- hr_data %>%
group_by(MaritalStatus) %>%
summarise(Total = n(), Attrition = sum(Attrition_Binary), Rate = mean(Attrition_Binary) * 100, .groups = "drop")
p3 <- ggplot(marital_summary, aes(x = reorder(MaritalStatus, -Rate), y = Rate, fill = MaritalStatus)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = paste0(round(Rate, 1), "%")), vjust = -0.3, size = 4, fontface = "bold") +
labs(x = "Marital Status", y = "Attrition Rate (%)", title = "By Marital Status") +
theme_minimal() +
theme(legend.position = "none", plot.title = element_text(face = "bold")) +
scale_fill_brewer(palette = "Pastel1") +
ylim(0, max(marital_summary$Rate) * 1.25)
travel_summary <- hr_data %>%
group_by(BusinessTravel) %>%
summarise(Total = n(), Attrition = sum(Attrition_Binary), Rate = mean(Attrition_Binary) * 100, .groups = "drop")
p4 <- ggplot(travel_summary, aes(x = reorder(BusinessTravel, -Rate), y = Rate, fill = BusinessTravel)) +
geom_bar(stat = "identity", width = 0.6) +
geom_text(aes(label = paste0(round(Rate, 1), "%")), vjust = -0.3, size = 4, fontface = "bold") +
labs(x = "Business Travel", y = "Attrition Rate (%)", title = "By Business Travel") +
theme_minimal() +
theme(legend.position = "none", axis.text.x = element_text(angle = 15, hjust = 1), plot.title = element_text(face = "bold")) +
scale_fill_brewer(palette = "Blues") +
ylim(0, max(travel_summary$Rate) * 1.25)
grid.arrange(p1, p2, p3, p4, ncol = 2)
#| label: chi-square-tests
chi_overtime <- chisq.test(table(hr_data$OverTime, hr_data$Attrition))
chi_dept <- chisq.test(table(hr_data$Department, hr_data$Attrition))
chi_marital <- chisq.test(table(hr_data$MaritalStatus, hr_data$Attrition))
chi_travel <- chisq.test(table(hr_data$BusinessTravel, hr_data$Attrition))
cramers_v <- function(chi) {
n <- sum(chi$observed)
min_dim <- min(nrow(chi$observed) - 1, ncol(chi$observed) - 1)
sqrt(chi$statistic / (n * min_dim))
}
chi_results <- data.frame(
Variable = c("OverTime", "Department", "MaritalStatus", "BusinessTravel"),
ChiSquare = c(chi_overtime$statistic, chi_dept$statistic, chi_marital$statistic, chi_travel$statistic),
df = c(chi_overtime$parameter, chi_dept$parameter, chi_marital$parameter, chi_travel$parameter),
p_value = c(chi_overtime$p.value, chi_dept$p.value, chi_marital$p.value, chi_travel$p.value),
CramersV = c(cramers_v(chi_overtime), cramers_v(chi_dept), cramers_v(chi_marital), cramers_v(chi_travel))
)
chi_results$Significance <- ifelse(chi_results$p_value < 0.001, "***",
ifelse(chi_results$p_value < 0.01, "**",
ifelse(chi_results$p_value < 0.05, "*", "")))
kable(chi_results, digits = c(0, 2, 0, 4, 3, 0),
caption = "Table 3: Chi-Square Tests of Association with Attrition",
col.names = c("Variable", "Chi-Square", "df", "p-value", "Cramer's V", "Sig.")) %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
column_spec(1, bold = TRUE)
#| label: numeric-eda
#| fig-cap: "Figure 3: Distribution of Key Numeric Variables by Attrition Status"
numeric_vars <- c("Age", "MonthlyIncome", "DistanceFromHome", "YearsAtCompany",
"YearsSinceLastPromotion", "TotalWorkingYears")
plots <- lapply(numeric_vars, function(var) {
ggplot(hr_data, aes(x = factor(Attrition), y = .data[[var]], fill = factor(Attrition))) +
geom_boxplot(alpha = 0.8, outlier.shape = 21) +
labs(x = "Attrition", y = var, title = var) +
theme_minimal() +
theme(legend.position = "none", plot.title = element_text(size = 11, face = "bold")) +
scale_fill_manual(values = c("No" = "#6A994E", "Yes" = "#BC4749"))
})
grid.arrange(grobs = plots, ncol = 3)
#| label: t-tests
t_test_vars <- c("Age", "MonthlyIncome", "DistanceFromHome", "YearsAtCompany",
"YearsInCurrentRole", "YearsSinceLastPromotion", "TotalWorkingYears",
"JobSatisfaction", "EnvironmentSatisfaction", "WorkLifeBalance")
t_results <- data.frame(Variable = character(), Mean_No = numeric(), Mean_Yes = numeric(),
Difference = numeric(), p_value = numeric(), stringsAsFactors = FALSE)
for (var in t_test_vars) {
t_test <- t.test(hr_data[[var]] ~ hr_data$Attrition)
t_results <- rbind(t_results, data.frame(
Variable = var, Mean_No = round(t_test$estimate[1], 2), Mean_Yes = round(t_test$estimate[2], 2),
Difference = round(t_test$estimate[2] - t_test$estimate[1], 2), p_value = t_test$p.value))
}
t_results$Significance <- ifelse(t_results$p_value < 0.001, "***",
ifelse(t_results$p_value < 0.01, "**",
ifelse(t_results$p_value < 0.05, "*", "")))
kable(t_results, digits = c(0, 2, 2, 2, 4, 0),
caption = "Table 4: T-Tests Comparing Numeric Variables by Attrition Status",
col.names = c("Variable", "Mean (No)", "Mean (Yes)", "Difference", "p-value", "Sig.")) %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
column_spec(1, bold = TRUE)
#| label: q1-decision-tree
#| fig-cap: "Figure 4: Decision Tree for Work-Life Attrition Risk Profiling"
q1_data <- hr_data %>%
select(Attrition_Binary, OverTime_Binary, DistanceFromHome, WorkLifeBalance, YearsAtCompany)
set.seed(515)
train_idx <- createDataPartition(q1_data$Attrition_Binary, p = 0.7, list = FALSE)
train_data <- q1_data[train_idx, ]
test_data <- q1_data[-train_idx, ]
dt_model <- rpart(
Attrition_Binary ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance + YearsAtCompany,
data = train_data,
method = "class",
parms = list(prior = c(0.5, 0.5)),
control = rpart.control(maxdepth = 4, minsplit = 50, cp = 0.01)
)
rpart.plot(dt_model, type = 4, extra = 104, fallen.leaves = TRUE,
main = "Decision Tree: Work-Life Factors Predicting Attrition", cex = 0.8)
#| label: dt-importance
#| fig-cap: "Figure 5: Decision Tree Feature Importance"
dt_importance <- data.frame(
Variable = names(dt_model$variable.importance),
Importance = dt_model$variable.importance / sum(dt_model$variable.importance)
)
ggplot(dt_importance, aes(x = reorder(Variable, Importance), y = Importance, fill = Importance)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "Variable", y = "Relative Importance",
title = "Decision Tree Feature Importance") +
theme_minimal() +
theme(plot.title = element_text(face = "bold", size = 14), legend.position = "none") +
scale_fill_gradient(low = "#A8DADC", high = "#1D3557")
#| label: dt-performance
pred_dt <- predict(dt_model, test_data, type = "class")
pred_prob_dt <- predict(dt_model, test_data, type = "prob")[, 2]
cm_dt <- confusionMatrix(factor(pred_dt), factor(test_data$Attrition_Binary))
cat("\n=== Decision Tree Test Set Performance ===\n")
print(cm_dt)
roc_dt <- roc(test_data$Attrition_Binary, pred_prob_dt, quiet = TRUE)
cat("\nDecision Tree AUC-ROC:", round(auc(roc_dt), 4), "\n")
#| label: q1-logistic
hr_q1 <- hr_data %>%
mutate(
OT_x_WLB = OverTime_Binary * WorkLifeBalance,
OT_x_Distance = OverTime_Binary * DistanceFromHome,
OT_x_YearsAtCompany = OverTime_Binary * YearsAtCompany
)
logit_q1 <- glm(
Attrition_Binary ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance +
YearsAtCompany + OT_x_WLB + OT_x_Distance + OT_x_YearsAtCompany,
data = hr_q1,
family = binomial(link = "logit")
)
summary(logit_q1)
#| label: q1-odds-ratios
or_ci <- suppressMessages(confint(logit_q1))
or_table <- data.frame(
Variable = names(coef(logit_q1)),
Coefficient = coef(logit_q1),
OR = exp(coef(logit_q1)),
CI_Lower = exp(or_ci[, 1]),
CI_Upper = exp(or_ci[, 2]),
p_value = summary(logit_q1)$coefficients[, 4]
)
or_table$Significance <- ifelse(or_table$p_value < 0.001, "***",
ifelse(or_table$p_value < 0.01, "**",
ifelse(or_table$p_value < 0.05, "*", "")))
kable(or_table[-1, ], digits = 4,
caption = "Table 5: Logistic Regression Odds Ratios with Interaction Terms (Q1)",
col.names = c("Variable", "Beta", "OR", "95% CI Lower", "95% CI Upper", "p-value", "Sig."),
row.names = FALSE) %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
column_spec(1, bold = TRUE)
#| label: q1-logistic-diagnostics
#| fig-cap: "Figure 6: Logistic Regression Diagnostic Plots"
par(mfrow = c(2, 2))
hr_q1_logit_check <- hr_q1 %>%
mutate(YearsAtCompany_bin = cut(YearsAtCompany, breaks = 5)) %>%
group_by(YearsAtCompany_bin) %>%
summarise(mean_years = mean(YearsAtCompany), prop_attrition = mean(Attrition_Binary),
empirical_logit = log((prop_attrition + 0.01) / (1 - prop_attrition + 0.01)), .groups = "drop")
plot(hr_q1_logit_check$mean_years, hr_q1_logit_check$empirical_logit,
xlab = "YearsAtCompany (Binned Mean)", ylab = "Empirical Logit",
main = "1. Linearity Check: Years at Company", pch = 19, col = "#2E86AB", cex = 1.5)
abline(lm(empirical_logit ~ mean_years, data = hr_q1_logit_check), col = "#E94F37", lwd = 2)
plot(fitted(logit_q1), residuals(logit_q1, type = "deviance"),
xlab = "Fitted Values", ylab = "Deviance Residuals",
main = "2. Deviance Residuals vs Fitted", pch = 19, col = rgb(0, 0, 0, 0.3))
abline(h = 0, col = "#E94F37", lwd = 2, lty = 2)
cooks_d <- cooks.distance(logit_q1)
plot(cooks_d, type = "h", main = "3. Cook's Distance", ylab = "Cook's Distance",
xlab = "Observation Index", col = "#457B9D")
abline(h = 4/nrow(hr_q1), col = "#E94F37", lty = 2, lwd = 2)
plot(logit_q1, which = 5)
par(mfrow = c(1, 1))
#| label: q1-hl-test
hl_test <- hoslem.test(logit_q1$y, fitted(logit_q1), g = 10)
cat("\n=== Hosmer-Lemeshow Goodness-of-Fit Test ===\n")
cat("Chi-square statistic:", round(hl_test$statistic, 4), "\n")
cat("Degrees of freedom:", hl_test$parameter, "\n")
cat("p-value:", round(hl_test$p.value, 4), "\n")
cat("Interpretation:", ifelse(hl_test$p.value > 0.05,
"PASS - Model fits adequately (p > 0.05)", "CAUTION - Some lack of fit (p < 0.05)"), "\n")
cat("\n=== Variance Inflation Factors (VIF) ===\n")
vif_values <- vif(logit_q1)
print(round(vif_values, 2))
cat("\nAll VIF values < 5 indicate acceptable multicollinearity levels.\n")
#| label: q1-rf-tuning
#| fig-cap: "Figure 7: Random Forest Hyperparameter Tuning via 5-Fold Cross-Validation"
train_data_rf <- train_data %>%
mutate(Attrition_Factor = factor(ifelse(Attrition_Binary == 1, "Yes", "No"), levels = c("No", "Yes")))
train_control <- trainControl(
method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = "final"
)
rf_grid <- expand.grid(mtry = c(2, 3, 4))
set.seed(515)
rf_tuned <- train(
Attrition_Factor ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance + YearsAtCompany,
data = train_data_rf, method = "rf", trControl = train_control,
tuneGrid = rf_grid, ntree = 500, importance = TRUE, metric = "ROC"
)
cat("\n=== Random Forest 5-Fold Cross-Validation Results ===\n")
print(rf_tuned)
plot(rf_tuned, main = "Random Forest: Cross-Validated AUC by mtry Parameter")
#| label: q1-rf-final
#| fig-cap: "Figure 8: Random Forest Variable Importance (Tuned Model)"
rf_model <- rf_tuned$finalModel
importance_df <- data.frame(
Variable = rownames(importance(rf_model)),
MeanDecreaseGini = importance(rf_model)[, "MeanDecreaseGini"]
)
ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), y = MeanDecreaseGini, fill = MeanDecreaseGini)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "Variable", y = "Mean Decrease in Gini Impurity",
title = "Random Forest Variable Importance (Tuned)",
subtitle = paste("Optimal mtry =", rf_tuned$bestTune$mtry, "| CV AUC =", round(max(rf_tuned$results$ROC), 3))) +
theme_minimal() +
theme(plot.title = element_text(face = "bold", size = 14), legend.position = "none") +
scale_fill_gradient(low = "#A8DADC", high = "#1D3557")
#| label: q1-rf-evaluation
test_data_rf <- test_data %>%
mutate(Attrition_Factor = factor(ifelse(Attrition_Binary == 1, "Yes", "No"), levels = c("No", "Yes")))
pred_rf_prob <- predict(rf_model, test_data, type = "prob")[, "Yes"]
roc_rf <- roc(test_data$Attrition_Binary, pred_rf_prob, quiet = TRUE)
cat("\n=== Random Forest Performance (Test Set) ===\n")
cat("Test Set AUC-ROC:", round(auc(roc_rf), 4), "\n")
cat("Optimal mtry (from CV):", rf_tuned$bestTune$mtry, "\n")
cat("Cross-Validated AUC:", round(max(rf_tuned$results$ROC), 4), "\n")
#| label: q1-roc-comparison
#| fig-cap: "Figure 9: ROC Curve Comparison - Research Question 1 Models"
pred_logit_q1 <- predict(logit_q1, type = "response")
roc_logit_q1 <- roc(hr_q1$Attrition_Binary, pred_logit_q1, quiet = TRUE)
plot(roc_dt, col = "#2E86AB", lwd = 2, main = "ROC Curves Comparison - Q1: Work-Life Balance Models")
plot(roc_rf, col = "#E94F37", lwd = 2, add = TRUE)
plot(roc_logit_q1, col = "#6A994E", lwd = 2, add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "gray50")
legend("bottomright",
legend = c(paste("Decision Tree (AUC =", round(auc(roc_dt), 3), ")"),
paste("Random Forest (AUC =", round(auc(roc_rf), 3), ")"),
paste("Logistic Regression (AUC =", round(auc(roc_logit_q1), 3), ")")),
col = c("#2E86AB", "#E94F37", "#6A994E"), lwd = 2, bty = "n")
#| label: q2-setup
set.seed(515)
train_idx2 <- createDataPartition(hr_data$Attrition_Binary, p = 0.7, list = FALSE)
train_q2 <- hr_data[train_idx2, ]
test_q2 <- hr_data[-train_idx2, ]
cat("=== Q2 Train/Test Split ===\n")
cat("Training set:", nrow(train_q2), "observations\n")
cat("Test set:", nrow(test_q2), "observations\n")
#| label: q2-weighted-logit
n_total2 <- nrow(train_q2)
n_pos2 <- sum(train_q2$Attrition_Binary)
n_neg2 <- n_total2 - n_pos2
w_pos2 <- n_total2 / (2 * n_pos2)
w_neg2 <- n_total2 / (2 * n_neg2)
weights_q2 <- ifelse(train_q2$Attrition_Binary == 1, w_pos2, w_neg2)
logit_q2 <- suppressWarnings(glm(
Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole +
MonthlyIncome + PercentSalaryHike + JobLevel,
data = train_q2, family = binomial(), weights = weights_q2
))
summary(logit_q2)
#| label: q2-odds-ratios
or_ci_q2 <- suppressMessages(confint(logit_q2))
or_table_q2 <- data.frame(
Variable = names(coef(logit_q2)),
Coefficient = coef(logit_q2),
OR = exp(coef(logit_q2)),
CI_Lower = exp(or_ci_q2[, 1]),
CI_Upper = exp(or_ci_q2[, 2]),
p_value = summary(logit_q2)$coefficients[, 4]
)
or_table_q2$Significance <- ifelse(or_table_q2$p_value < 0.001, "***",
ifelse(or_table_q2$p_value < 0.01, "**",
ifelse(or_table_q2$p_value < 0.05, "*", "")))
kable(or_table_q2[-1, ], digits = 4,
caption = "Table 6: Weighted Logistic Regression Odds Ratios (Q2)",
col.names = c("Variable", "Beta", "OR", "95% CI Lower", "95% CI Upper", "p-value", "Sig."),
row.names = FALSE) %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
column_spec(1, bold = TRUE)
#| label: q2-threshold-effects
yslp_coef <- coef(logit_q2)["YearsSinceLastPromotion"]
yslp_or <- exp(yslp_coef)
years_no_promo <- 1:7
cumulative_or <- yslp_or^years_no_promo
threshold_effects <- data.frame(
Years = years_no_promo,
Cumulative_OR = round(cumulative_or, 2),
Percent_Increase = paste0(round((cumulative_or - 1) * 100, 0), "%"),
Risk_Category = c("Low", "Low", "Moderate", "Elevated", "High", "Very High", "Critical")
)
kable(threshold_effects,
caption = "Table 7: Cumulative Odds Multipliers by Years Without Promotion",
col.names = c("Years Without Promotion", "Cumulative Odds Multiplier", "% Increase vs Baseline", "Risk Category")) %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
row_spec(4, bold = TRUE, background = "#fff3cd") %>%
row_spec(5:7, background = "#f8d7da")
#| label: q2-lasso-training
#| fig-cap: "Figure 10: LASSO Cross-Validation on Training Set"
lasso_vars <- c(
"YearsSinceLastPromotion", "YearsInCurrentRole", "MonthlyIncome",
"PercentSalaryHike", "JobLevel", "Age", "TotalWorkingYears",
"YearsAtCompany", "YearsWithCurrManager", "DistanceFromHome",
"JobSatisfaction", "EnvironmentSatisfaction", "WorkLifeBalance",
"OverTime_Binary", "NumCompaniesWorked", "StockOptionLevel"
)
X_train <- as.matrix(train_q2[, lasso_vars])
y_train <- train_q2$Attrition_Binary
set.seed(515)
cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, nfolds = 10)
plot(cv_lasso, main = "LASSO Cross-Validation (Training Set Only)")
cat("\n=== LASSO Regularization Results ===\n")
cat("Optimal lambda (min):", round(cv_lasso$lambda.min, 6), "\n")
cat("Variables retained:", sum(coef(cv_lasso, s = "lambda.min") != 0) - 1, "of", length(lasso_vars), "\n")
#| label: lasso-coefficients
#| fig-cap: "Figure 11: LASSO Selected Variable Coefficients"
lasso_coefs <- coef(cv_lasso, s = "lambda.min")
lasso_df <- data.frame(
Variable = rownames(lasso_coefs)[-1],
Coefficient = as.vector(lasso_coefs)[-1]
) %>%
filter(abs(Coefficient) > 0.001) %>%
arrange(desc(abs(Coefficient)))
ggplot(lasso_df, aes(x = reorder(Variable, Coefficient), y = Coefficient,
fill = ifelse(Coefficient > 0, "Increases Risk", "Decreases Risk"))) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "Variable", y = "LASSO Coefficient",
title = "LASSO Variable Selection Results",
subtitle = paste(nrow(lasso_df), "variables retained"), fill = "Effect Direction") +
theme_minimal() +
theme(plot.title = element_text(face = "bold", size = 14), legend.position = "bottom") +
scale_fill_manual(values = c("Increases Risk" = "#BC4749", "Decreases Risk" = "#6A994E"))
#| label: lasso-table
kable(lasso_df, digits = 4,
caption = "Table 8: LASSO Selected Variables",
col.names = c("Variable", "Coefficient")) %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
column_spec(1, bold = TRUE)
cat("\n=== Variables Eliminated by LASSO ===\n")
eliminated <- setdiff(lasso_vars, lasso_df$Variable)
if(length(eliminated) > 0) cat("Variables shrunk to zero:", paste(eliminated, collapse = ", "), "\n")
#| label: lasso-test-evaluation
X_test <- as.matrix(test_q2[, lasso_vars])
y_test <- test_q2$Attrition_Binary
pred_lasso <- predict(cv_lasso, newx = X_test, s = "lambda.min", type = "response")
roc_lasso <- roc(y_test, as.vector(pred_lasso), quiet = TRUE)
cat("\n=== LASSO Performance on Test Set ===\n")
cat("Test Set AUC-ROC:", round(auc(roc_lasso), 4), "\n")
#| label: q2-threshold-analysis
#| fig-cap: "Figure 12: ROC Curve and Threshold Optimization"
pred_probs_q2 <- predict(logit_q2, newdata = test_q2, type = "response")
roc_q2 <- roc(test_q2$Attrition_Binary, pred_probs_q2, quiet = TRUE)
coords_best <- coords(roc_q2, "best", ret = c("threshold", "sensitivity", "specificity"))
par(mfrow = c(1, 2))
plot(roc_q2, lwd = 3, col = "#2E86AB", main = "ROC Curve - Career Stagnation Model")
points(coords_best$specificity, coords_best$sensitivity, pch = 19, cex = 2, col = "#E94F37")
text(coords_best$specificity - 0.12, coords_best$sensitivity + 0.05,
paste("Optimal:", round(coords_best$threshold, 3)), cex = 0.9, font = 2)
legend("bottomright", paste("AUC =", round(auc(roc_q2), 3)), bty = "n", cex = 1.1)
thresholds <- seq(0.1, 0.9, by = 0.05)
metrics <- sapply(thresholds, function(t) {
pred <- ifelse(pred_probs_q2 >= t, 1, 0)
c(Sensitivity = mean(pred[test_q2$Attrition_Binary == 1] == 1),
Specificity = mean(pred[test_q2$Attrition_Binary == 0] == 0),
Accuracy = mean(pred == test_q2$Attrition_Binary))
})
plot(thresholds, metrics["Sensitivity", ], type = "l", lwd = 3, col = "#2E86AB",
ylim = c(0, 1), xlab = "Classification Threshold", ylab = "Metric Value",
main = "Classification Metrics vs Threshold")
lines(thresholds, metrics["Specificity", ], lwd = 3, col = "#E94F37")
lines(thresholds, metrics["Accuracy", ], lwd = 3, col = "#6A994E")
abline(v = coords_best$threshold, lty = 2, lwd = 2)
legend("right", c("Sensitivity", "Specificity", "Accuracy"),
col = c("#2E86AB", "#E94F37", "#6A994E"), lwd = 3, bty = "n")
par(mfrow = c(1, 1))
#| label: threshold-table
threshold_results <- data.frame(Threshold = c(0.1, 0.2, 0.3, 0.4, 0.5, round(coords_best$threshold, 3)),
Sensitivity = numeric(6), Specificity = numeric(6), Accuracy = numeric(6))
for (i in 1:6) {
t <- threshold_results$Threshold[i]
pred <- ifelse(pred_probs_q2 >= t, 1, 0)
threshold_results$Sensitivity[i] <- round(mean(pred[test_q2$Attrition_Binary == 1] == 1), 3)
threshold_results$Specificity[i] <- round(mean(pred[test_q2$Attrition_Binary == 0] == 0), 3)
threshold_results$Accuracy[i] <- round(mean(pred == test_q2$Attrition_Binary), 3)
}
threshold_results$Note <- c("", "", "", "", "", "Optimal (Youden's J)")
kable(threshold_results, caption = "Table 9: Classification Performance at Different Thresholds") %>%
kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
row_spec(6, bold = TRUE, background = "#d4edda")
#| label: q2-calibration
#| fig-cap: "Figure 13: Calibration Curve - Model Probability Assessment"
cal_data <- data.frame(predicted = pred_probs_q2, actual = test_q2$Attrition_Binary) %>%
mutate(predicted_bin = cut(predicted, breaks = seq(0, 1, 0.1), include.lowest = TRUE)) %>%
group_by(predicted_bin) %>%
summarise(mean_predicted = mean(predicted), mean_actual = mean(actual), n = n(), .groups = "drop") %>%
filter(!is.na(predicted_bin) & n > 5)
plot(cal_data$mean_predicted, cal_data$mean_actual,
xlim = c(0, 1), ylim = c(0, 1),
xlab = "Mean Predicted Probability", ylab = "Observed Proportion",
main = "Calibration Curve - Weighted Logistic Regression",
pch = 19, cex = 2, col = "#2E86AB")
abline(0, 1, lty = 2, col = "#E94F37", lwd = 2)
grid()
cal_cor <- cor(cal_data$mean_predicted, cal_data$mean_actual)
cat("\n=== Calibration Assessment ===\n")
cat("Calibration correlation:", round(cal_cor, 3), "\n")
#| label: gbm-training
#| include: false
# GBM uses same predictors as our other Q2 models for fair comparison
gbm_vars <- c("OverTime_Binary", "YearsAtCompany", "YearsSinceLastPromotion",
"YearsInCurrentRole", "MonthlyIncome", "PercentSalaryHike",
"JobLevel", "JobSatisfaction", "EnvironmentSatisfaction",
"WorkLifeBalance", "StockOptionLevel", "DistanceFromHome",
"Age", "TotalWorkingYears")
set.seed(515)
gbm_model <- gbm(
Attrition_Binary ~ .,
data = train_q2[, c("Attrition_Binary", gbm_vars)],
distribution = "bernoulli",  # For binary classification
n.trees = 500,               # Number of trees to build sequentially
interaction.depth = 3,       # How complex each tree can be
shrinkage = 0.01,            # Learning rate - how much each tree contributes
bag.fraction = 0.7,          # Random sampling for each tree
cv.folds = 5,                # Cross-validation to find optimal trees
verbose = FALSE
)
# Find optimal number of trees using cross-validation
best_iter <- gbm.perf(gbm_model, method = "cv", plot.it = FALSE)
importance_gbm <- summary(gbm_model, n.trees = best_iter, plotit = FALSE)
#| label: gbm-importance
#| fig-cap: "Figure 14: GBM Variable Importance - Exploratory Analysis"
ggplot(importance_gbm[1:10, ], aes(x = reorder(var, rel.inf), y = rel.inf)) +
geom_bar(stat = "identity", fill = "#6A994E", alpha = 0.8) +
coord_flip() +
labs(x = "Variable", y = "Relative Influence (%)",
title = "GBM Variable Importance (Exploratory)",
subtitle = "Included as innovation component - not covered in STAT 515") +
theme_minimal()
#| label: gbm-evaluation
pred_gbm <- predict(gbm_model, newdata = test_q2, n.trees = best_iter, type = "response")
roc_gbm <- roc(test_q2$Attrition_Binary, pred_gbm, quiet = TRUE)
cat("=== GBM Exploratory Results ===\n")
cat("Test Set AUC:", round(auc(roc_gbm), 4), "\n")
cat("\nKey Observation: OverTime_Binary remains the top predictor,\n")
cat("consistent with all other methods in our analysis.\n")
