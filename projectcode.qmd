---
title: "Project Code"
subtitle: "Complete R Code for IBM HR Employee Attrition Analysis"
author: "Rutvij Reddy Vakati & Sean Grieg"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-depth: 2
    code-fold: false
    code-summary: "Show Code"
execute:
  echo: true
  warning: false
  message: false
  eval: false
---

# Setup and Libraries

```{r}
#| label: setup

# =============================================================================
# STAT 515 Final Project - IBM HR Employee Attrition Analysis
# Team: Rutvij Reddy Vakati & Sean Grieg
# George Mason University | Fall 2025
# =============================================================================

# Core libraries
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(knitr)
library(kableExtra)
library(scales)
library(corrplot)
library(car)

# Machine Learning libraries
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(glmnet)
library(pROC)
library(gridExtra)
library(rlang)
library(ResourceSelection)  # For Hosmer-Lemeshow test
library(gbm)                # For Gradient Boosting Machine

# Set seed for reproducibility
set.seed(515)
```

# Data Loading and Preprocessing

```{r}
#| label: data-loading

# Load the dataset - try multiple possible paths
possible_paths <- c(
  "data/HR_Data.xlsx",
  "HR_Data.xlsx",
  "./data/HR_Data.xlsx",
  "./HR_Data.xlsx"
)

data_path <- NULL
for (p in possible_paths) {
  if (file.exists(p)) {
    data_path <- p
    break
  }
}

if (is.null(data_path)) {
  cat("Current working directory:", getwd(), "\n")
  cat("Files in current directory:", paste(list.files(), collapse = ", "), "\n")
  if (dir.exists("data")) {
    cat("Files in data folder:", paste(list.files("data"), collapse = ", "), "\n")
  }
  stop("HR_Data.xlsx not found. Please ensure it's in the project folder or 'data' subfolder.")
}

hr_data <- read_excel(data_path)
cat("Data loaded successfully from:", data_path, "\n")

# Examine structure
glimpse(hr_data)
dim(hr_data)
```

```{r}
#| label: column-standardization

# Column name resolution function for flexibility with different data formats
resolve_col <- function(df, candidates) {
  hit <- candidates[candidates %in% names(df)][1]
  if (is.na(hit)) stop("Missing expected column. Tried: ", paste(candidates, collapse = ", "))
  hit
}

# Column mapping to handle variations in column names
col_map <- list(
  Attrition                 = c("Attrition"),
  OverTime                  = c("Over Time", "OverTime"),
  DistanceFromHome          = c("Distance From Home", "DistanceFromHome"),
  WorkLifeBalance           = c("Work Life Balance", "WorkLifeBalance"),
  YearsAtCompany            = c("Years At Company", "YearsAtCompany"),
  YearsSinceLastPromotion   = c("Years Since Last Promotion", "YearsSinceLastPromotion"),
  YearsInCurrentRole        = c("Years In Current Role", "YearsInCurrentRole"),
  MonthlyIncome             = c("Monthly Income", "MonthlyIncome"),
  PercentSalaryHike         = c("Percent Salary Hike", "PercentSalaryHike"),
  JobLevel                  = c("Job Level", "JobLevel"),
  JobSatisfaction           = c("Job Satisfaction", "JobSatisfaction"),
  EnvironmentSatisfaction   = c("Environment Satisfaction", "EnvironmentSatisfaction"),
  RelationshipSatisfaction  = c("Relationship Satisfaction", "RelationshipSatisfaction"),
  JobInvolvement            = c("Job Involvement", "JobInvolvement"),
  YearsWithCurrManager      = c("Years With Curr Manager", "Years With Current Manager",
                                "YearsWithCurrManager", "YearsWithCurrentManager"),
  TotalWorkingYears         = c("Total Working Years", "TotalWorkingYears"),
  NumCompaniesWorked        = c("Num Companies Worked", "NumCompaniesWorked"),
  StockOptionLevel          = c("Stock Option Level", "StockOptionLevel"),
  Age                       = c("Age"),
  Department                = c("Department"),
  BusinessTravel            = c("Business Travel", "BusinessTravel"),
  MaritalStatus             = c("Marital Status", "MaritalStatus"),
  Gender                    = c("Gender"),
  JobRole                   = c("Job Role", "JobRole")
)

# Build rename list and apply
rename_list <- list()
for (new_nm in names(col_map)) {
  old_nm <- resolve_col(hr_data, col_map[[new_nm]])
  if (old_nm != new_nm) rename_list[[new_nm]] <- old_nm
}

if (length(rename_list) > 0) {
  hr_data <- hr_data %>% rename(!!!rename_list)
}

# Create binary variables
hr_data <- hr_data %>%
  mutate(
    Attrition_Binary = ifelse(Attrition == "Yes", 1, 0),
    OverTime_Binary  = ifelse(OverTime  == "Yes", 1, 0)
  )

# Ensure numeric columns are numeric
numeric_cols <- c(
  "Age","MonthlyIncome","YearsAtCompany","DistanceFromHome","WorkLifeBalance",
  "YearsSinceLastPromotion","YearsInCurrentRole","PercentSalaryHike","JobLevel",
  "JobSatisfaction","EnvironmentSatisfaction","RelationshipSatisfaction",
  "JobInvolvement","YearsWithCurrManager","TotalWorkingYears","NumCompaniesWorked",
  "StockOptionLevel","Attrition_Binary","OverTime_Binary"
)

numeric_cols <- numeric_cols[numeric_cols %in% names(hr_data)]
hr_data <- hr_data %>% mutate(across(all_of(numeric_cols), ~ suppressWarnings(as.numeric(.))))

# Check class distribution
table(hr_data$Attrition)
prop.table(table(hr_data$Attrition)) * 100

# Calculate class weights for imbalanced data
n_total <- nrow(hr_data)
n_pos <- sum(hr_data$Attrition_Binary)
n_neg <- n_total - n_pos
weight_pos <- n_total / (2 * n_pos)
weight_neg <- n_total / (2 * n_neg)

cat("Class Weights:\n")
cat("  Class 0 (No):", weight_neg, "\n")
cat("  Class 1 (Yes):", weight_pos, "\n")
```

# Exploratory Data Analysis

```{r}
#| label: eda-categorical

# Attrition distribution
attrition_dist <- hr_data %>%
  count(Attrition) %>%
  mutate(Percentage = n / sum(n) * 100)

print(attrition_dist)

# Attrition by OverTime
hr_data %>%
  group_by(OverTime) %>%
  summarise(
    Total = n(),
    Attrition = sum(Attrition_Binary),
    Attrition_Rate = mean(Attrition_Binary) * 100,
    .groups = "drop"
  )

# Attrition by Department
hr_data %>%
  group_by(Department) %>%
  summarise(
    Total = n(),
    Attrition = sum(Attrition_Binary),
    Attrition_Rate = mean(Attrition_Binary) * 100,
    .groups = "drop"
  )

# Attrition by Marital Status
hr_data %>%
  group_by(MaritalStatus) %>%
  summarise(
    Total = n(),
    Attrition = sum(Attrition_Binary),
    Attrition_Rate = mean(Attrition_Binary) * 100,
    .groups = "drop"
  )

# Attrition by Business Travel
hr_data %>%
  group_by(BusinessTravel) %>%
  summarise(
    Total = n(),
    Attrition = sum(Attrition_Binary),
    Attrition_Rate = mean(Attrition_Binary) * 100,
    .groups = "drop"
  )
```

```{r}
#| label: eda-chi-square

# Chi-square tests for categorical variables
chi_overtime <- chisq.test(table(hr_data$OverTime, hr_data$Attrition))
print(chi_overtime)

chi_dept <- chisq.test(table(hr_data$Department, hr_data$Attrition))
print(chi_dept)

chi_marital <- chisq.test(table(hr_data$MaritalStatus, hr_data$Attrition))
print(chi_marital)

chi_travel <- chisq.test(table(hr_data$BusinessTravel, hr_data$Attrition))
print(chi_travel)

# Cramér's V calculation function
cramers_v <- function(chi) {
  n <- sum(chi$observed)
  min_dim <- min(nrow(chi$observed) - 1, ncol(chi$observed) - 1)
  sqrt(chi$statistic / (n * min_dim))
}

# Effect sizes
cat("\nCramér's V Effect Sizes:\n")
cat("OverTime:", cramers_v(chi_overtime), "\n")
cat("Department:", cramers_v(chi_dept), "\n")
cat("MaritalStatus:", cramers_v(chi_marital), "\n")
cat("BusinessTravel:", cramers_v(chi_travel), "\n")
```

```{r}
#| label: eda-t-tests

# T-tests for numeric variables
t_test_vars <- c("Age", "MonthlyIncome", "DistanceFromHome", "YearsAtCompany",
                 "YearsInCurrentRole", "YearsSinceLastPromotion", "TotalWorkingYears",
                 "JobSatisfaction", "EnvironmentSatisfaction", "WorkLifeBalance")

cat("\n=== T-Tests: Numeric Variables by Attrition Status ===\n\n")

for (var in t_test_vars) {
  t_test <- t.test(hr_data[[var]] ~ hr_data$Attrition)
  cat(var, ":\n")
  cat("  Mean (No):", round(t_test$estimate[1], 2), "\n")
  cat("  Mean (Yes):", round(t_test$estimate[2], 2), "\n")
  cat("  p-value:", format(t_test$p.value, digits = 4), "\n\n")
}
```

```{r}
#| label: eda-correlation

# Correlation matrix for numeric variables
cor_vars <- c("Age", "MonthlyIncome", "YearsAtCompany", "YearsSinceLastPromotion",
              "YearsInCurrentRole", "TotalWorkingYears", "DistanceFromHome",
              "JobSatisfaction", "EnvironmentSatisfaction", "WorkLifeBalance",
              "OverTime_Binary", "Attrition_Binary")

cor_matrix <- cor(hr_data[, cor_vars], use = "complete.obs")

# Print correlation matrix
print(round(cor_matrix, 3))

# Plot correlation matrix
corrplot(cor_matrix, method = "color", type = "upper",
         tl.col = "black", tl.srt = 45, tl.cex = 0.8,
         addCoef.col = "black", number.cex = 0.6,
         col = colorRampPalette(c("#BC4749", "white", "#6A994E"))(200),
         title = "Correlation Matrix: Key Variables",
         mar = c(0, 0, 2, 0))
```

# Research Question 1: Work-Life Imbalance Analysis

## Decision Tree Model

```{r}
#| label: q1-decision-tree

# Prepare data for Q1
q1_data <- hr_data %>%
  select(Attrition_Binary, OverTime_Binary, DistanceFromHome, WorkLifeBalance, YearsAtCompany)

# Train-test split with stratification
set.seed(515)
train_idx <- createDataPartition(q1_data$Attrition_Binary, p = 0.7, list = FALSE)
train_data <- q1_data[train_idx, ]
test_data <- q1_data[-train_idx, ]

cat("Training set:", nrow(train_data), "observations\n")
cat("Test set:", nrow(test_data), "observations\n")

# Fit decision tree with balanced priors
dt_model <- rpart(
  Attrition_Binary ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance + YearsAtCompany,
  data = train_data,
  method = "class",
  parms = list(prior = c(0.5, 0.5)),
  control = rpart.control(maxdepth = 4, minsplit = 50, cp = 0.01)
)

# Print summary
printcp(dt_model)

# Plot decision tree
rpart.plot(dt_model, type = 4, extra = 104, fallen.leaves = TRUE,
           main = "Decision Tree: Work-Life Factors Predicting Attrition", cex = 0.8)

# Feature importance
dt_importance <- dt_model$variable.importance / sum(dt_model$variable.importance)
print(sort(dt_importance, decreasing = TRUE))

# Predictions and evaluation
pred_dt <- predict(dt_model, test_data, type = "class")
pred_prob_dt <- predict(dt_model, test_data, type = "prob")[, 2]

# Confusion matrix
cm_dt <- confusionMatrix(factor(pred_dt), factor(test_data$Attrition_Binary))
print(cm_dt)

# ROC curve
roc_dt <- roc(test_data$Attrition_Binary, pred_prob_dt, quiet = TRUE)
cat("\nDecision Tree AUC-ROC:", round(auc(roc_dt), 4), "\n")
```

## Logistic Regression with Interaction Terms

```{r}
#| label: q1-logistic

# Create interaction terms
hr_q1 <- hr_data %>%
  mutate(
    OT_x_WLB = OverTime_Binary * WorkLifeBalance,
    OT_x_Distance = OverTime_Binary * DistanceFromHome,
    OT_x_YearsAtCompany = OverTime_Binary * YearsAtCompany
  )

# Fit logistic regression with interactions
logit_q1 <- glm(
  Attrition_Binary ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance + 
    YearsAtCompany + OT_x_WLB + OT_x_Distance + OT_x_YearsAtCompany,
  data = hr_q1,
  family = binomial(link = "logit")
)

# Model summary
summary(logit_q1)

# Odds ratios with confidence intervals
or_ci <- suppressMessages(confint(logit_q1))
cat("\n=== Odds Ratios ===\n")
print(exp(coef(logit_q1)))

cat("\n=== 95% Confidence Intervals for Odds Ratios ===\n")
print(exp(or_ci))
```

## Logistic Regression Diagnostics

```{r}
#| label: q1-diagnostics

# Hosmer-Lemeshow Goodness-of-Fit Test
hl_test <- hoslem.test(logit_q1$y, fitted(logit_q1), g = 10)

cat("\n=== Hosmer-Lemeshow Goodness-of-Fit Test ===\n")
cat("Chi-square statistic:", round(hl_test$statistic, 4), "\n")
cat("Degrees of freedom:", hl_test$parameter, "\n")
cat("p-value:", round(hl_test$p.value, 4), "\n")
cat("Interpretation:", ifelse(hl_test$p.value > 0.05, 
    "PASS - Model fits adequately (p > 0.05)", "CAUTION - Some lack of fit (p < 0.05)"), "\n")

# VIF Analysis
cat("\n=== Variance Inflation Factors (VIF) ===\n")
vif_values <- vif(logit_q1)
print(round(vif_values, 2))

# Diagnostic plots
par(mfrow = c(2, 2))

# 1. Linearity check (empirical logit)
hr_q1_logit_check <- hr_q1 %>%
  mutate(YearsAtCompany_bin = cut(YearsAtCompany, breaks = 5)) %>%
  group_by(YearsAtCompany_bin) %>%
  summarise(mean_years = mean(YearsAtCompany), prop_attrition = mean(Attrition_Binary),
    empirical_logit = log((prop_attrition + 0.01) / (1 - prop_attrition + 0.01)), .groups = "drop")

plot(hr_q1_logit_check$mean_years, hr_q1_logit_check$empirical_logit,
     xlab = "YearsAtCompany (Binned Mean)", ylab = "Empirical Logit",
     main = "1. Linearity Check: Years at Company", pch = 19, col = "#2E86AB", cex = 1.5)
abline(lm(empirical_logit ~ mean_years, data = hr_q1_logit_check), col = "#E94F37", lwd = 2)

# 2. Deviance residuals vs fitted
plot(fitted(logit_q1), residuals(logit_q1, type = "deviance"),
     xlab = "Fitted Values", ylab = "Deviance Residuals",
     main = "2. Deviance Residuals vs Fitted", pch = 19, col = rgb(0, 0, 0, 0.3))
abline(h = 0, col = "#E94F37", lwd = 2, lty = 2)

# 3. Cook's distance
cooks_d <- cooks.distance(logit_q1)
plot(cooks_d, type = "h", main = "3. Cook's Distance", ylab = "Cook's Distance",
     xlab = "Observation Index", col = "#457B9D")
abline(h = 4/nrow(hr_q1), col = "#E94F37", lty = 2, lwd = 2)

# 4. Residuals vs Leverage
plot(logit_q1, which = 5)

par(mfrow = c(1, 1))

# ROC for logistic regression
pred_logit_q1 <- predict(logit_q1, type = "response")
roc_logit_q1 <- roc(hr_q1$Attrition_Binary, pred_logit_q1, quiet = TRUE)
cat("\nLogistic Regression AUC-ROC:", round(auc(roc_logit_q1), 4), "\n")
```

## Random Forest with Cross-Validation

```{r}
#| label: q1-random-forest

# Prepare data for RF
train_data_rf <- train_data %>%
  mutate(Attrition_Factor = factor(ifelse(Attrition_Binary == 1, "Yes", "No"), levels = c("No", "Yes")))

# Cross-validation setup
train_control <- trainControl(
  method = "cv", 
  number = 5, 
  classProbs = TRUE, 
  summaryFunction = twoClassSummary, 
  savePredictions = "final"
)

# Hyperparameter grid
rf_grid <- expand.grid(mtry = c(2, 3, 4))

# Train Random Forest with CV
set.seed(515)
rf_tuned <- train(
  Attrition_Factor ~ OverTime_Binary + DistanceFromHome + WorkLifeBalance + YearsAtCompany,
  data = train_data_rf, 
  method = "rf", 
  trControl = train_control,
  tuneGrid = rf_grid, 
  ntree = 500, 
  importance = TRUE, 
  metric = "ROC"
)

# Print CV results
cat("\n=== Random Forest 5-Fold Cross-Validation Results ===\n")
print(rf_tuned)
print(rf_tuned$results)

# Plot tuning results
plot(rf_tuned, main = "Random Forest: Cross-Validated AUC by mtry Parameter")

# Final model
rf_model <- rf_tuned$finalModel
print(rf_model)

# Variable importance
cat("\n=== Variable Importance ===\n")
print(importance(rf_model))
varImpPlot(rf_model, main = "Random Forest Variable Importance")

# Test set predictions
test_data_rf <- test_data %>%
  mutate(Attrition_Factor = factor(ifelse(Attrition_Binary == 1, "Yes", "No"), levels = c("No", "Yes")))

pred_rf_prob <- predict(rf_model, test_data, type = "prob")[, "Yes"]
roc_rf <- roc(test_data$Attrition_Binary, pred_rf_prob, quiet = TRUE)

cat("\n=== Random Forest Performance (Test Set) ===\n")
cat("Test Set AUC-ROC:", round(auc(roc_rf), 4), "\n")
cat("Optimal mtry (from CV):", rf_tuned$bestTune$mtry, "\n")
cat("Cross-Validated AUC:", round(max(rf_tuned$results$ROC), 4), "\n")
```

## Q1 Model Comparison

```{r}
#| label: q1-roc-comparison

# Plot all Q1 ROC curves
plot(roc_dt, col = "#2E86AB", lwd = 2, main = "ROC Curves Comparison - Q1: Work-Life Balance Models")
plot(roc_rf, col = "#E94F37", lwd = 2, add = TRUE)
plot(roc_logit_q1, col = "#6A994E", lwd = 2, add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "gray50")

legend("bottomright",
       legend = c(paste("Decision Tree (AUC =", round(auc(roc_dt), 3), ")"),
                  paste("Random Forest (AUC =", round(auc(roc_rf), 3), ")"),
                  paste("Logistic Regression (AUC =", round(auc(roc_logit_q1), 3), ")")),
       col = c("#2E86AB", "#E94F37", "#6A994E"), lwd = 2, bty = "n")

cat("\n=== Q1 Model Comparison ===\n")
cat("Decision Tree AUC:", round(auc(roc_dt), 4), "\n")
cat("Random Forest AUC:", round(auc(roc_rf), 4), "\n")
cat("Logistic Regression AUC:", round(auc(roc_logit_q1), 4), "\n")
```

# Research Question 2: Career Stagnation Analysis

## Train/Test Split

```{r}
#| label: q2-setup

# Create train/test split for Q2
set.seed(515)
train_idx2 <- createDataPartition(hr_data$Attrition_Binary, p = 0.7, list = FALSE)
train_q2 <- hr_data[train_idx2, ]
test_q2 <- hr_data[-train_idx2, ]

cat("=== Q2 Train/Test Split ===\n")
cat("Training set:", nrow(train_q2), "observations\n")
cat("Test set:", nrow(test_q2), "observations\n")
```

## Weighted Logistic Regression

```{r}
#| label: q2-weighted-logit

# Calculate class weights for training set
n_total2 <- nrow(train_q2)
n_pos2 <- sum(train_q2$Attrition_Binary)
n_neg2 <- n_total2 - n_pos2
w_pos2 <- n_total2 / (2 * n_pos2)
w_neg2 <- n_total2 / (2 * n_neg2)
weights_q2 <- ifelse(train_q2$Attrition_Binary == 1, w_pos2, w_neg2)

cat("Class weights:\n")
cat("  Weight for No:", round(w_neg2, 4), "\n")
cat("  Weight for Yes:", round(w_pos2, 4), "\n")

# Fit weighted logistic regression
logit_q2 <- suppressWarnings(glm(
  Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole +
    MonthlyIncome + PercentSalaryHike + JobLevel,
  data = train_q2, 
  family = binomial(), 
  weights = weights_q2
))

# Model summary
summary(logit_q2)

# Odds ratios with confidence intervals
or_ci_q2 <- suppressMessages(confint(logit_q2))

cat("\n=== Odds Ratios ===\n")
print(exp(coef(logit_q2)))

cat("\n=== 95% Confidence Intervals for Odds Ratios ===\n")
print(exp(or_ci_q2))
```

## Career Stagnation Threshold Analysis

```{r}
#| label: q2-threshold-effects

# Calculate cumulative odds by years without promotion
yslp_coef <- coef(logit_q2)["YearsSinceLastPromotion"]
yslp_or <- exp(yslp_coef)

years_no_promo <- 1:7
cumulative_or <- yslp_or^years_no_promo

threshold_effects <- data.frame(
  Years = years_no_promo,
  Cumulative_OR = round(cumulative_or, 2),
  Percent_Increase = paste0(round((cumulative_or - 1) * 100, 0), "%"),
  Risk_Category = c("Low", "Low", "Moderate", "Elevated", "High", "Very High", "Critical")
)

cat("\n=== Cumulative Odds Multipliers by Years Without Promotion ===\n")
print(threshold_effects)
```

## LASSO Variable Selection

```{r}
#| label: q2-lasso

# Define LASSO variables
lasso_vars <- c(
  "YearsSinceLastPromotion", "YearsInCurrentRole", "MonthlyIncome",
  "PercentSalaryHike", "JobLevel", "Age", "TotalWorkingYears",
  "YearsAtCompany", "YearsWithCurrManager", "DistanceFromHome",
  "JobSatisfaction", "EnvironmentSatisfaction", "WorkLifeBalance",
  "OverTime_Binary", "NumCompaniesWorked", "StockOptionLevel"
)

# Prepare matrices
X_train <- as.matrix(train_q2[, lasso_vars])
y_train <- train_q2$Attrition_Binary

# Cross-validated LASSO
set.seed(515)
cv_lasso <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 1, nfolds = 10)

# Plot CV results
plot(cv_lasso, main = "LASSO Cross-Validation (Training Set Only)")

cat("\n=== LASSO Regularization Results ===\n")
cat("Optimal lambda (min):", round(cv_lasso$lambda.min, 6), "\n")
cat("Lambda 1SE:", round(cv_lasso$lambda.1se, 6), "\n")
cat("Variables retained:", sum(coef(cv_lasso, s = "lambda.min") != 0) - 1, "of", length(lasso_vars), "\n")

# Extract and display coefficients
lasso_coefs <- coef(cv_lasso, s = "lambda.min")
cat("\n=== LASSO Coefficients ===\n")
print(lasso_coefs)

# Create coefficient dataframe for visualization
lasso_df <- data.frame(
  Variable = rownames(lasso_coefs)[-1],
  Coefficient = as.vector(lasso_coefs)[-1]
) %>%
  filter(abs(Coefficient) > 0.001) %>%
  arrange(desc(abs(Coefficient)))

print(lasso_df)

# Variables eliminated by LASSO
eliminated <- setdiff(lasso_vars, lasso_df$Variable)
if(length(eliminated) > 0) {
  cat("\n=== Variables Eliminated by LASSO (shrunk to zero) ===\n")
  cat(paste(eliminated, collapse = ", "), "\n")
}

# Test set evaluation
X_test <- as.matrix(test_q2[, lasso_vars])
y_test <- test_q2$Attrition_Binary

pred_lasso <- predict(cv_lasso, newx = X_test, s = "lambda.min", type = "response")
roc_lasso <- roc(y_test, as.vector(pred_lasso), quiet = TRUE)

cat("\n=== LASSO Performance on Test Set ===\n")
cat("Test Set AUC-ROC:", round(auc(roc_lasso), 4), "\n")
```

## ROC-AUC and Threshold Analysis

```{r}
#| label: q2-threshold

# Predictions on test set
pred_probs_q2 <- predict(logit_q2, newdata = test_q2, type = "response")
roc_q2 <- roc(test_q2$Attrition_Binary, pred_probs_q2, quiet = TRUE)

# Optimal threshold (Youden's J)
coords_best <- coords(roc_q2, "best", ret = c("threshold", "sensitivity", "specificity"))
cat("\n=== Optimal Threshold (Youden's J) ===\n")
cat("Optimal Threshold:", round(coords_best$threshold, 4), "\n")
cat("Sensitivity:", round(coords_best$sensitivity, 4), "\n")
cat("Specificity:", round(coords_best$specificity, 4), "\n")

# ROC plot with optimal point
par(mfrow = c(1, 2))

plot(roc_q2, lwd = 3, col = "#2E86AB", main = "ROC Curve - Career Stagnation Model")
points(coords_best$specificity, coords_best$sensitivity, pch = 19, cex = 2, col = "#E94F37")
text(coords_best$specificity - 0.12, coords_best$sensitivity + 0.05,
     paste("Optimal:", round(coords_best$threshold, 3)), cex = 0.9, font = 2)
legend("bottomright", paste("AUC =", round(auc(roc_q2), 3)), bty = "n", cex = 1.1)

# Threshold analysis plot
thresholds <- seq(0.1, 0.9, by = 0.05)
metrics <- sapply(thresholds, function(t) {
  pred <- ifelse(pred_probs_q2 >= t, 1, 0)
  c(Sensitivity = mean(pred[test_q2$Attrition_Binary == 1] == 1),
    Specificity = mean(pred[test_q2$Attrition_Binary == 0] == 0),
    Accuracy = mean(pred == test_q2$Attrition_Binary))
})

plot(thresholds, metrics["Sensitivity", ], type = "l", lwd = 3, col = "#2E86AB",
     ylim = c(0, 1), xlab = "Classification Threshold", ylab = "Metric Value",
     main = "Classification Metrics vs Threshold")
lines(thresholds, metrics["Specificity", ], lwd = 3, col = "#E94F37")
lines(thresholds, metrics["Accuracy", ], lwd = 3, col = "#6A994E")
abline(v = coords_best$threshold, lty = 2, lwd = 2)
legend("right", c("Sensitivity", "Specificity", "Accuracy"), 
       col = c("#2E86AB", "#E94F37", "#6A994E"), lwd = 3, bty = "n")

par(mfrow = c(1, 1))

# Classification at different thresholds
cat("\n=== Classification Performance at Different Thresholds ===\n")
for (thresh in c(0.1, 0.2, 0.3, 0.4, 0.5, coords_best$threshold)) {
  pred <- ifelse(pred_probs_q2 >= thresh, 1, 0)
  sens <- mean(pred[test_q2$Attrition_Binary == 1] == 1)
  spec <- mean(pred[test_q2$Attrition_Binary == 0] == 0)
  acc <- mean(pred == test_q2$Attrition_Binary)
  cat(sprintf("Threshold %.3f: Sens=%.3f, Spec=%.3f, Acc=%.3f\n", thresh, sens, spec, acc))
}
```

## Calibration Assessment

```{r}
#| label: q2-calibration

# Calibration curve
cal_data <- data.frame(predicted = pred_probs_q2, actual = test_q2$Attrition_Binary) %>%
  mutate(predicted_bin = cut(predicted, breaks = seq(0, 1, 0.1), include.lowest = TRUE)) %>%
  group_by(predicted_bin) %>%
  summarise(mean_predicted = mean(predicted), mean_actual = mean(actual), n = n(), .groups = "drop") %>%
  filter(!is.na(predicted_bin) & n > 5)

plot(cal_data$mean_predicted, cal_data$mean_actual,
     xlim = c(0, 1), ylim = c(0, 1),
     xlab = "Mean Predicted Probability", ylab = "Observed Proportion",
     main = "Calibration Curve - Weighted Logistic Regression",
     pch = 19, cex = 2, col = "#2E86AB")
abline(0, 1, lty = 2, col = "#E94F37", lwd = 2)
grid()

cal_cor <- cor(cal_data$mean_predicted, cal_data$mean_actual)
cat("\n=== Calibration Assessment ===\n")
cat("Calibration correlation:", round(cal_cor, 3), "\n")
```

# Gradient Boosting Machine (GBM)

```{r}
#| label: gbm-training

# Define variables for GBM
gbm_vars <- c("OverTime_Binary", "YearsAtCompany", "YearsSinceLastPromotion",
              "YearsInCurrentRole", "MonthlyIncome", "PercentSalaryHike",
              "JobLevel", "JobSatisfaction", "EnvironmentSatisfaction",
              "WorkLifeBalance", "StockOptionLevel", "DistanceFromHome",
              "Age", "TotalWorkingYears")

# Train GBM with cross-validation
set.seed(515)
gbm_model <- gbm(
  Attrition_Binary ~ .,
  data = train_q2[, c("Attrition_Binary", gbm_vars)],
  distribution = "bernoulli",
  n.trees = 1000,
  interaction.depth = 4,
  shrinkage = 0.01,
  bag.fraction = 0.7,
  train.fraction = 1.0,
  n.minobsinnode = 10,
  cv.folds = 5,
  verbose = FALSE
)

# Find optimal number of trees
best_iter <- gbm.perf(gbm_model, method = "cv", plot.it = TRUE)

cat("\n=== Gradient Boosting Machine Training Results ===\n")
cat("Total trees trained: 1000\n")
cat("Optimal trees (5-fold CV):", best_iter, "\n")
cat("Learning rate: 0.01\n")
cat("Interaction depth: 4\n")
```

```{r}
#| label: gbm-importance

# Variable importance
importance_gbm <- summary(gbm_model, n.trees = best_iter, plotit = FALSE)

cat("\n=== GBM Variable Importance Rankings ===\n")
print(importance_gbm)

# Plot variable importance
ggplot(importance_gbm[1:min(15, nrow(importance_gbm)), ], 
       aes(x = reorder(var, rel.inf), y = rel.inf)) +
  geom_bar(stat = "identity", fill = "#6A994E", alpha = 0.8) +
  coord_flip() +
  labs(x = "Variable", y = "Relative Influence (%)",
       title = "Gradient Boosting Variable Importance",
       subtitle = paste("Based on", best_iter, "trees")) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))
```

```{r}
#| label: gbm-evaluation

# Predictions on test set
pred_gbm <- predict(gbm_model, newdata = test_q2, n.trees = best_iter, type = "response")
roc_gbm <- roc(test_q2$Attrition_Binary, pred_gbm, quiet = TRUE)

cat("\n=== GBM Performance on Test Set ===\n")
cat("Test Set AUC-ROC:", round(auc(roc_gbm), 4), "\n")
cat("Trees used:", best_iter, "\n")

# Q2 Model Comparison
cat("\n=== Q2 Model Comparison ===\n")
cat("Weighted Logistic Regression: AUC =", round(auc(roc_q2), 4), "\n")
cat("LASSO Logistic Regression:    AUC =", round(auc(roc_lasso), 4), "\n")
cat("Gradient Boosting Machine:    AUC =", round(auc(roc_gbm), 4), "\n")
```

```{r}
#| label: gbm-partial-dependence

# Partial dependence plots for top 4 variables
par(mfrow = c(2, 2))

top_vars <- importance_gbm$var[1:4]

for (var in top_vars) {
  plot(gbm_model, i.var = var, n.trees = best_iter,
       main = paste("Partial Dependence:", var),
       xlab = var, ylab = "Log-Odds Contribution")
}

par(mfrow = c(1, 1))
```

# Research Question 3: Department-Stratified Analysis

## VIF Analysis

```{r}
#| label: q3-vif

# VIF model for checking multicollinearity
vif_model <- lm(
  Attrition_Binary ~ JobSatisfaction + EnvironmentSatisfaction + RelationshipSatisfaction +
    WorkLifeBalance + JobInvolvement + MonthlyIncome + Age,
  data = hr_data
)

vif_vals <- vif(vif_model)

cat("\n=== Variance Inflation Factors ===\n")
print(round(vif_vals, 4))

vif_results <- data.frame(
  Variable = names(vif_vals), 
  VIF = round(as.numeric(vif_vals), 4),
  Status = ifelse(vif_vals < 2, "Excellent", ifelse(vif_vals < 5, "Acceptable", "Moderate"))
)
print(vif_results)
```

## Stratified Logistic Regression

```{r}
#| label: q3-stratified

# Fit models for each department
departments <- sort(unique(as.character(hr_data$Department)))
dept_results <- list()

for (dept in departments) {
  cat("\n========================================\n")
  cat("DEPARTMENT:", dept, "\n")
  cat("========================================\n")
  
  df_dept <- hr_data %>% filter(Department == dept)
  
  cat("Sample Size:", nrow(df_dept), "\n")
  cat("Attrition Rate:", round(mean(df_dept$Attrition_Binary) * 100, 1), "%\n")
  
  model_dept <- glm(
    Attrition_Binary ~ JobSatisfaction + EnvironmentSatisfaction + RelationshipSatisfaction,
    data = df_dept,
    family = binomial()
  )
  
  dept_results[[dept]] <- list(
    n = nrow(df_dept),
    attrition_rate = mean(df_dept$Attrition_Binary) * 100,
    model = model_dept,
    aic = AIC(model_dept)
  )
  
  cat("AIC:", round(AIC(model_dept), 2), "\n\n")
  print(summary(model_dept)$coefficients)
  
  cat("\nOdds Ratios:\n")
  print(round(exp(coef(model_dept)), 4))
  
  # AUC
  pred_probs <- predict(model_dept, type = "response")
  if(length(unique(df_dept$Attrition_Binary)) > 1 && sum(df_dept$Attrition_Binary) > 5) {
    roc_dept <- roc(df_dept$Attrition_Binary, pred_probs, quiet = TRUE)
    cat("\nAUC:", round(auc(roc_dept), 3), "\n")
  }
}
```

## Department Coefficient Comparison

```{r}
#| label: q3-coefficient-comparison

# Build coefficient comparison dataframe
coef_comparison <- data.frame(
  Department = character(), 
  Variable = character(),
  Coefficient = numeric(), 
  OR = numeric(), 
  p_value = numeric(), 
  stringsAsFactors = FALSE
)

satisfaction_vars <- c("JobSatisfaction", "EnvironmentSatisfaction", "RelationshipSatisfaction")

for (dept in names(dept_results)) {
  model <- dept_results[[dept]]$model
  for (var in satisfaction_vars) {
    coef_comparison <- rbind(coef_comparison, data.frame(
      Department = dept, 
      Variable = var, 
      Coefficient = coef(model)[var],
      OR = exp(coef(model)[var]), 
      p_value = summary(model)$coefficients[var, 4]
    ))
  }
}

coef_comparison$Significance <- ifelse(coef_comparison$p_value < 0.001, "***",
                                       ifelse(coef_comparison$p_value < 0.01, "**",
                                              ifelse(coef_comparison$p_value < 0.05, "*", "")))

cat("\n=== Coefficient Comparison by Department ===\n")
print(coef_comparison)

# Plot
ggplot(coef_comparison, aes(x = Variable, y = Coefficient, fill = Department)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(x = "Satisfaction Variable", y = "Coefficient (Log-Odds)",
       title = "Satisfaction Variable Effects by Department",
       subtitle = "Negative coefficients indicate protective effect against attrition") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 15, hjust = 1)) +
  scale_fill_brewer(palette = "Set2")
```

# Comprehensive Model Comparison

```{r}
#| label: model-comparison

# Collect all AUC values
auc_dt_val <- as.numeric(auc(roc_dt))
auc_rf_val <- as.numeric(auc(roc_rf))
auc_logit_q1_val <- as.numeric(auc(roc_logit_q1))
auc_q2_val <- as.numeric(auc(roc_q2))
auc_lasso_val <- as.numeric(auc(roc_lasso))
auc_gbm_val <- as.numeric(auc(roc_gbm))

# Build comparison table
model_comparison <- data.frame(
  Model = c("Decision Tree (Q1)", 
            "Random Forest - Tuned (Q1)", 
            "Logistic w/ Interactions (Q1)",
            "Weighted Logistic Regression (Q2)", 
            "LASSO Logistic Regression (Q2)",
            "Gradient Boosting Machine (Q2)"),
  Type = c("Tree", "Ensemble", "Regression", "Regression", "Regularized", "Ensemble"),
  Research_Question = c("Q1", "Q1", "Q1", "Q2", "Q2", "Q2"),
  AUC = c(auc_dt_val, auc_rf_val, auc_logit_q1_val, auc_q2_val, auc_lasso_val, auc_gbm_val)
)

cat("\n=== Comprehensive Model Performance Comparison ===\n")
print(model_comparison)

# Find best model
best_idx <- which.max(model_comparison$AUC)
cat("\nBest Overall Model:", model_comparison$Model[best_idx], "\n")
cat("Best AUC:", round(model_comparison$AUC[best_idx], 4), "\n")

# Plot comparison
ggplot(model_comparison, aes(x = reorder(Model, AUC), y = AUC, fill = Type)) +
  geom_bar(stat = "identity", alpha = 0.8) +
  geom_text(aes(label = round(AUC, 3)), hjust = -0.1, size = 4, fontface = "bold") +
  coord_flip() +
  labs(x = "", y = "Test Set AUC-ROC", 
       title = "Comprehensive Model Performance Comparison",
       subtitle = "Higher AUC indicates better discrimination") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14), legend.position = "bottom") +
  scale_fill_brewer(palette = "Set2") +
  ylim(0, max(model_comparison$AUC) * 1.15)
```

```{r}
#| label: all-roc-curves

# Plot all ROC curves together
plot(roc_dt, col = "#2E86AB", lwd = 2, main = "ROC Curves - All Models Comparison")
plot(roc_rf, col = "#E94F37", lwd = 2, add = TRUE)
plot(roc_logit_q1, col = "#6A994E", lwd = 2, add = TRUE)
plot(roc_q2, col = "#9B59B6", lwd = 2, add = TRUE)
plot(roc_lasso, col = "#F39C12", lwd = 2, add = TRUE)
plot(roc_gbm, col = "#1ABC9C", lwd = 2, add = TRUE)
abline(a = 0, b = 1, lty = 2, col = "gray50")

legend("bottomright",
       legend = c(paste("Decision Tree (", round(auc_dt_val, 3), ")"),
                  paste("Random Forest (", round(auc_rf_val, 3), ")"),
                  paste("Logistic Q1 (", round(auc_logit_q1_val, 3), ")"),
                  paste("Weighted LR Q2 (", round(auc_q2_val, 3), ")"),
                  paste("LASSO (", round(auc_lasso_val, 3), ")"),
                  paste("GBM (", round(auc_gbm_val, 3), ")")),
       col = c("#2E86AB", "#E94F37", "#6A994E", "#9B59B6", "#F39C12", "#1ABC9C"),
       lwd = 2, bty = "n", cex = 0.8)
```

# Final Summary

```{r}
#| label: summary

cat("=============================================\n")
cat("FINAL SUMMARY\n")
cat("=============================================\n\n")

cat("=== Dataset ===\n")
cat("Total Employees:", nrow(hr_data), "\n")
cat("Overall Attrition Rate:", round(mean(hr_data$Attrition_Binary) * 100, 1), "%\n\n")

cat("=== Key Statistics ===\n")
overtime_attr <- hr_data %>% filter(OverTime == "Yes") %>% summarise(rate = mean(Attrition_Binary) * 100) %>% pull(rate)
no_overtime_attr <- hr_data %>% filter(OverTime == "No") %>% summarise(rate = mean(Attrition_Binary) * 100) %>% pull(rate)
cat("OverTime Attrition Rate:", round(overtime_attr, 1), "%\n")
cat("Non-OverTime Attrition Rate:", round(no_overtime_attr, 1), "%\n")
cat("Relative Risk:", round(overtime_attr/no_overtime_attr, 1), "x\n\n")

cat("=== Model Performance ===\n")
cat("- Decision Tree AUC:", round(auc(roc_dt), 3), "\n")
cat("- Random Forest AUC:", round(auc(roc_rf), 3), "\n")
cat("- Logistic Regression (Q1) AUC:", round(auc(roc_logit_q1), 3), "\n")
cat("- Weighted Logistic Regression (Q2) AUC:", round(auc(roc_q2), 3), "\n")
cat("- LASSO AUC:", round(auc(roc_lasso), 3), "\n")
cat("- Gradient Boosting Machine AUC:", round(auc(roc_gbm), 3), "\n\n")

cat("=== Best Model ===\n")
cat("Model:", model_comparison$Model[best_idx], "\n")
cat("AUC:", round(model_comparison$AUC[best_idx], 3), "\n\n")

cat("=== Key Findings ===\n")
cat("1. OverTime is the strongest predictor of attrition across all models\n")
cat("2. Years Since Last Promotion shows compounding effect - 4-year mark is critical\n")
cat("3. Satisfaction effects vary significantly by department\n")
cat("4. LASSO selected", nrow(lasso_df), "of 16 variables as important predictors\n")
```

# Session Info

```{r}
#| label: session-info

sessionInfo()
```
