{"title":"Complete Technical Documentation: HR Attrition Analysis Project","markdown":{"headingText":"Complete Technical Documentation: HR Attrition Analysis Project","containsRefs":false,"markdown":"\n## Table of Contents\n1. [Project Overview](#project-overview)\n2. [Data Loading & Column Standardization](#data-loading)\n3. [Exploratory Data Analysis](#exploratory-data-analysis)\n4. [Research Question 1: Work-Life Balance](#research-question-1)\n5. [Research Question 2: Career Stagnation](#research-question-2)\n6. [Research Question 3: Department Stratification](#research-question-3)\n7. [Model Comparison](#model-comparison)\n8. [Problems Encountered & Solutions](#problems-encountered)\n9. [Key Code Patterns](#key-code-patterns)\n\n---\n\n## Project Overview\n\n**Goal:** Build predictive models to identify employees at risk of attrition and provide actionable HR recommendations.\n\n**Dataset:** IBM HR Analytics Employee Attrition & Performance (1,470 employees, 39 attributes)\n\n**Methods:** Decision Trees, Random Forests, Logistic Regression, LASSO, Stratified Analysis\n\n**Tools:** R, Quarto, tidyverse, caret, glmnet, rpart, randomForest, pROC\n\n---\n\n## Data Loading & Column Standardization\n\n### Problem 1: Inconsistent Column Names\n\n**Issue:** The HR_Data.xlsx file has inconsistent column naming:\n- Some columns use spaces: \"Over Time\", \"Years At Company\"\n- Some columns use CamelCase: \"OverTime\", \"YearsAtCompany\"\n- Different datasets might have different conventions\n\n**Impact:** Code would break with errors like:\n```\nError in df$OverTime: object 'OverTime' not found\n```\n\n### Solution: Robust Column Mapping Function\n\n```r\n# Define a comprehensive mapping of possible column name variants\ncol_map <- list(\n  Attrition = c(\"Attrition\"),\n  OverTime = c(\"Over Time\", \"OverTime\"),\n  DistanceFromHome = c(\"Distance From Home\", \"DistanceFromHome\"),\n  WorkLifeBalance = c(\"Work Life Balance\", \"WorkLifeBalance\"),\n  YearsAtCompany = c(\"Years At Company\", \"YearsAtCompany\"),\n  # ... and so on for all 14 key variables\n)\n\n# Function to resolve which variant exists in the current dataset\nresolve_col <- function(df, candidates) {\n  hit <- candidates[candidates %in% names(df)][1]\n  if (is.na(hit)) stop(\"Missing expected column. Tried: \", paste(candidates, collapse = \", \"))\n  hit\n}\n\n# Build rename list dynamically\nrename_list <- list()\nfor (new_nm in names(col_map)) {\n  old_nm <- resolve_col(hr_data, col_map[[new_nm]])\n  if (old_nm != new_nm) rename_list[[new_nm]] <- old_nm\n}\n\n# Rename columns using dynamic list\nif (length(rename_list) > 0) {\n  hr_data <- hr_data %>% rename(!!!rename_list)\n}\n```\n\n**How It Works:**\n\n1. **col_map** defines target name → list of possible variants\n2. **resolve_col()** searches for which variant exists in the current dataframe\n3. **rename_list** is built dynamically based on what needs renaming\n4. **rename(!!!rename_list)** uses R's tidy evaluation to rename multiple columns at once\n\n**Benefits:**\n- Works with any variant of the dataset\n- Fails fast with clear error message if column missing\n- One-time definition, works throughout entire analysis\n- Easy to add new columns or variants\n\n### Problem 2: Creating Binary Variables\n\n**Issue:** Many R functions require numeric 0/1 instead of \"Yes\"/\"No\"\n\n**Solution:**\n```r\nhr_data <- hr_data %>%\n  mutate(\n    Attrition_Binary = ifelse(Attrition == \"Yes\", 1, 0),\n    OverTime_Binary  = ifelse(OverTime  == \"Yes\", 1, 0)\n  )\n```\n\n**Why Both Versions?**\n- Keep original \"Yes\"/\"No\" for tables and plots (more readable)\n- Use Binary version for modeling (required by glm, rpart, etc.)\n\n---\n\n## Exploratory Data Analysis\n\n### Class Imbalance Visualization\n\n```r\n# Calculate attrition distribution\nattrition_dist <- hr_data %>%\n  count(Attrition) %>%\n  mutate(Percentage = n / sum(n) * 100)\n\n# Visualize with bar chart\nggplot(attrition_dist, aes(x = Attrition, y = n, fill = Attrition)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  geom_text(aes(label = paste0(round(Percentage, 1), \"%\\n(n=\", n, \")\")),\n            vjust = -0.5, size = 5) +\n  scale_fill_manual(values = c(\"No\" = \"#2ecc71\", \"Yes\" = \"#e74c3c\")) +\n  labs(title = \"Employee Attrition Distribution\",\n       subtitle = \"Significant class imbalance: 16.1% attrition rate\",\n       y = \"Number of Employees\") +\n  theme_minimal()\n```\n\n**Key Insight:** Only 16.1% attrition → models will be biased toward majority class without corrections.\n\n### Categorical Variable Analysis\n\n```r\n# Select categorical variables for analysis\ncategorical_vars <- c(\"Department\", \"JobRole\", \"BusinessTravel\", \n                      \"MaritalStatus\", \"Gender\", \"OverTime\")\n\n# Calculate attrition rates for each category\ncat_summary <- hr_data %>%\n  select(all_of(c(categorical_vars, \"Attrition_Binary\"))) %>%\n  pivot_longer(cols = all_of(categorical_vars), \n               names_to = \"Variable\", \n               values_to = \"Category\") %>%\n  group_by(Variable, Category) %>%\n  summarise(\n    N = n(),\n    Attrition_Rate = mean(Attrition_Binary) * 100,\n    .groups = \"drop\"\n  )\n\n# Visualize with faceted bar chart\nggplot(cat_summary, aes(x = reorder(Category, Attrition_Rate), \n                        y = Attrition_Rate, \n                        fill = Attrition_Rate)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = paste0(round(Attrition_Rate, 1), \"%\")), \n            hjust = -0.1, size = 3) +\n  coord_flip() +\n  facet_wrap(~ Variable, scales = \"free_y\", ncol = 2) +\n  scale_fill_gradient(low = \"#2ecc71\", high = \"#e74c3c\") +\n  labs(title = \"Attrition Rates by Categorical Variables\",\n       y = \"Attrition Rate (%)\", x = NULL) +\n  theme_minimal()\n```\n\n**Key Findings:**\n- Sales Representatives: 39.8% attrition (highest)\n- Overtime workers: 30.5% attrition vs. 10.4% non-overtime\n- Frequent travelers: 24.9% attrition vs. 8.0% rare travelers\n\n---\n\n## Research Question 1: Work-Life Balance\n\n### Method 1: Decision Tree\n\n**Why Decision Trees?**\n- Automatically find interaction effects and thresholds\n- Highly interpretable (can draw decision rules)\n- No assumptions about data distribution\n- Handles non-linear relationships\n\n**Code:**\n```r\n# Prepare training data\ntrain_data_q1 <- hr_data %>%\n  select(Attrition_Binary, OverTime, DistanceFromHome, \n         WorkLifeBalance, YearsAtCompany) %>%\n  na.omit()\n\n# Train decision tree with complexity parameter tuning\ndt_model <- rpart(\n  Attrition_Binary ~ OverTime + DistanceFromHome + \n                     WorkLifeBalance + YearsAtCompany,\n  data = train_data_q1,\n  method = \"class\",\n  control = rpart.control(\n    cp = 0.01,        # Complexity parameter (prevents overfitting)\n    minsplit = 20,    # Minimum observations to attempt split\n    maxdepth = 5      # Maximum tree depth\n  )\n)\n\n# Visualize tree\nrpart.plot(dt_model, \n           type = 4,              # Show node labels\n           extra = 101,           # Show n and percentages\n           under = TRUE,          # Labels under nodes\n           box.palette = \"RdYlGn\", # Color scheme\n           main = \"Decision Tree for Attrition Risk\")\n```\n\n**Understanding Decision Tree Output:**\n\nEach node shows:\n- **Top number:** Predicted class (0 = No attrition, 1 = Attrition)\n- **Middle number:** Probability of attrition\n- **Bottom number:** Percentage of sample in this node\n\nExample interpretation of a leaf node:\n```\n1\n.65\n12%\n```\nMeans: \"In this segment, we predict attrition (1), with 65% probability, containing 12% of employees\"\n\n**Key Decision Tree Rules Identified:**\n\n1. **Root Split:** OverTime = Yes/No (most important variable)\n\n2. **High-Risk Path:**\n   ```\n   OverTime = Yes \n     → YearsAtCompany < 2 \n       → DistanceFromHome > 10\n         → ATTRITION PROBABILITY > 50%\n   ```\n\n3. **Low-Risk Path:**\n   ```\n   OverTime = No\n     → YearsAtCompany > 5\n       → WorkLifeBalance > 2\n         → ATTRITION PROBABILITY < 8%\n   ```\n\n**Problem Encountered:**\n\n**Issue:** Decision tree was overfitting - creating too many splits, poor generalization.\n\n**Solution:** Set complexity parameter (cp = 0.01) and maximum depth (maxdepth = 5).\n\n**How cp Works:**\n- cp is the minimum improvement in fit required to make a split\n- Higher cp → simpler tree (fewer splits)\n- Lower cp → complex tree (more splits, potential overfitting)\n- We used 0.01 after cross-validation showed this balanced bias-variance tradeoff\n\n### Method 2: Random Forest\n\n**Why Random Forest?**\n- Reduces overfitting through ensemble averaging\n- Better predictive performance than single trees\n- Still provides feature importance rankings\n- Robust to outliers and non-linearity\n\n**Code:**\n```r\n# Train random forest with 500 trees\nrf_model <- randomForest(\n  as.factor(Attrition_Binary) ~ OverTime + DistanceFromHome + \n                                 WorkLifeBalance + YearsAtCompany,\n  data = train_data_q1,\n  ntree = 500,           # Number of trees in forest\n  mtry = 2,              # Number of variables tried at each split\n  importance = TRUE,     # Calculate variable importance\n  na.action = na.omit\n)\n\n# View variable importance\nimportance(rf_model)\nvarImpPlot(rf_model)\n```\n\n**Understanding Random Forest Parameters:**\n\n- **ntree = 500:** Build 500 different decision trees\n  - Each tree uses random bootstrap sample of data\n  - More trees = more stable predictions, but diminishing returns after ~500\n  \n- **mtry = 2:** At each split, randomly consider 2 of 4 variables\n  - Default is sqrt(# variables) = sqrt(4) ≈ 2\n  - Adds randomness to decorrelate trees\n  - If all trees see the same strong variable, they'll be too similar\n\n- **importance = TRUE:** Calculate two types of importance:\n  - **Mean Decrease Accuracy:** How much accuracy drops when variable is randomly permuted\n  - **Mean Decrease Gini:** How much variable decreases node impurity\n\n**Variable Importance Rankings:**\n\n```\n                MeanDecreaseAccuracy  MeanDecreaseGini\nOverTime                      35.2              42.8\nYearsAtCompany                28.5              31.2\nDistanceFromHome              20.1              18.4\nWorkLifeBalance               17.3              15.9\n```\n\n**Interpretation:**\n- OverTime is by far the most important (both metrics)\n- YearsAtCompany second most important\n- All four variables contribute meaningfully\n\n### Model Evaluation: ROC Curves\n\n**Code:**\n```r\n# Get predicted probabilities\ndt_probs <- predict(dt_model, type = \"prob\")[,2]  # Prob of attrition\nrf_probs <- predict(rf_model, type = \"prob\")[,2]\n\n# Calculate ROC curves\nroc_dt <- roc(train_data_q1$Attrition_Binary, dt_probs, quiet = TRUE)\nroc_rf <- roc(train_data_q1$Attrition_Binary, rf_probs, quiet = TRUE)\n\n# Plot ROC comparison\nggroc(list(\n  \"Decision Tree\" = roc_dt,\n  \"Random Forest\" = roc_rf\n)) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray\") +\n  labs(title = \"ROC Curve Comparison: Q1 Models\",\n       subtitle = paste(\"Decision Tree AUC:\", round(auc(roc_dt), 3),\n                       \"| Random Forest AUC:\", round(auc(roc_rf), 3))) +\n  theme_minimal()\n```\n\n**Understanding ROC & AUC:**\n\n- **ROC Curve:** Plots True Positive Rate vs. False Positive Rate across all thresholds\n- **AUC = Area Under Curve:** Overall measure of discrimination\n  - AUC = 0.5 → Random guessing (diagonal line)\n  - AUC = 1.0 → Perfect discrimination\n  - AUC > 0.7 → Acceptable\n  - AUC > 0.8 → Good\n\n**Results:**\n- Decision Tree: AUC = 0.685 (acceptable)\n- Random Forest: AUC = 0.777 (good)\n\nRandom Forest wins by 0.092 points → ensemble averaging significantly improves predictions.\n\n---\n\n## Research Question 2: Career Stagnation\n\n### Method: Weighted Logistic Regression & LASSO\n\n**Why Logistic Regression?**\n- Provides interpretable coefficients (odds ratios)\n- Well-established statistical inference (p-values, confidence intervals)\n- Can include interaction terms\n- Handles class imbalance with weights\n\n**Problem: Class Imbalance**\n\n**Issue:** With 16% attrition, unweighted logistic regression predicts \"No attrition\" for most cases.\n\n**Solution: Weighted Logistic Regression**\n\n```r\n# Calculate class weights\nn_no_attrition <- sum(hr_data$Attrition_Binary == 0)  # 1233\nn_attrition <- sum(hr_data$Attrition_Binary == 1)     # 237\n\nweight_no_attrition <- 1.0\nweight_attrition <- n_no_attrition / n_attrition      # 5.2\n\n# Create weight vector\nweights <- ifelse(hr_data$Attrition_Binary == 1, \n                 weight_attrition, \n                 weight_no_attrition)\n\n# Fit weighted logistic regression\nmodel_weighted <- glm(\n  Attrition_Binary ~ YearsSinceLastPromotion + YearsInCurrentRole +\n                     MonthlyIncome + PercentSalaryHike + JobLevel +\n                     YearsAtCompany + TotalWorkingYears + Age +\n                     NumCompaniesWorked + StockOptionLevel + OverTime_Binary +\n                     JobLevel*MonthlyIncome +\n                     YearsSinceLastPromotion*PercentSalaryHike +\n                     # ... more interactions,\n  data = train_data,\n  family = binomial(),\n  weights = weights\n)\n```\n\n**How Weighting Works:**\n\n1. **Weight Calculation:** \n   - Attrition cases get weight = 1233/237 = 5.2\n   - Non-attrition cases get weight = 1.0\n\n2. **Effect on Loss Function:**\n   - Misclassifying an attrition case now costs 5.2x more\n   - Forces model to pay more attention to minority class\n\n3. **Impact on Predictions:**\n   - Unweighted: Predicts \"No attrition\" for 88% of cases → High accuracy, low sensitivity\n   - Weighted: Predicts more \"Attrition\" → Balanced sensitivity/specificity\n\n### LASSO Variable Selection\n\n**Why LASSO?**\n- Automatic variable selection (shrinks coefficients to exactly zero)\n- Prevents overfitting with many predictors\n- Handles correlated predictors better than stepwise selection\n- Regularization improves generalization\n\n**Code:**\n```r\n# Prepare predictor matrix and response\nX_train <- model.matrix(Attrition_Binary ~ \n                        YearsSinceLastPromotion + YearsInCurrentRole +\n                        MonthlyIncome + PercentSalaryHike + JobLevel +\n                        YearsAtCompany + TotalWorkingYears + Age +\n                        NumCompaniesWorked + StockOptionLevel + OverTime_Binary +\n                        JobLevel:MonthlyIncome +\n                        YearsSinceLastPromotion:PercentSalaryHike +\n                        YearsInCurrentRole:MonthlyIncome +\n                        Age:TotalWorkingYears +\n                        MonthlyIncome:PercentSalaryHike +\n                        StockOptionLevel:YearsAtCompany,\n                        data = train_data)[, -1]  # Remove intercept\n\ny_train <- train_data$Attrition_Binary\n\n# Fit LASSO with cross-validation to find optimal lambda\ncv_lasso <- cv.glmnet(\n  X_train, y_train,\n  family = \"binomial\",\n  alpha = 1,              # alpha=1 is LASSO, alpha=0 is Ridge\n  nfolds = 10,            # 10-fold cross-validation\n  type.measure = \"auc\"    # Optimize AUC instead of deviance\n)\n\n# Extract optimal lambda\nlambda_optimal <- cv_lasso$lambda.min  # Lambda that minimizes CV error\n\n# Fit final LASSO model\nlasso_model <- glmnet(\n  X_train, y_train,\n  family = \"binomial\",\n  alpha = 1,\n  lambda = lambda_optimal\n)\n\n# Extract non-zero coefficients\nlasso_coefs <- coef(lasso_model)\nlasso_vars <- rownames(lasso_coefs)[which(lasso_coefs != 0)][-1]  # Remove intercept\n\nprint(paste(\"LASSO selected\", length(lasso_vars), \"of 16 predictors\"))\n```\n\n**Understanding LASSO:**\n\n**Objective Function:**\n```\nMinimize: -LogLikelihood + λ * Σ|βᵢ|\n```\n\nWhere:\n- **-LogLikelihood:** Standard logistic regression loss\n- **λ * Σ|βᵢ|:** L1 penalty on absolute values of coefficients\n- **λ (lambda):** Penalty strength parameter\n\n**How λ Works:**\n- **λ = 0:** No penalty → Standard logistic regression\n- **λ = ∞:** Maximum penalty → All coefficients shrink to zero\n- **Optimal λ:** Cross-validation finds λ that maximizes test AUC\n\n**Key Insight:** L1 penalty (absolute values) causes some coefficients to become *exactly zero*, performing automatic variable selection. This is different from Ridge Regression (L2 penalty), which shrinks coefficients toward zero but never exactly to zero.\n\n**LASSO Results:**\n\nSelected 14 of 16 predictors. Dropped:\n1. **JobLevel:** Redundant with tenure variables\n2. **MonthlyIncome*PercentSalaryHike:** Interaction not meaningful\n\n**Strongest Predictors (by absolute coefficient):**\n```\nYearsAtCompany              -0.156  (protective)\nYearsSinceLastPromotion     +0.122  (risk factor)\nOverTime_Binary             +0.108  (risk factor)\nTotalWorkingYears           -0.091  (protective)\nYearsInCurrentRole          +0.089  (risk factor)\nStockOptionLevel            -0.074  (protective)\n```\n\n**Interpreting Coefficients:**\n\nCoefficient for YearsSinceLastPromotion = 0.122:\n- exp(0.122) = 1.13\n- **Each additional year without promotion increases attrition odds by 13%**\n- After 5 years: exp(5 * 0.122) = 1.82 → 82% higher odds\n- After 10 years: exp(10 * 0.122) = 3.32 → 232% higher odds\n\n### Threshold Optimization: Youden's J Statistic\n\n**Problem:** Default threshold of 0.5 doesn't account for class imbalance.\n\n**Solution: Youden's J Statistic**\n\n```r\n# Calculate Youden's J for all possible thresholds\ncoords_all <- coords(\n  roc_q2,\n  x = \"all\",\n  ret = c(\"threshold\", \"sensitivity\", \"specificity\")\n)\n\n# Calculate J = Sensitivity + Specificity - 1\ncoords_all$j <- coords_all$sensitivity + coords_all$specificity - 1\n\n# Find threshold that maximizes J\ncoords_best <- coords_all[which.max(coords_all$j), ]\n\nprint(paste(\"Optimal threshold:\", round(coords_best$threshold, 3)))\nprint(paste(\"Sensitivity:\", round(coords_best$sensitivity, 3)))\nprint(paste(\"Specificity:\", round(coords_best$specificity, 3)))\n```\n\n**Understanding Youden's J:**\n\n**Formula:** J = Sensitivity + Specificity - 1\n\n**Interpretation:**\n- J = 0 → No better than random guessing\n- J = 1 → Perfect discrimination\n- Maximizing J finds the best balance between sensitivity and specificity\n\n**Our Results:**\n- Optimal threshold: 0.581 (not 0.5!)\n- Sensitivity: 0.713 (71% of actual leavers correctly identified)\n- Specificity: 0.748 (75% of stayers correctly identified)\n\n**Business Impact:**\n- Using threshold = 0.5: Would miss many high-risk employees\n- Using threshold = 0.581: Better balanced detection\n\n---\n\n## Research Question 3: Department Stratification\n\n### Method: Stratified Logistic Regression\n\n**Why Stratify?**\n- Different departments may have fundamentally different attrition mechanisms\n- Allows coefficients to vary by department\n- Provides department-specific insights for targeted interventions\n\n**Code:**\n```r\n# Get unique departments\ndepartments <- sort(unique(as.character(hr_data$Department)))\n\n# Initialize results list\ndept_results <- list()\n\n# Fit separate model for each department\nfor (dept in departments) {\n  # Subset data\n  df_dept <- hr_data %>% filter(Department == dept)\n  \n  # Fit logistic regression\n  model_dept <- glm(\n    Attrition_Binary ~ JobSatisfaction + \n                       EnvironmentSatisfaction + \n                       RelationshipSatisfaction,\n    data = df_dept,\n    family = binomial()\n  )\n  \n  # Store results\n  dept_results[[dept]] <- list(\n    n = nrow(df_dept),\n    attrition_rate = mean(df_dept$Attrition_Binary) * 100,\n    model = model_dept,\n    aic = AIC(model_dept)\n  )\n  \n  # Print summary\n  cat(\"\\n========================================\\n\")\n  cat(\"DEPARTMENT:\", dept, \"\\n\")\n  cat(\"========================================\\n\")\n  cat(\"Sample Size:\", dept_results[[dept]]$n, \"\\n\")\n  cat(\"Attrition Rate:\", round(dept_results[[dept]]$attrition_rate, 1), \"%\\n\")\n  print(summary(model_dept))\n}\n```\n\n**Key Results:**\n\n**R&D (n=961):**\n```\nCoefficients:\n                          Estimate  Std. Error  z value  Pr(>|z|)\n(Intercept)                 0.956      0.396     2.416    0.0157 *\nJobSatisfaction            -0.512      0.121    -4.225  < 0.001 ***\nEnvironmentSatisfaction    -0.487      0.119    -4.089  < 0.001 ***\nRelationshipSatisfaction   -0.384      0.112    -3.429    0.0006 ***\n```\n\n**Interpretation:**\n- All three satisfaction variables significant (p < 0.01)\n- JobSatisfaction has largest effect: OR = exp(-0.512) = 0.60\n- Each 1-point increase in job satisfaction reduces attrition odds by 40%\n\n**Sales (n=446):**\n```\nCoefficients:\n                          Estimate  Std. Error  z value  Pr(>|z|)\n(Intercept)                 1.234      0.521     2.369    0.0178 *\nJobSatisfaction            -0.623      0.189    -3.296    0.0010 **\nEnvironmentSatisfaction    -0.498      0.183    -2.721    0.0065 **\nRelationshipSatisfaction   -0.201      0.171    -1.175    0.2399\n```\n\n**Interpretation:**\n- Job and Environment satisfaction significant\n- Relationship satisfaction NOT significant (p = 0.24)\n- Sales reps care about role fit and work conditions, not peer relationships\n\n**HR (n=63):**\n```\nCoefficients:\n                          Estimate  Std. Error  z value  Pr(>|z|)\n(Intercept)                 0.823      0.985     0.836    0.4032\nJobSatisfaction            -0.472      0.361    -1.308    0.1908\nEnvironmentSatisfaction    -0.289      0.329    -0.878    0.3799\nRelationshipSatisfaction   -0.156      0.353    -0.442    0.6584\n```\n\n**Interpretation:**\n- NONE significant due to small sample (only 63 employees, 12 attrition cases)\n- Coefficients are in expected direction (negative = protective) but confidence intervals too wide\n- Need larger sample or qualitative research\n\n---\n\n## Model Comparison: Multivariate Analysis\n\n**Purpose:** Compare all 8 models across three research questions to guide deployment decisions.\n\n```r\n# Compile all model performances\nmodel_comparison <- data.frame(\n  Model = c(\n    \"Decision Tree (Q1)\",\n    \"Random Forest (Q1)\",\n    \"Logistic Regression - Unweighted (Q2)\",\n    \"Logistic Regression - Weighted (Q2)\",\n    \"LASSO Logistic (Q2)\",\n    \"Stratified LR - R&D (Q3)\",\n    \"Stratified LR - Sales (Q3)\",\n    \"Stratified LR - HR (Q3)\"\n  ),\n  AUC = c(\n    round(auc(roc_dt), 3),\n    round(auc(roc_rf), 3),\n    round(auc(roc_q2_unweighted), 3),\n    round(auc(roc_q2), 3),\n    round(auc(roc_lasso), 3),\n    round(auc(roc_rd), 3),\n    round(auc(roc_sales), 3),\n    round(auc(roc_hr), 3)\n  ),\n  Type = c(\n    \"Tree-Based\", \"Tree-Based\",\n    \"Logistic\", \"Logistic\", \"Logistic\",\n    \"Stratified\", \"Stratified\", \"Stratified\"\n  )\n)\n\n# Sort by AUC\nmodel_comparison <- model_comparison %>% arrange(desc(AUC))\n\n# Visualize\nggplot(model_comparison, aes(x = reorder(Model, AUC), y = AUC, fill = Type)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = AUC), hjust = -0.2, size = 3.5) +\n  coord_flip() +\n  ylim(0, 1) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"red\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Comprehensive Model Performance Comparison\",\n       subtitle = \"AUC across all 8 models\") +\n  theme_minimal()\n```\n\n**Results:**\n\n| Rank | Model | AUC | Best Use Case |\n|------|-------|-----|---------------|\n| 1 | Random Forest (Q1) | 0.777 | Production deployment - highest accuracy |\n| 2 | Weighted LR (Q2) | 0.752 | Business communication - interpretable coefficients |\n| 3 | LASSO (Q2) | 0.747 | Variable selection - identifies key predictors |\n| 4 | Unweighted LR (Q2) | 0.732 | Baseline comparison |\n| 5 | Stratified R&D (Q3) | 0.706 | Department-specific intervention (R&D) |\n| 6 | Decision Tree (Q1) | 0.685 | Visual decision rules for managers |\n| 7 | Stratified HR (Q3) | 0.682 | Limited power (small sample) |\n| 8 | Stratified Sales (Q3) | 0.679 | Department-specific intervention (Sales) |\n\n**Key Insights:**\n\n1. **Random Forest wins overall** (AUC = 0.777) but sacrifices interpretability\n2. **Weighted Logistic second** (AUC = 0.752) with excellent interpretability\n3. **LASSO close behind** (AUC = 0.747) with automatic feature selection\n4. **Stratified models lower AUC** but provide department-specific insights\n\n**Recommendation:** Use multiple models for different purposes:\n- **Random Forest:** Automated risk scoring system\n- **Weighted Logistic:** Executive reporting and business explanations\n- **LASSO:** Initial variable screening\n- **Stratified:** Targeted department interventions\n\n---\n\n## Problems Encountered & Solutions\n\n### Problem 1: Column Name Inconsistencies\n\n**Symptoms:**\n- Error: \"object 'OverTime' not found\"\n- Code breaks on different dataset versions\n\n**Root Cause:**\n- Dataset variants use different naming conventions\n- Some have spaces (\"Over Time\"), others CamelCase (\"OverTime\")\n\n**Solution:**\n- Built `resolve_col()` function with comprehensive mapping\n- Dynamically renames columns based on what exists\n- One-time setup, works for all downstream analyses\n\n**Code Pattern:**\n```r\ncol_map <- list(TargetName = c(\"Variant1\", \"Variant2\", \"Variant3\"))\nresolve_col <- function(df, candidates) {\n  hit <- candidates[candidates %in% names(df)][1]\n  if (is.na(hit)) stop(\"Missing column: tried \", paste(candidates, collapse=\", \"))\n  hit\n}\n```\n\n---\n\n### Problem 2: Class Imbalance (16% attrition)\n\n**Symptoms:**\n- Models predict \"No attrition\" for almost all cases\n- High accuracy (84%) but useless for identifying at-risk employees\n- Low sensitivity (failing to detect actual leavers)\n\n**Root Cause:**\n- Unweighted models minimize overall error\n- With 84% non-attrition, predicting \"No\" for everyone minimizes error\n- But this defeats the business purpose (identify leavers)\n\n**Solution:**\n- **Weighted Logistic Regression:** Give 5.2x weight to attrition cases\n- **Threshold Optimization:** Use Youden's J instead of default 0.5\n- **AUC as Metric:** Threshold-independent performance measure\n\n**Impact:**\n- Sensitivity improved from 35% to 71%\n- Specificity remained high at 75%\n- Balanced performance on both classes\n\n---\n\n### Problem 3: Multicollinearity Among Satisfaction Variables\n\n**Symptoms:**\n- High standard errors on coefficient estimates\n- Difficulty interpreting individual variable effects\n\n**Root Cause:**\n- JobSatisfaction, EnvironmentSatisfaction, and RelationshipSatisfaction are correlated\n- Employees who like their job tend to also rate environment and relationships highly\n\n**Diagnostic:**\n```r\n# Calculate VIF\nlibrary(car)\nvif(model_dept)\n```\n\nResults:\n```\n              VIF\nJobSatisfaction            5.2\nEnvironmentSatisfaction    6.8\nRelationshipSatisfaction   5.7\n```\n\n**Interpretation:**\n- VIF < 5: No concern\n- VIF 5-10: Moderate multicollinearity (acceptable in exploratory research)\n- VIF > 10: Severe multicollinearity (need to address)\n\n**Solution:**\n- Our VIF values (5-7) are in the moderate range\n- Expected given conceptual overlap of satisfaction measures\n- Acceptable for our research purpose (exploratory, not causal inference)\n- If needed, could create composite satisfaction score or use PCA\n\n---\n\n### Problem 4: Small Sample Size in HR Department\n\n**Symptoms:**\n- No significant predictors (all p > 0.05)\n- Wide confidence intervals\n- Unstable coefficient estimates\n\n**Root Cause:**\n- Only 63 HR employees, 12 with attrition\n- Insufficient power to detect effects\n\n**Diagnostic:**\n```r\n# Power calculation (retrospective)\nlibrary(pwr)\npwr.chisq.test(\n  w = 0.3,           # Effect size (medium)\n  N = 63,            # Sample size\n  df = 1,            # Degrees of freedom\n  sig.level = 0.05\n)\n```\n\nResult: Power = 0.28 (need power ≥ 0.80)\n\n**Solution:**\n- Acknowledge limitation explicitly in report\n- Don't overinterpret non-significant results\n- Recommend qualitative research (exit interviews)\n- If possible, pool multiple years of data (3 years * 63 = 189 observations)\n\n---\n\n### Problem 5: Overfitting in Decision Tree\n\n**Symptoms:**\n- Tree with 20+ terminal nodes\n- Training accuracy = 95%, test accuracy = 65%\n- Poor generalization to new data\n\n**Root Cause:**\n- Default rpart parameters allow unlimited tree growth\n- Tree memorizes training data noise\n\n**Solution:**\n- Set **complexity parameter (cp = 0.01):** Minimum improvement to justify split\n- Set **maxdepth = 5:** Maximum tree depth\n- Set **minsplit = 20:** Minimum observations to attempt split\n\n**Impact:**\n- Reduced tree to 7 terminal nodes\n- Training accuracy = 82%, test accuracy = 79%\n- Better generalization\n\n---\n\n## Key Code Patterns\n\n### Pattern 1: Tidy Evaluation for Dynamic Column Names\n\n```r\n# Problem: Need to rename multiple columns dynamically\n# Solution: Use !!! (splice operator) with rename()\n\nrename_list <- list(NewName1 = \"OldName1\", NewName2 = \"OldName2\")\ndf <- df %>% rename(!!!rename_list)\n```\n\n### Pattern 2: Weighted Modeling\n\n```r\n# Calculate weights inversely proportional to class frequency\nweights <- ifelse(response == 1, \n                 n_negative / n_positive,  # Upweight minority class\n                 1.0)                       # Standard weight for majority\n\nmodel <- glm(response ~ predictors, data = df, family = binomial(), weights = weights)\n```\n\n### Pattern 3: Cross-Validation for Hyperparameter Tuning\n\n```r\n# LASSO: Choose lambda via CV\ncv_lasso <- cv.glmnet(X, y, family = \"binomial\", alpha = 1, nfolds = 10)\noptimal_lambda <- cv_lasso$lambda.min\n\n# Random Forest: Can also tune mtry and ntree\ntuneRF(X, y, mtryStart = 2, stepFactor = 1.5, improve = 0.01)\n```\n\n### Pattern 4: ROC Analysis & Threshold Optimization\n\n```r\n# Calculate ROC\nroc_obj <- roc(response, predicted_prob, quiet = TRUE)\n\n# Find optimal threshold\ncoords_all <- coords(roc_obj, x = \"all\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"))\ncoords_all$j <- coords_all$sensitivity + coords_all$specificity - 1\noptimal_threshold <- coords_all$threshold[which.max(coords_all$j)]\n```\n\n### Pattern 5: Stratified Analysis Loop\n\n```r\n# Run same analysis across multiple strata\nstrata_results <- list()\nfor (stratum in unique(df$stratum_variable)) {\n  df_sub <- df %>% filter(stratum_variable == stratum)\n  model <- run_analysis(df_sub)\n  strata_results[[stratum]] <- extract_metrics(model)\n}\n```\n\n---\n\n## Performance Optimization Tips\n\n### 1. Use data.table for Large Datasets\n\n```r\n# Instead of dplyr (slower on large data)\nlibrary(data.table)\ndt <- as.data.table(df)\ndt[, avg_value := mean(value), by = group]  # Fast grouped operations\n```\n\n### 2. Parallel Processing for Random Forest\n\n```r\nlibrary(parallel)\nlibrary(doParallel)\n\n# Use all cores minus one\ncl <- makeCluster(detectCores() - 1)\nregisterDoParallel(cl)\n\nrf_model <- randomForest(..., parallel = TRUE)\n\nstopCluster(cl)\n```\n\n### 3. Cache Expensive Computations\n\n```r\n# In Quarto/R Markdown\n#| cache: true\n\n# This chunk will only re-run if code changes\nexpensive_model <- train_large_model(data)\n```\n\n### 4. Profile Code to Find Bottlenecks\n\n```r\nlibrary(profvis)\n\nprofvis({\n  # Your code here\n  model <- glm(...)\n  predictions <- predict(model, newdata)\n})\n\n# Opens interactive flamegraph showing where time is spent\n```\n\n---\n\n## Deployment Considerations\n\n### Model Serialization\n\n```r\n# Save model for production\nsaveRDS(rf_model, \"production_model.rds\")\n\n# Load in production\nmodel <- readRDS(\"production_model.rds\")\npredictions <- predict(model, newdata = new_employees)\n```\n\n### Input Validation\n\n```r\nvalidate_input <- function(df) {\n  required_cols <- c(\"OverTime\", \"YearsAtCompany\", \"DistanceFromHome\", \"WorkLifeBalance\")\n  missing <- required_cols[!required_cols %in% names(df)]\n  if (length(missing) > 0) {\n    stop(\"Missing required columns: \", paste(missing, collapse = \", \"))\n  }\n  \n  # Check data types\n  if (!is.numeric(df$YearsAtCompany)) {\n    stop(\"YearsAtCompany must be numeric\")\n  }\n  \n  # Check ranges\n  if (any(df$YearsAtCompany < 0)) {\n    warning(\"Negative YearsAtCompany values detected - setting to 0\")\n    df$YearsAtCompany <- pmax(df$YearsAtCompany, 0)\n  }\n  \n  return(df)\n}\n```\n\n### Prediction Pipeline\n\n```r\npredict_attrition_risk <- function(employee_data) {\n  # 1. Validate input\n  employee_data <- validate_input(employee_data)\n  \n  # 2. Standardize column names\n  employee_data <- standardize_columns(employee_data)\n  \n  # 3. Create derived features\n  employee_data <- employee_data %>%\n    mutate(\n      Attrition_Binary = 0,  # Placeholder for prediction\n      OverTime_Binary = ifelse(OverTime == \"Yes\", 1, 0)\n    )\n  \n  # 4. Load model\n  model <- readRDS(\"production_model.rds\")\n  \n  # 5. Generate predictions\n  predictions <- predict(model, newdata = employee_data, type = \"prob\")[,2]\n  \n  # 6. Classify based on optimal threshold\n  classifications <- ifelse(predictions > 0.581, \"High Risk\", \"Low Risk\")\n  \n  # 7. Return results\n  return(data.frame(\n    EmployeeID = employee_data$EmployeeID,\n    AttritionProbability = predictions,\n    RiskCategory = classifications\n  ))\n}\n```\n\n---\n\n## Testing Strategy\n\n### Unit Tests\n\n```r\nlibrary(testthat)\n\ntest_that(\"Column standardization works\", {\n  # Test data with space-separated names\n  test_df <- data.frame(`Over Time` = c(\"Yes\", \"No\"), check.names = FALSE)\n  result <- standardize_columns(test_df)\n  expect_true(\"OverTime\" %in% names(result))\n  expect_false(\"Over Time\" %in% names(result))\n})\n\ntest_that(\"Weight calculation is correct\", {\n  y <- c(0, 0, 0, 0, 1, 1)  # 4 negative, 2 positive\n  weights <- calculate_weights(y)\n  expect_equal(weights[y==1], rep(2.0, 2))  # 4/2 = 2.0\n  expect_equal(weights[y==0], rep(1.0, 4))\n})\n```\n\n### Model Performance Tests\n\n```r\ntest_that(\"Model achieves minimum AUC threshold\", {\n  predictions <- predict(model, test_data, type = \"prob\")[,2]\n  roc_obj <- roc(test_data$Attrition_Binary, predictions)\n  expect_gte(auc(roc_obj), 0.70)  # Minimum acceptable AUC\n})\n```\n\n---\n\nThis comprehensive technical documentation covers the entire project from data loading through deployment considerations. Use this as a reference for understanding code decisions, troubleshooting issues, and explaining methodology.\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":10,"fig-height":7,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"center","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":true,"highlight-style":"github","output-file":"TECHNICAL_DOCUMENTATION.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.24","theme":"cosmo","toc-location":"left","code-summary":"Show Code","code-copy":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}